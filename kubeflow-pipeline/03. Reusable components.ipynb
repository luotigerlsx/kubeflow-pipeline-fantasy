{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusable components\n",
    "\n",
    "This tutorial describes the manual way of writing a full component program (in any language) and a component definition for it. For quickly building component from a python function see Build component from Python function and Data Passing in Python components.\n",
    "\n",
    "Below is a summary of the steps involved in creating and using a component:\n",
    "\n",
    "- Write the program that contains your component’s logic. The program must use files and command-line arguments to pass data to and from the component.\n",
    "- Containerize the program.\n",
    "- Write a component specification in YAML format that describes the component for the Kubeflow Pipelines system.\n",
    "- Use the Kubeflow Pipelines SDK to load your component, use it in a pipeline and run that pipeline.\n",
    "\n",
    "**Note: Make sure that you have docker installed in the local environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='kubeflow-pipeline-fantasy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the program code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a file `app.py` that contains a Python script. The script takes a GCS bucket name as an input argument, gets the lists of blobs in that bucket, prints the list of blobs and also writes them to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/reuse_components/minist_training\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/reuse_components/minist_training/app.py <<HERE\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--model_file', type=str, required=True, help='Name of the model file.')\n",
    "parser.add_argument(\n",
    "    '--bucket', type=str, required=True, help='GCS bucket name.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "bucket=args.bucket\n",
    "model_file=args.model_file\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())    \n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.TensorBoard(log_dir=bucket + '/logs/' + datetime.now().date().__str__()),\n",
    "  # Interrupt training if val_loss stops improving for over 2 epochs\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "]\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "model.save(model_file)\n",
    "\n",
    "from tensorflow import gfile\n",
    "\n",
    "gcs_path = bucket + \"/\" + model_file\n",
    "\n",
    "if gfile.Exists(gcs_path):\n",
    "    gfile.Remove(gcs_path)\n",
    "\n",
    "gfile.Copy(model_file, gcs_path)\n",
    "with open('/output.txt', 'w') as f:\n",
    "  f.write(gcs_path)\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker container\n",
    "Create your own container image that includes your program. \n",
    "- If your component creates some outputs to be fed as inputs to the downstream components, each separate output must be written as a string to a separate local text file inside the container image. \n",
    "- For example, if a trainer component needs to output the trained model path, it can write the path to a local file `/output.txt`. \n",
    "- The string written to an output file cannot be too big. If it is too big (>> 100 kB), it is recommended to save the output to an external persistent storage and pass the storage path to the next component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a container that runs the script. Start by creating a `Dockerfile`. A `Dockerfile` contains the instructions to assemble a Docker image. The `FROM` statement specifies the Base Image from which you are building. `WORKDIR` sets the working directory. When you assemble the Docker image, `COPY` will copy the required files and directories (for example, `app.py`) to the filesystem of the container. `RUN` will execute a command (for example, install the dependencies) and commits the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/reuse_components/minist_training/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.15.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our Dockerfile we can create our Docker image. Then we need to push the image to a registry to host the image. Now create a Shell script that builds a container image and stores it in the Google Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"{PROJECT_ID}\"\n",
    "\n",
    "IMAGE_NAME=\"minist_training_kf_pipeline\"\n",
    "TAG=\"latest\" # \"v_$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "# Create script to build docker image and push it.\n",
    "cat > ./tmp/reuse_components/minist_training/build_image.sh <<HERE\n",
    "PROJECT_ID=\"${1}\"\n",
    "IMAGE_NAME=\"${IMAGE_NAME}\"\n",
    "TAG=\"${TAG}\"\n",
    "GCR_IMAGE=\"gcr.io/\\${PROJECT_ID}/\\${IMAGE_NAME}:\\${TAG}\"\n",
    "docker build -t \\${IMAGE_NAME} .\n",
    "docker tag \\${IMAGE_NAME} \\${GCR_IMAGE}\n",
    "docker push \\${GCR_IMAGE}\n",
    "docker inspect --format=\"{{index .RepoDigests 0}}\" \"${IMAGE_NAME}\"\n",
    "docker image rm \\${IMAGE_NAME}\n",
    "docker image rm \\${GCR_IMAGE}\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon   5.12kB\r",
      "\r\n",
      "Step 1/3 : FROM tensorflow/tensorflow:1.15.0-py3\n",
      " ---> f24a5ca8605f\n",
      "Step 2/3 : WORKDIR /app\n",
      " ---> Running in 0b26d4bfd52f\n",
      "Removing intermediate container 0b26d4bfd52f\n",
      " ---> 056736b48ba8\n",
      "Step 3/3 : COPY . /app\n",
      " ---> e28c0f74ace2\n",
      "Successfully built e28c0f74ace2\n",
      "Successfully tagged minist_training_kf_pipeline:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline]\n",
      "fd9add579af0: Preparing\n",
      "0c9a46f378a1: Preparing\n",
      "84c3bc63b701: Preparing\n",
      "56ec85ad394c: Preparing\n",
      "aefe991487a2: Preparing\n",
      "4a58ecdd995f: Preparing\n",
      "fa9f3f4bd775: Preparing\n",
      "2bf9e296738e: Preparing\n",
      "92486bede3ce: Preparing\n",
      "19331eff40f0: Preparing\n",
      "100ef12ce3a4: Preparing\n",
      "97e6b67a30f1: Preparing\n",
      "a090697502b8: Preparing\n",
      "4a58ecdd995f: Waiting\n",
      "fa9f3f4bd775: Waiting\n",
      "2bf9e296738e: Waiting\n",
      "92486bede3ce: Waiting\n",
      "19331eff40f0: Waiting\n",
      "100ef12ce3a4: Waiting\n",
      "97e6b67a30f1: Waiting\n",
      "a090697502b8: Waiting\n",
      "84c3bc63b701: Layer already exists\n",
      "56ec85ad394c: Layer already exists\n",
      "aefe991487a2: Layer already exists\n",
      "4a58ecdd995f: Layer already exists\n",
      "fa9f3f4bd775: Layer already exists\n",
      "2bf9e296738e: Layer already exists\n",
      "92486bede3ce: Layer already exists\n",
      "19331eff40f0: Layer already exists\n",
      "100ef12ce3a4: Layer already exists\n",
      "a090697502b8: Layer already exists\n",
      "97e6b67a30f1: Layer already exists\n",
      "fd9add579af0: Pushed\n",
      "0c9a46f378a1: Pushed\n",
      "latest: digest: sha256:205c12a4cde07b4daa25d8867785b7f392cb5b08a66896df9aefd90ea93440e5 size: 3038\n",
      "gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline@sha256:205c12a4cde07b4daa25d8867785b7f392cb5b08a66896df9aefd90ea93440e5\n",
      "Untagged: minist_training_kf_pipeline:latest\n",
      "Untagged: gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline:latest\n",
      "Untagged: gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline@sha256:205c12a4cde07b4daa25d8867785b7f392cb5b08a66896df9aefd90ea93440e5\n",
      "Deleted: sha256:e28c0f74ace27cdf218d9e42bedfa38a86ecdc8f74a349a90ba0131a2b3021ce\n",
      "Deleted: sha256:405a2d72275337811b13eb7a2341d38cd7d0a26f4aa2800cd72b12606c45a1d4\n",
      "Deleted: sha256:056736b48ba831942e42175cd06f37952ce613f379b8bb60d7087143e96e36e1\n",
      "Deleted: sha256:994bd9e6f1464c7f7810772e48aaca8f00e4ae6294e18104af6e4eb490d216ab\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Build and push the image.\n",
    "cd tmp/reuse_components/minist_training\n",
    "bash build_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing your component definition file\n",
    "To create a component from your containerized program you need to write component specification in YAML format that describes the component for the Kubeflow Pipelines system.\n",
    "\n",
    "For the complete definition of a Kubeflow Pipelines component, see the [component specification](https://www.kubeflow.org/docs/pipelines/reference/component-spec/). However, for this tutorial you don’t need to know the full schema of the component specification. The tutorial provides enough information for the relevant the components.\n",
    "\n",
    "Start writing the component definition (component.yaml) by specifying your container image in the component’s implementation section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create Yaml\n",
    "\n",
    "cat > minist_component.yaml <<HERE\n",
    "name: Minist training\n",
    "description: Train a minist model and save to GCS\n",
    "inputs:\n",
    "  - name: model_file\n",
    "    description: 'Name of the model file.'\n",
    "    type: String\n",
    "  - name: bucket\n",
    "    description: 'GCS bucket name.'\n",
    "    type: String\n",
    "outputs:\n",
    "  - name: model_path\n",
    "    description: 'Trained model path.'\n",
    "    type: String\n",
    "implementation:\n",
    "  container:\n",
    "    image: gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline@sha256:205c12a4cde07b4daa25d8867785b7f392cb5b08a66896df9aefd90ea93440e5\n",
    "    command: [\n",
    "      python, /app/app.py,\n",
    "      --model_file, {inputValue: model_file},\n",
    "      --bucket,     {inputValue: bucket},\n",
    "    ]\n",
    "    fileOutputs:\n",
    "      model_path: /output.txt\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your workflow as a Python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your pipeline as a Python function. ` @kfp.dsl.pipeline` is a required decoration including `name` and `description` properties. Then compile the pipeline function. After the compilation is completed, a pipeline file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "minist_train_op = kfp.components.load_component_from_file(os.path.join('./', 'minist_component.yaml')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComponentSpec(name='Minist training', description='Train a minist model and save to GCS', metadata=None, inputs=[InputSpec(name='model_file', type='String', description='Name of the model file.', default=None, optional=False), InputSpec(name='bucket', type='String', description='GCS bucket name.', default=None, optional=False)], outputs=[OutputSpec(name='model_path', type='String', description='Trained model path.')], implementation=ContainerImplementation(container=ContainerSpec(image='gcr.io/kubeflow-pipeline-fantasy/minist_training_kf_pipeline@sha256:205c12a4cde07b4daa25d8867785b7f392cb5b08a66896df9aefd90ea93440e5', command=['python', '/app/app.py', '--model_file', InputValuePlaceholder(input_name='model_file'), '--bucket', InputValuePlaceholder(input_name='bucket')], args=None, env=None, file_outputs={'model_path': '/output.txt'})), version='google.com/cloud/pipelines/component/v1')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minist_train_op.component_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Minist pipeline',\n",
    "   description='A toy pipeline that performs minist model training.'\n",
    ")\n",
    "def minist_reuse_component_pipeline(\n",
    "    model_file: str = 'mnist_model.h5', \n",
    "    bucket: str = \"gs://kubeflow-pipeline-fantasy-kubeflow1-bucket\"\n",
    "):\n",
    "    minist_train_op(model_file=model_file, bucket=bucket).apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get or create an experiment and submit a pipeline run\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    host = \"https://kubeflow1.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline\"\n",
    "    client_id = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "    other_client_id = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "    other_client_secret = \"cB8Xj-rb9JWCYcCRDlpTMfhc\"\n",
    "    client = kfp.Client(host=host, \n",
    "                        client_id=client_id,\n",
    "                        other_client_id=other_client_id, \n",
    "                        other_client_secret=other_client_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow1.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/experiments/details/a1bdd769-68c9-42eb-a48b-f239f2f0a94e\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow1.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/runs/details/1d1c9cfd-719e-4e81-bdea-5e6b1c8f4fe3\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_func = minist_reuse_component_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "#Submit a pipeline run\n",
    "arguments = {\"model_file\":\"mnist_model.h5\",\n",
    "             \"bucket\":\"gs://kubeflow-pipeline-fantasy-kubeflow1-bucket\"}\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "experiment = client.create_experiment('python-functions-minist')\n",
    "\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [instructions](https://www.kubeflow.org/docs/other-guides/accessing-uis/) on kubeflow.org to access Kubeflow UIs. Upload the created pipeline and run it.\n",
    "\n",
    "**Warning:** When the pipeline is run, it pulls the image from the repository to the Kubernetes cluster to create a container. Kubernetes caches pulled images. One solution is to use the image digest instead of the tag in your component dsl, for example, `s/v1/sha256:9509182e27dcba6d6903fccf444dc6188709cc094a018d5dd4211573597485c9/g`. Alternatively, if you don't want to update the digest every time, you can try `:latest` tag, which will force the k8s to always pull the latest image.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualPython35",
   "language": "python",
   "name": "virtualpython35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
