{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local development and docker image components\n",
    "\n",
    "- This section assumes that you have already created a program to perform the task required in a particular step of your ML workflow. This example uses an MNIST model training script.\n",
    "\n",
    "- Then, this example packages your program as a Docker container image.\n",
    "\n",
    "- Then, this example calls kfp.components.ContainerOp to convert it to a Kubeflow pipeline component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Ensure that you have Docker installed, if you want to build the image locally, by running the following command:\n",
    " \n",
    "`which docker`\n",
    " \n",
    "The result should be something like:\n",
    "\n",
    "`/usr/bin/docker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './config/kubeflow-pipeline-fantasy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameter"
    ]
   },
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "PROJECT_ID='kubeflow-pipeline-fantasy'\n",
    "GCS_BUCKET='gs://kubeflow-pipeline-ptt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "If you run this notebook **outside** of a Kubeflow cluster, run the following command:\n",
    "- `host`: The URL of your Kubeflow Pipelines instance, for example \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- `client_id`: The client ID used by Identity-Aware Proxy\n",
    "- `other_client_id`: The client ID used to obtain the auth codes and refresh tokens.\n",
    "- `other_client_secret`: The client secret used to obtain the auth codes and refresh tokens.\n",
    "\n",
    "```python\n",
    "client = kfp.Client(host, client_id, other_client_id, other_client_secret)\n",
    "```\n",
    "\n",
    "If you run this notebook **within** a Kubeflow cluster, run the following command:\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```\n",
    "\n",
    "You'll need to create OAuth client ID credentials of type `Other` to get `other_client_id` and `other_client_secret`. Learn more about [creating OAuth credentials](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside Kubeflow cluster\n",
    "\n",
    "# # The host for full deployment of Kubeflow ends with '/pipeline'\n",
    "# HOST = 'https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline'\n",
    "# # Full deployment of Kubeflow on GCP is usually protected through IAP, therefore the following \n",
    "# # will be needed to access the endpoint\n",
    "# CLIENT_ID = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "# OTHER_CLIENT_ID = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "# OTHER_CLIENT_SECRET = \"cB8Xj-rb9JWCYcCRDlpTMfhc\"\n",
    "\n",
    "# The host for managed 'AI Platform Pipeline' ends with 'pipelines.googleusercontent.com'\n",
    "HOST = 'https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya29.c.KoAB2Aeb2BSyRb_xo_WewV61G-IqBbsmHHC1cNOh5OLRilh5TwlX45-iDQ6FSRCYVrlnaIGM8Gd8ZiL24WT-z5CR_M2e2wvP1hXB6FC7iKVv-ZWY0eaUWkwPuNty-WtnzTI_byUZnzf6-cnlESNys6Mje7d2hs89OlfdFFItawLznjw\n"
     ]
    }
   ],
   "source": [
    "# This is to ensure the proper access token is present to reach the end point for managed 'AI Platform Pipeline'\n",
    "# If you are not working with managed 'AI Platform Pipeline', this step is not necessary\n",
    "! gcloud auth print-access-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    if HOST.endswith('googleusercontent.com'):\n",
    "        CLIENT_ID = None\n",
    "        OTHER_CLIENT_ID = None\n",
    "        OTHER_CLIENT_SECRET = None\n",
    "\n",
    "    client = kfp.Client(host=HOST, \n",
    "                        client_id=CLIENT_ID,\n",
    "                        other_client_id=OTHER_CLIENT_ID, \n",
    "                        other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap an existing Docker container image using `ContainerOp`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the program code\n",
    "\n",
    "The following cell creates a file `app.py` that contains a Python script. The script downloads MNIST dataset, trains a Neural Network based classification model, writes the training log and exports the trained model to Google Cloud Storage.\n",
    "\n",
    "Your component can create outputs that the downstream components can use as inputs. Each output must be a string and the container image must write each output to a separate local text file. For example, if a training component needs to output the path of the trained model, the component writes the path into a local file, such as `/output.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/components/mnist_training\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/components/mnist_training/app.py <<HERE\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "gfile = tf.io.gfile\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--model_file', type=str, required=True, help='Name of the model file.')\n",
    "parser.add_argument(\n",
    "    '--bucket', type=str, required=True, help='GCS bucket name.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "bucket=args.bucket\n",
    "model_file=args.model_file\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())    \n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "callbacks = [\n",
    "  tf.keras.callbacks.TensorBoard(log_dir=bucket + '/logs/' + datetime.now().date().__str__()),\n",
    "  # Interrupt training if val_loss stops improving for over 2 epochs\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "]\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=5, callbacks=callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "metrics = {\n",
    "    'metrics': [\n",
    "        {\n",
    "            'name': 'accuracy', # The name of the metric. Visualized as the column name in the runs table.\n",
    "            'numberValue':  float(history.history['accuracy'][-1]), # The value of the metric. Must be a numeric value.\n",
    "            'format': \"PERCENTAGE\",\n",
    "        },\n",
    "        {\n",
    "            'name': 'val-accuracy', # The name of the metric. Visualized as the column name in the runs table.\n",
    "            'numberValue':  float(history.history['val_accuracy'][-1]), # The value of the metric. Must be a numeric value.\n",
    "            'format': \"PERCENTAGE\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "with open('/mlpipeline-metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f)\n",
    "\n",
    "model.save(model_file)\n",
    "\n",
    "gcs_path = bucket + \"/\" + model_file\n",
    "\n",
    "if gfile.exists(gcs_path):\n",
    "    gfile.remove(gcs_path)\n",
    "\n",
    "gfile.copy(model_file, gcs_path)\n",
    "with open('/output.txt', 'w') as f:\n",
    "  f.write(gcs_path)\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a container that runs the script. Start by creating a Dockerfile. A Dockerfile contains the instructions to assemble a Docker image. The `FROM` statement specifies the Base Image from which you are building. `WORKDIR` sets the working directory. When you assemble the Docker image, `COPY` copies the required files and directories (for example, `app.py`) to the file system of the container. `RUN` executes a command (for example, install the dependencies) and commits the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/components/mnist_training/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:2.1.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our Dockerfile for creating our Docker image. Then we need to push the image to a registry to host the image. \n",
    "- We are going to use Cloud Build to build the image and push to the Container Registry (GCR).\n",
    "- It is possible to build the `kfp.containers.build_image_from_working_dir` to build the image and push to the Container Registry (GCR), which uses [kaniko](https://cloud.google.com/blog/products/gcp/introducing-kaniko-build-container-images-in-kubernetes-and-google-container-builder-even-without-root-access).\n",
    "- It is possible to build the image locally using Docker and then to push it to GCR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "If you run this notebook **within Kubeflow cluster**, **with Kubeflow version >= 0.7**, you need to ensure that valid credentials are created within your notebook's namespace.\n",
    "- With Kubeflow version >= 0.7, the credential is supposed to be copied automatically while creating notebook through `Configurations`, which doesn't work properly at the time of creating this notebook. \n",
    "- You can also add credentials to the new namespace by either [copying credentials from an existing Kubeflow namespace, or by creating a new service account](https://www.kubeflow.org/docs/gke/authentication/#kubeflow-v0-6-and-before-gcp-service-account-key-as-secret).\n",
    "- The following cell demonstrates how to copy the default secret to your own namespace.\n",
    "\n",
    "```bash\n",
    "%%bash\n",
    "\n",
    "NAMESPACE=<your notebook name space>\n",
    "SOURCE=kubeflow\n",
    "NAME=user-gcp-sa\n",
    "SECRET=$(kubectl get secrets \\${NAME} -n \\${SOURCE} -o jsonpath=\"{.data.\\${NAME}\\.json}\" | base64 -D)\n",
    "kubectl create -n \\${NAMESPACE} secret generic \\${NAME} --from-literal=\"\\${NAME}.json=\\${SECRET}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"mnist_training_kf_pipeline\"\n",
    "TAG=\"latest\" # \"v_$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "GCR_IMAGE=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "APP_FOLDER='./tmp/components/mnist_training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"b53df073-55c5-494e-8b37-e9cf7645acb1\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597600022.4-a65bbbfc8e114d9895793a02808c92e0.tgz#1597600023280109\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597600022.4-a65bbbfc8e114d9895793a02808c92e0.tgz#1597600023280109...\n",
      "/ [1 files][  1.1 KiB/  1.1 KiB]                                                \n",
      "Operation completed over 1 objects/1.1 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "\n",
      "                   ***** NOTICE *****\n",
      "\n",
      "Alternative official `docker` images, including multiple versions across\n",
      "multiple platforms, are maintained by the Docker Team. For details, please\n",
      "visit https://hub.docker.com/_/docker.\n",
      "\n",
      "                ***** END OF NOTICE *****\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/3 : FROM tensorflow/tensorflow:2.1.0-py3\n",
      "2.1.0-py3: Pulling from tensorflow/tensorflow\n",
      "2746a4a261c9: Pulling fs layer\n",
      "4c1d20cdee96: Pulling fs layer\n",
      "0d3160e1d0de: Pulling fs layer\n",
      "c8e37668deea: Pulling fs layer\n",
      "e52cad4ccd83: Pulling fs layer\n",
      "e97116da5f98: Pulling fs layer\n",
      "75c61371a2e3: Pulling fs layer\n",
      "8592f093fc78: Pulling fs layer\n",
      "dccb0709d7fb: Pulling fs layer\n",
      "107f0b841886: Pulling fs layer\n",
      "edc69fe5c6be: Pulling fs layer\n",
      "c8e37668deea: Waiting\n",
      "e52cad4ccd83: Waiting\n",
      "e97116da5f98: Waiting\n",
      "75c61371a2e3: Waiting\n",
      "8592f093fc78: Waiting\n",
      "dccb0709d7fb: Waiting\n",
      "107f0b841886: Waiting\n",
      "edc69fe5c6be: Waiting\n",
      "0d3160e1d0de: Verifying Checksum\n",
      "0d3160e1d0de: Download complete\n",
      "4c1d20cdee96: Verifying Checksum\n",
      "4c1d20cdee96: Download complete\n",
      "2746a4a261c9: Verifying Checksum\n",
      "2746a4a261c9: Download complete\n",
      "c8e37668deea: Verifying Checksum\n",
      "c8e37668deea: Download complete\n",
      "e52cad4ccd83: Verifying Checksum\n",
      "e52cad4ccd83: Download complete\n",
      "75c61371a2e3: Verifying Checksum\n",
      "75c61371a2e3: Download complete\n",
      "8592f093fc78: Verifying Checksum\n",
      "8592f093fc78: Download complete\n",
      "107f0b841886: Verifying Checksum\n",
      "107f0b841886: Download complete\n",
      "edc69fe5c6be: Verifying Checksum\n",
      "edc69fe5c6be: Download complete\n",
      "e97116da5f98: Verifying Checksum\n",
      "e97116da5f98: Download complete\n",
      "dccb0709d7fb: Verifying Checksum\n",
      "dccb0709d7fb: Download complete\n",
      "2746a4a261c9: Pull complete\n",
      "4c1d20cdee96: Pull complete\n",
      "0d3160e1d0de: Pull complete\n",
      "c8e37668deea: Pull complete\n",
      "e52cad4ccd83: Pull complete\n",
      "e97116da5f98: Pull complete\n",
      "75c61371a2e3: Pull complete\n",
      "8592f093fc78: Pull complete\n",
      "dccb0709d7fb: Pull complete\n",
      "107f0b841886: Pull complete\n",
      "edc69fe5c6be: Pull complete\n",
      "Digest: sha256:14ec674cefd622aa9d45f07485500da254acaf8adfef80bd0f279db03c735689\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:2.1.0-py3\n",
      " ---> 53187075965b\n",
      "Step 2/3 : WORKDIR /app\n",
      " ---> Running in 72b34d522257\n",
      "Removing intermediate container 72b34d522257\n",
      " ---> 0f4dd8b4042f\n",
      "Step 3/3 : COPY . /app\n",
      " ---> a6ddeebbb001\n",
      "Successfully built a6ddeebbb001\n",
      "Successfully tagged gcr.io/kubeflow-pipeline-fantasy/mnist_training_kf_pipeline:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-pipeline-fantasy/mnist_training_kf_pipeline:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/mnist_training_kf_pipeline]\n",
      "3b3d12935b2b: Preparing\n",
      "71badfeb5be5: Preparing\n",
      "bbd58cb63302: Preparing\n",
      "fb8dafd6f834: Preparing\n",
      "b0f840119697: Preparing\n",
      "d3d0f89939b4: Preparing\n",
      "2f9b4f91799b: Preparing\n",
      "74bb2e59dcc5: Preparing\n",
      "0ea2b2c9ed16: Preparing\n",
      "918efb8f161b: Preparing\n",
      "27dd43ea46a8: Preparing\n",
      "9f3bfcc4a1a8: Preparing\n",
      "2dc9f76fb25b: Preparing\n",
      "d3d0f89939b4: Waiting\n",
      "2f9b4f91799b: Waiting\n",
      "74bb2e59dcc5: Waiting\n",
      "0ea2b2c9ed16: Waiting\n",
      "918efb8f161b: Waiting\n",
      "27dd43ea46a8: Waiting\n",
      "9f3bfcc4a1a8: Waiting\n",
      "2dc9f76fb25b: Waiting\n",
      "fb8dafd6f834: Layer already exists\n",
      "b0f840119697: Layer already exists\n",
      "bbd58cb63302: Layer already exists\n",
      "d3d0f89939b4: Layer already exists\n",
      "2f9b4f91799b: Layer already exists\n",
      "74bb2e59dcc5: Layer already exists\n",
      "0ea2b2c9ed16: Layer already exists\n",
      "918efb8f161b: Layer already exists\n",
      "27dd43ea46a8: Layer already exists\n",
      "9f3bfcc4a1a8: Layer already exists\n",
      "2dc9f76fb25b: Layer already exists\n",
      "71badfeb5be5: Pushed\n",
      "3b3d12935b2b: Pushed\n",
      "latest: digest: sha256:9dc0b9676d5dd4ae2c8e29ab72a0020ed59b309819744039a263c7600a77bd28 size: 3039\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                              IMAGES                                                                 STATUS\n",
      "b53df073-55c5-494e-8b37-e9cf7645acb1  2020-08-16T17:47:04+00:00  1M16S     gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597600022.4-a65bbbfc8e114d9895793a02808c92e0.tgz  gcr.io/kubeflow-pipeline-fantasy/mnist_training_kf_pipeline (+1 more)  SUCCESS\n",
      "gcr.io/kubeflow-pipeline-fantasy/mnist_training_kf_pipeline@sha256:9dc0b9676d5dd4ae2c8e29ab72a0020ed59b309819744039a263c7600a77bd28\n"
     ]
    }
   ],
   "source": [
    "if HOST.endswith('googleusercontent.com'):\n",
    "    # kaniko is not pre-installed with managed \"AI Platform Pipeline\"\n",
    "    import subprocess\n",
    "    # ! gcloud builds submit --tag ${IMAGE_NAME} ${APP_FOLDER}\n",
    "    cmd = ['gcloud', 'builds', 'submit', '--tag', GCR_IMAGE, APP_FOLDER]\n",
    "    build_log = (subprocess.run(cmd, stdout=subprocess.PIPE).stdout[:-1].decode('utf-8'))\n",
    "    print(build_log)\n",
    "\n",
    "    import re\n",
    "    m = re.search(r'latest: digest: sha256:.* size', build_log)\n",
    "    digest = m.group(0).split(' ')[2]\n",
    "    \n",
    "    image_name = \"gcr.io/{PROJECT_ID}/{IMAGE_NAME}@{DIGEST}\".format(\n",
    "        PROJECT_ID=PROJECT_ID,\n",
    "        IMAGE_NAME=IMAGE_NAME,\n",
    "        DIGEST=digest\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    if kfp.__version__ <= '0.1.36':\n",
    "        # kfp with version 0.1.36+ introduce broken change that will make the following code not working'\n",
    "        import subprocess\n",
    "        CLUSTER_NAME=''\n",
    "        ZONE=''\n",
    "        if CLUSTER_NAME and ZONE:\n",
    "            # ! gcloud container clusters get-credentials ${CLUSTER_NAME} --region ${ZONE}\n",
    "            cmd = ['gcloud', 'container', 'clusters', 'get-credentials', CLUSTER_NAME, ZONE]\n",
    "            update_kubeconfig= (subprocess.run(cmd, stdout=subprocess.PIPE).stdout[:-1].decode('utf-8'))\n",
    "            print(update_kubeconfig)\n",
    "        \n",
    "        builder = kfp.containers._container_builder.ContainerBuilder(\n",
    "            gcs_staging=GCS_BUCKET + \"/kfp_container_build_staging\"\n",
    "        )\n",
    "\n",
    "        image_name = kfp.containers.build_image_from_working_dir(\n",
    "            image_name=GCR_IMAGE,\n",
    "            working_dir=APP_FOLDER,\n",
    "            builder=builder\n",
    "        )\n",
    "    else:\n",
    "        raise(\"Please build the docker image use either [Docker] or [Cloud Build]\")\n",
    "\n",
    "print(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to use docker to build the image\n",
    "Run the following in a cell\n",
    "```bash\n",
    "%%bash -s \"{PROJECT_ID}\"\n",
    "\n",
    "IMAGE_NAME=\"mnist_training_kf_pipeline\"\n",
    "TAG=\"latest\" # \"v_$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "# Create script to build docker image and push it.\n",
    "cat > ./tmp/components/mnist_training/build_image.sh <<HERE\n",
    "PROJECT_ID=\"${1}\"\n",
    "IMAGE_NAME=\"${IMAGE_NAME}\"\n",
    "TAG=\"${TAG}\"\n",
    "GCR_IMAGE=\"gcr.io/\\${PROJECT_ID}/\\${IMAGE_NAME}:\\${TAG}\"\n",
    "docker build -t \\${IMAGE_NAME} .\n",
    "docker tag \\${IMAGE_NAME} \\${GCR_IMAGE}\n",
    "docker push \\${GCR_IMAGE}\n",
    "docker image rm \\${IMAGE_NAME}\n",
    "docker image rm \\${GCR_IMAGE}\n",
    "HERE\n",
    "\n",
    "cd tmp/components/mnist_training\n",
    "bash build_image.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define each component\n",
    "Define a component by creating an instance of `kfp.dsl.ContainerOp` that describes the interactions with the Docker container image created in the previous step. You need to specify \n",
    "- component name\n",
    "- the image to use\n",
    "- the command to run after the container starts (If None, uses default CMD in defined in container.)\n",
    "- the input arguments\n",
    "- the file outputs (In the `app.py` above, the path of the trained model is written to `/output.txt`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_train_op(model_file, bucket):\n",
    "    return dsl.ContainerOp(\n",
    "      name=\"mnist_training_container\",\n",
    "      image=image_name,\n",
    "      command=['python', '/app/app.py'],\n",
    "      file_outputs={\n",
    "          'outputs': '/output.txt',\n",
    "          'mlpipeline-metrics': '/mlpipeline-metrics.json'\n",
    "      },\n",
    "      arguments=['--bucket', bucket, '--model_file', model_file]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can use the kfp.dsl.component decorator to enable static type checking in the DSL compiler.\n",
    "```python\n",
    "@kfp.dsl.component\n",
    "def mnist_train_op(model_file: kfp.dsl.types.String(), \n",
    "                   bucket: kfp.dsl.types.String()) -> kfp.dsl.types.GCSPath():\n",
    "    return dsl.ContainerOp(\n",
    "      name=\"mnist_training_container\",\n",
    "      image='gcr.io/{}/mnist_training_kf_pipeline:latest'.format(PROJECT_ID),\n",
    "      command=['python', '/app/app.py'],\n",
    "      file_outputs={'outputs': '/output.txt'},\n",
    "      arguments=['--bucket', bucket, '--model_file', model_file]\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your workflow as a Python function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your pipeline as a Python function. ` @kfp.dsl.pipeline` is a required decoration including `name` and `description` properties. Then compile the pipeline function. After the compilation is completed, a pipeline file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Mnist pipeline',\n",
    "   description='A toy pipeline that performs mnist model training.'\n",
    ")\n",
    "def mnist_container_pipeline(\n",
    "    model_file: str = 'mnist_model.h5', \n",
    "    bucket: str = GCS_BUCKET\n",
    "):\n",
    "    mnist_train_op(model_file=model_file, bucket=bucket) # .apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = mnist_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com/#/experiments/details/69e9656d-5b14-4811-b3c3-d249cff65d70\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com/#/runs/details/cb389b47-82ba-4754-acf8-1fdfac6719e4\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'minist_kubeflow'\n",
    "\n",
    "arguments = {\"model_file\":\"mnist_model.h5\",\n",
    "             \"bucket\":GCS_BUCKET}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As an alternative, you can compile the pipeline into a package.** The compiled pipeline can be easily shared and reused by others to run the pipeline.\n",
    "\n",
    "```python\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "\n",
    "experiment = client.create_experiment('python-functions-mnist')\n",
    "\n",
    "run_result = client.run_pipeline(\n",
    "    experiment_id=experiment.id, \n",
    "    job_name=run_name, \n",
    "    pipeline_package_path=pipeline_filename, \n",
    "    params=arguments)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
