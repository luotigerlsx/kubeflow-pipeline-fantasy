{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and XGboost Pipeline\n",
    "\n",
    "This tutorial demonstrate building a machine learning pipeling with spark and XGBoost. The pipeline \n",
    "- starts by creating an Google DataProc cluster, and then running analysis, transformation, distributed training and prediction in the created cluster. \n",
    "- Then a single node confusion-matrix and ROC aggregator is used (for classification case) to provide the confusion matrix data, and ROC data to the front end, respectively. \n",
    "- Finally, a delete cluster operation runs to destroy the cluster it creates in the beginning. The delete cluster operation is used as an exit handler, meaning it will run regardless of whether the pipeline fails or not.\n",
    "\n",
    "**Please do not forget to enable the Dataproc API in your cluster** https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp import gcp\n",
    "import kfp.compiler as compiler\n",
    "\n",
    "import kubernetes as k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='kubeflow-pipeline-fantasy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_BUCKET='gs://kubeflow-pipeline-fantasy-kubeflow1-bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = \"https://kubeflow1.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline\"\n",
    "CLIENT_ID = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_ID = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_SECRET = \"cB8Xj-rb9JWCYcCRDlpTMfhc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "**If submit outside the kubeflow cluster, need the following**\n",
    "- host = \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- And, you'll first need to create OAuth client ID credentials of type `Other` according to the tutorial [here](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)\n",
    "\n",
    "**If you run and submit within the kubeflow cluster**, the following is enough\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get or create an experiment and submit a pipeline run\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    client = kfp.Client(host=HOST, \n",
    "                        client_id=CLIENT_ID,\n",
    "                        other_client_id=OTHER_CLIENT_ID, \n",
    "                        other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reusable components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/4e7e6e866c1256e641b0c3effc55438e6e4b30f6/components/local/confusion_matrix/component.yaml')\n",
    "\n",
    "roc_op = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/4e7e6e866c1256e641b0c3effc55438e6e4b30f6/components/local/roc/component.yaml')\n",
    "\n",
    "dataproc_create_cluster_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/4e7e6e866c1256e641b0c3effc55438e6e4b30f6/components/gcp/dataproc/create_cluster/component.yaml')\n",
    "\n",
    "dataproc_delete_cluster_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/4e7e6e866c1256e641b0c3effc55438e6e4b30f6/components/gcp/dataproc/delete_cluster/component.yaml')\n",
    "\n",
    "dataproc_submit_pyspark_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/4e7e6e866c1256e641b0c3effc55438e6e4b30f6/components/gcp/dataproc/submit_pyspark_job/component.yaml'\n",
    ")\n",
    "\n",
    "dataproc_submit_spark_op = components.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/4e7e6e866c1256e641b0c3effc55438e6e4b30f6/components/gcp/dataproc/submit_spark_job/component.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PYSRC_PREFIX = 'gs://ml-pipeline-playground/dataproc-example' # Common path to python src.\n",
    "\n",
    "_XGBOOST_PKG = 'gs://ml-pipeline-playground/xgboost4j-example-0.8-SNAPSHOT-jar-with-dependencies.jar'\n",
    "\n",
    "_TRAINER_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostTrainer'\n",
    "\n",
    "_PREDICTOR_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostPredictor'\n",
    "\n",
    "\n",
    "def delete_directory_from_gcs(dir_path):\n",
    "  \"\"\"Delete a GCS dir recursively. Ignore errors.\"\"\"\n",
    "  try:\n",
    "    subprocess.call(['gsutil', '-m', 'rm', '-r', dir_path])\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data analyze and transform operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc_analyze_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    schema,\n",
    "    train_data,\n",
    "    output):\n",
    "    \"\"\"Submit dataproc analyze as a pyspark job.\n",
    "    :param project: GCP project ID.\n",
    "    :param region: Which zone to run this analyze.\n",
    "    :param cluster_name: Name of the cluster.\n",
    "    :param schema: GCS path to the schema.\n",
    "    :param train_data: GCS path to the training data.\n",
    "    :param output: GCS path to store the output.\n",
    "    \"\"\"\n",
    "    return dataproc_submit_pyspark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_python_file_uri=os.path.join(_PYSRC_PREFIX, 'analyze_run.py'),\n",
    "      args=['--output', str(output), '--train', str(train_data), '--schema', str(schema)]\n",
    "    )\n",
    "\n",
    "\n",
    "def dataproc_transform_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    target,\n",
    "    analysis,\n",
    "    output\n",
    "):\n",
    "    \"\"\"Submit dataproc transform as a pyspark job.\n",
    "    :param project: GCP project ID.\n",
    "    :param region: Which zone to run this analyze.\n",
    "    :param cluster_name: Name of the cluster.\n",
    "    :param train_data: GCS path to the training data.\n",
    "    :param eval_data: GCS path of the eval csv file.\n",
    "    :param target: Target column name.\n",
    "    :param analysis: GCS path of the analysis results\n",
    "    :param output: GCS path to use for output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove existing [output]/train and [output]/eval if they exist.\n",
    "    delete_directory_from_gcs(os.path.join(output, 'train'))\n",
    "    delete_directory_from_gcs(os.path.join(output, 'eval'))\n",
    "\n",
    "    return dataproc_submit_pyspark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_python_file_uri=os.path.join(_PYSRC_PREFIX,\n",
    "                                        'transform_run.py'),\n",
    "      args=[\n",
    "        '--output',\n",
    "        str(output),\n",
    "        '--analysis',\n",
    "        str(analysis),\n",
    "        '--target',\n",
    "        str(target),\n",
    "        '--train',\n",
    "        str(train_data),\n",
    "        '--eval',\n",
    "        str(eval_data)\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and prediction operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc_train_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    target,\n",
    "    analysis,\n",
    "    workers,\n",
    "    rounds,\n",
    "    output,\n",
    "    is_classification=True\n",
    "):\n",
    "\n",
    "    if is_classification:\n",
    "        config='gs://ml-pipeline-playground/trainconfcla.json'\n",
    "    else:\n",
    "        config='gs://ml-pipeline-playground/trainconfreg.json'\n",
    "\n",
    "    return dataproc_submit_spark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_class=_TRAINER_MAIN_CLS,\n",
    "      spark_job=json.dumps({ 'jarFileUris': [_XGBOOST_PKG]}),\n",
    "      args=json.dumps([\n",
    "        str(config),\n",
    "        str(rounds),\n",
    "        str(workers),\n",
    "        str(analysis),\n",
    "        str(target),\n",
    "        str(train_data),\n",
    "        str(eval_data),\n",
    "        str(output)\n",
    "      ]))\n",
    "\n",
    "\n",
    "def dataproc_predict_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    data,\n",
    "    model,\n",
    "    target,\n",
    "    analysis,\n",
    "    output\n",
    "):\n",
    "\n",
    "    return dataproc_submit_spark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_class=_PREDICTOR_MAIN_CLS,\n",
    "      spark_job=json.dumps({ 'jarFileUris': [_XGBOOST_PKG]}),\n",
    "      args=json.dumps([\n",
    "        str(model),\n",
    "        str(data),\n",
    "        str(analysis),\n",
    "        str(target),\n",
    "        str(output)\n",
    "      ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='XGBoost Trainer',\n",
    "    description='A trainer that does end-to-end distributed training for XGBoost models.'\n",
    ")\n",
    "def xgb_train_pipeline(\n",
    "    output=GCS_BUCKET,\n",
    "    project=PROJECT_ID,\n",
    "    cluster_name='xgb-%s' % dsl.RUN_ID_PLACEHOLDER,\n",
    "    region='us-central1',\n",
    "    train_data='gs://ml-pipeline-playground/sfpd/train.csv',\n",
    "    eval_data='gs://ml-pipeline-playground/sfpd/eval.csv',\n",
    "    schema='gs://ml-pipeline-playground/sfpd/schema.json',\n",
    "    target='resolution',\n",
    "    rounds=200,\n",
    "    workers=2,\n",
    "    true_label='ACTION',\n",
    "):\n",
    "    output_template = str(output) + '/' + dsl.RUN_ID_PLACEHOLDER + '/data'\n",
    "\n",
    "    # Current GCP pyspark/spark op do not provide outputs as return values, instead,\n",
    "    # we need to use strings to pass the uri around.\n",
    "    analyze_output = output_template\n",
    "    transform_output_train = os.path.join(output_template, 'train', 'part-*')\n",
    "    transform_output_eval = os.path.join(output_template, 'eval', 'part-*')\n",
    "    train_output = os.path.join(output_template, 'train_output')\n",
    "    predict_output = os.path.join(output_template, 'predict_output')\n",
    "\n",
    "    with dsl.ExitHandler(exit_op=dataproc_delete_cluster_op(\n",
    "        project_id=project,\n",
    "        region=region,\n",
    "        name=cluster_name\n",
    "    )):\n",
    "        _create_cluster_op = dataproc_create_cluster_op(\n",
    "            project_id=project,\n",
    "            region=region,\n",
    "            name=cluster_name,\n",
    "            initialization_actions=[\n",
    "              os.path.join(_PYSRC_PREFIX,\n",
    "                           'initialization_actions.sh'),\n",
    "            ],\n",
    "            image_version='1.2'\n",
    "        )\n",
    "\n",
    "        _analyze_op = dataproc_analyze_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            schema=schema,\n",
    "            train_data=train_data,\n",
    "            output=output_template\n",
    "        ).after(_create_cluster_op).set_display_name('Analyzer')\n",
    "\n",
    "        _transform_op = dataproc_transform_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            train_data=train_data,\n",
    "            eval_data=eval_data,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            output=output_template\n",
    "        ).after(_analyze_op).set_display_name('Transformer')\n",
    "\n",
    "        _train_op = dataproc_train_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            train_data=transform_output_train,\n",
    "            eval_data=transform_output_eval,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            workers=workers,\n",
    "            rounds=rounds,\n",
    "            output=train_output\n",
    "        ).after(_transform_op).set_display_name('Trainer')\n",
    "\n",
    "        _predict_op = dataproc_predict_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            data=transform_output_eval,\n",
    "            model=train_output,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            output=predict_output\n",
    "        ).after(_train_op).set_display_name('Predictor')\n",
    "\n",
    "        _cm_op = confusion_matrix_op(\n",
    "            predictions=os.path.join(predict_output, 'part-*.csv'),\n",
    "            output_dir=output_template\n",
    "        ).after(_predict_op)\n",
    "\n",
    "        _roc_op = roc_op(\n",
    "            predictions_dir=os.path.join(predict_output, 'part-*.csv'),\n",
    "            true_class=true_label,\n",
    "            true_score_column=true_label,\n",
    "            output_dir=output_template\n",
    "        ).after(_predict_op)\n",
    "\n",
    "    dsl.get_pipeline_conf().add_op_transformer(\n",
    "        gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/kfp/components/_data_passing.py:133: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"200\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/kfp/components/_data_passing.py:133: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"2\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow1.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/experiments/details/c654c30e-f25b-4dc2-b5a8-5faf1b9cc333\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow1.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/runs/details/2ce50fa5-ea99-49e7-a887-8711dbf93431\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_func = xgb_train_pipeline\n",
    "pipeline_filename = pipeline_func.__name__ + '.pipeline.zip'\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func, pipeline_filename)\n",
    "#Submit a pipeline run\n",
    "arguments = {\"project\":PROJECT_ID,\n",
    "             \"output\": GCS_BUCKET}\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "experiment = client.create_experiment('Spark-and-XGBoost')\n",
    "\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualPython35",
   "language": "python",
   "name": "virtualpython35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
