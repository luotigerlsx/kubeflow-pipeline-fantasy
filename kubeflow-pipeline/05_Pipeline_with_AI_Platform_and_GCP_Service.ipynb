{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating model training and deployment with Kubeflow Pipelines (KFP) and Cloud AI Platform\n",
    "\n",
    "In this lab, you will develop, deploy, and run a KFP pipeline that orchestrates BigQuery and Cloud AI Platform services to train a scikit-learn model.\n",
    "\n",
    "The pipeline you develop in the lab orchestrates GCP managed services. The source data is in BigQuery. The pipeline uses:\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with KFP distribution:\n",
    "    - [BigQuery query component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/bigquery/query)\n",
    "    - [AI Platform Training component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/ml_engine/train)\n",
    "    - [AI Platform Deploy component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/ml_engine/deploy)\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's [Lightweight Python Components](https://www.kubeflow.org/docs/pipelines/sdk/lightweight-python-components/) mechanism. The code for the components is in the `helper_components.py` file:\n",
    "    - **Retrieve Best Run**. This component retrieves the tuning metric and hyperparameter values for the best run of the AI Platform Training hyperparameter tuning job.\n",
    "    - **Evaluate Model**. This component evaluates the *sklearn* trained model using a provided metric and a testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab dataset\n",
    "This lab uses the [Covertype Dat Set](https://archive.ics.uci.edu/ml/datasets/covertype). The pipeline developed in the lab sources the dataset from BigQuery. Before proceeding with the lab upload the dataset to BigQuery:\n",
    "\n",
    "1. Open new terminal in you **JupyterLab**\n",
    "\n",
    "2. Create the BigQuery dataset and upload the Cover Type csv file.\n",
    "\n",
    "```\n",
    "export PROJECT_ID=$(gcloud config get-value core/project)\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://workshop-datasets/covertype/full/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "from kfp.dsl import types\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s\n",
    "\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './config/kubeflow-pipeline-fantasy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "PROJECT_ID='kubeflow-pipeline-fantasy'\n",
    "GCS_BUCKET='gs://kubeflow-pipeline-ptt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX=PROJECT_ID\n",
    "NAMESPACE='kubeflow'\n",
    "AIP_REGION='us-central1'\n",
    "AIP_ZONE='us-central1-a'\n",
    "GCS_STAGING_PATH='{}/staging'.format(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "If you run this notebook **outside** of a Kubeflow cluster, run the following command:\n",
    "- `host`: The URL of your Kubeflow Pipelines instance, for example \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- `client_id`: The client ID used by Identity-Aware Proxy\n",
    "- `other_client_id`: The client ID used to obtain the auth codes and refresh tokens.\n",
    "- `other_client_secret`: The client secret used to obtain the auth codes and refresh tokens.\n",
    "\n",
    "```python\n",
    "client = kfp.Client(host, client_id, other_client_id, other_client_secret)\n",
    "```\n",
    "\n",
    "If you run this notebook **within** a Kubeflow cluster, run the following command:\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```\n",
    "\n",
    "You'll need to create OAuth client ID credentials of type `Other` to get `other_client_id` and `other_client_secret`. Learn more about [creating OAuth credentials](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside Kubeflow cluster\n",
    "\n",
    "# # The host for full deployment of Kubeflow ends with '/pipeline'\n",
    "# HOST = 'https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline'\n",
    "# # Full deployment of Kubeflow on GCP is usually protected through IAP, therefore the following \n",
    "# # will be needed to access the endpoint\n",
    "# CLIENT_ID = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "# OTHER_CLIENT_ID = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "# OTHER_CLIENT_SECRET = \"cB8Xj-rb9JWCYcCRDlpTMfhc\"\n",
    "\n",
    "# The host for managed 'AI Platform Pipeline' ends with 'pipelines.googleusercontent.com'\n",
    "HOST = 'https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya29.c.KoAB2AfaFyB1A2Yraw_SJEhh1diW8Yr7eLYdwM5sXSf0uHVm2zLtUTfzdBVStp-aD5_hdl168_rUapuQGnxEeWMPTx1RlrG_CpheUBTOZM5s_yYj-JuAh3E0Yl2xpjTIIQCLezsD8yqABDVZBgQ5SNpl2ZHZ2gpF5VKW93xeG4Tg01U\n"
     ]
    }
   ],
   "source": [
    "# This is to ensure the proper access token is present to reach the end point for managed 'AI Platform Pipeline'\n",
    "# If you are not working with managed 'AI Platform Pipeline', this step is not necessary\n",
    "! gcloud auth print-access-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    if HOST.endswith('googleusercontent.com'):\n",
    "        CLIENT_ID = None\n",
    "        OTHER_CLIENT_ID = None\n",
    "        OTHER_CLIENT_SECRET = None\n",
    "\n",
    "    client = kfp.Client(host=HOST, \n",
    "                        client_id=CLIENT_ID,\n",
    "                        other_client_id=OTHER_CLIENT_ID, \n",
    "                        other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(\n",
    "    project_id: str, \n",
    "    job_id: str\n",
    ") -> NamedTuple('Outputs', [('metric_value', float), ('alpha', float),\n",
    "                            ('max_iter', int)]):\n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "\n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "    alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "    max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "    return (metric_value, alpha, max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    dataset_path: str, model_path: str, metric_name: str\n",
    ") -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    from tensorflow import gfile\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Cover_Type', axis=1)\n",
    "    y_test = df_test['Cover_Type']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.pkl'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    \n",
    "    if gfile.Exists(model_filename):\n",
    "        gfile.Remove(model_filename)\n",
    "\n",
    "    gfile.Copy(gcs_model_filepath, model_filename)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the program code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a file `train.py` that contains a Python script. The script use the Covertype Data Set to develop a multi-class classification model that predicts the type of forest cover from cartographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/aip_pipeline/covertype_training\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/aip_pipeline/covertype_training/train.py <<HERE\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from tensorflow import gfile\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path,\n",
    "                   alpha, max_iter, hptune):\n",
    "    with gfile.Open(training_dataset_path, 'r') as f:\n",
    "        # Assume there is no header\n",
    "        df_train = pd.read_csv(f)\n",
    "\n",
    "    with gfile.Open(validation_dataset_path, 'r') as f:\n",
    "        # Assume there is no header\n",
    "        df_validation = pd.read_csv(f)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_feature_indexes),\n",
    "            ('cat', OneHotEncoder(), categorical_feature_indexes)\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log'))\n",
    "    ])\n",
    "\n",
    "    num_features_type_map = {feature: 'float64' for feature in\n",
    "                             df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map)\n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "        y_validation = df_validation['Cover_Type']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='accuracy',\n",
    "            metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "\n",
    "        model_filename = 'model.pkl'\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "\n",
    "        if gfile.Exists(gcs_model_path):\n",
    "            gfile.Remove(gcs_model_path)\n",
    "\n",
    "        with gfile.Open(gcs_model_path, 'w') as wf:\n",
    "            pickle.dump(pipeline, wf)\n",
    "\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker container\n",
    "Create your own container image that includes your program. Now create a container that runs the script. Start by creating a Dockerfile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p tmp/aip_pipeline/covertype_base\n",
    "\n",
    "cat > ./tmp/aip_pipeline/covertype_base/requirements.txt <<EOF\n",
    "fire\n",
    "cloudml-hypertune\n",
    "gcsfs\n",
    "pandas==0.24.0\n",
    "google-api-python-client==1.7.8\n",
    "joblib==0.13.0\n",
    "scikit-learn==0.20.2\n",
    "EOF\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/aip_pipeline/covertype_base/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "RUN pip install -r requirements.txt\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "\n",
    "BASE_IMAGE_URI=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "BASE_APP_FOLDER='./tmp/aip_pipeline/covertype_base/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 211 bytes before compression.\n",
      "Uploading tarball of [./tmp/aip_pipeline/covertype_base/] to [gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597426528.38-b8e745fc69f240339115355e3c4b60b5.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-pipeline-fantasy/builds/0063c724-034e-4170-89db-e8409058745e].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/0063c724-034e-4170-89db-e8409058745e?project=493831447550].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"0063c724-034e-4170-89db-e8409058745e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597426528.38-b8e745fc69f240339115355e3c4b60b5.tgz#1597426529528883\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597426528.38-b8e745fc69f240339115355e3c4b60b5.tgz#1597426529528883...\n",
      "/ [1 files][  317.0 B/  317.0 B]                                                \n",
      "Operation completed over 1 objects/317.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "\n",
      "                   ***** NOTICE *****\n",
      "\n",
      "Alternative official `docker` images, including multiple versions across\n",
      "multiple platforms, are maintained by the Docker Team. For details, please\n",
      "visit https://hub.docker.com/_/docker.\n",
      "\n",
      "                ***** END OF NOTICE *****\n",
      "\n",
      "Sending build context to Docker daemon  3.072kB\n",
      "Step 1/4 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Pulling fs layer\n",
      "14ca88e9f672: Pulling fs layer\n",
      "a31c3b1caad4: Pulling fs layer\n",
      "b054a26005b7: Pulling fs layer\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "b054a26005b7: Waiting\n",
      "8832e3773578: Waiting\n",
      "5e671b828b2a: Waiting\n",
      "2b940936f993: Waiting\n",
      "016724bbd2c9: Waiting\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "14ca88e9f672: Verifying Checksum\n",
      "14ca88e9f672: Download complete\n",
      "a31c3b1caad4: Verifying Checksum\n",
      "a31c3b1caad4: Download complete\n",
      "5b7339215d1d: Verifying Checksum\n",
      "5b7339215d1d: Download complete\n",
      "b054a26005b7: Verifying Checksum\n",
      "b054a26005b7: Download complete\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "5b7339215d1d: Pull complete\n",
      "14ca88e9f672: Pull complete\n",
      "a31c3b1caad4: Pull complete\n",
      "b054a26005b7: Pull complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/4 : WORKDIR /app\n",
      " ---> Running in c139b11745f1\n",
      "Removing intermediate container c139b11745f1\n",
      " ---> 2a114b8c8837\n",
      "Step 3/4 : COPY . /app\n",
      " ---> 299447fec91c\n",
      "Step 4/4 : RUN pip install -r requirements.txt\n",
      " ---> Running in 6e768305f3d7\n",
      "Collecting fire (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
      "Collecting cloudml-hypertune (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Collecting gcsfs (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/5c/bc61dbd2e5b61d84486a96a64ca43512c9ac085487464562182f58406290/gcsfs-0.6.2-py2.py3-none-any.whl\n",
      "Collecting pandas==0.24.0 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/e1/4a63ed31e1b1362d40ce845a5735c717a959bda992669468dae3420af2cd/pandas-0.24.0-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "Collecting google-api-python-client==1.7.8 (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/55/e9/e8fb2e3a031cb69b9524b80a92b126665d9a17421700a219555e3233ab6a/google_api_python_client-1.7.8-py3-none-any.whl (56kB)\n",
      "Collecting joblib==0.13.0 (from -r requirements.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/1b/995167f6c66848d4eb7eabc386aebe07a1571b397629b2eac3b7bebdc343/joblib-0.13.0-py2.py3-none-any.whl (276kB)\n",
      "Collecting scikit-learn==0.20.2 (from -r requirements.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/3a/b92670f5c368c20329ecc4c255993fae7934564d485c3ed7ea7b8da7f741/scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->-r requirements.txt (line 1)) (1.1.0)\n",
      "Collecting decorator (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting requests (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
      "Collecting fsspec>=0.6.0 (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/66/974e01194980d9780cc09724315111f9cccba26b4351552fdb4d97eb842e/fsspec-0.8.0-py3-none-any.whl (85kB)\n",
      "Collecting google-auth-oauthlib (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting google-auth>=1.2 (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/79/4c59796bb02535aee5e5d2e2c5e16008aaf48903c2ec2ff566a2774bb3e0/google_auth-1.20.1-py2.py3-none-any.whl (91kB)\n",
      "Collecting pytz>=2011k (from pandas==0.24.0->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\n",
      "Collecting python-dateutil>=2.5.0 (from pandas==0.24.0->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.0->-r requirements.txt (line 4)) (1.16.4)\n",
      "Collecting httplib2<1dev,>=0.9.2 (from google-api-python-client==1.7.8->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/ad/d9d9331850ea5bd4f5cb8c650c0bfa119a4abd6b0ad7c45b6506bc979fc0/httplib2-0.18.1-py3-none-any.whl (95kB)\n",
      "Collecting google-auth-httplib2>=0.0.3 (from google-api-python-client==1.7.8->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/4e/992849016f8b0c27fb604aafd0a7a724db16128906197bd1245c6f18e6a1/google_auth_httplib2-0.0.4-py2.py3-none-any.whl\n",
      "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client==1.7.8->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/0c/60d82c077998feb631608dca3cc1fe19ac074e772bf0c24cf409b977b815/uritemplate-3.0.1-py2.py3-none-any.whl\n",
      "Collecting scipy>=0.13.3 (from scikit-learn==0.20.2->-r requirements.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/a8/f4c66eb529bb252d50e83dbf2909c6502e2f857550f22571ed8556f62d95/scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
      "Collecting chardet<4,>=3.0.2 (from requests->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->gcsfs->-r requirements.txt (line 3)) (2.6)\n",
      "Collecting certifi>=2017.4.17 (from requests->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\" (from google-auth>=1.2->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth>=1.2->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs->-r requirements.txt (line 3)) (41.0.1)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.2->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth>=1.2->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: fire, cloudml-hypertune, decorator, chardet, certifi, urllib3, requests, fsspec, oauthlib, requests-oauthlib, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-oauthlib, gcsfs, pytz, python-dateutil, pandas, httplib2, google-auth-httplib2, uritemplate, google-api-python-client, joblib, scipy, scikit-learn\n",
      "Successfully installed cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 cloudml-hypertune-0.1.0.dev6 decorator-4.4.2 fire-0.3.1 fsspec-0.8.0 gcsfs-0.6.2 google-api-python-client-1.7.8 google-auth-1.20.1 google-auth-httplib2-0.0.4 google-auth-oauthlib-0.4.1 httplib2-0.18.1 joblib-0.13.0 oauthlib-3.1.0 pandas-0.24.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.1 pytz-2020.1 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scikit-learn-0.20.2 scipy-1.5.2 uritemplate-3.0.1 urllib3-1.25.10\n",
      "\u001b[91mWARNING: You are using pip version 19.1.1, however version 20.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 6e768305f3d7\n",
      " ---> 7e6c9ef0e7a9\n",
      "Successfully built 7e6c9ef0e7a9\n",
      "Successfully tagged gcr.io/kubeflow-pipeline-fantasy/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-pipeline-fantasy/base_image:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/base_image]\n",
      "59994943fd9c: Preparing\n",
      "d87d92dda3d2: Preparing\n",
      "262082ba234d: Preparing\n",
      "a144de6e67e6: Preparing\n",
      "4b8ec9124f1c: Preparing\n",
      "652cdcb17d30: Preparing\n",
      "dd7f77c80a16: Preparing\n",
      "f4cb77175ac9: Preparing\n",
      "31835e84bcc0: Preparing\n",
      "75e70aa52609: Preparing\n",
      "dda151859818: Preparing\n",
      "fbd2732ad777: Preparing\n",
      "ba9de9d8475e: Preparing\n",
      "652cdcb17d30: Waiting\n",
      "dd7f77c80a16: Waiting\n",
      "f4cb77175ac9: Waiting\n",
      "31835e84bcc0: Waiting\n",
      "75e70aa52609: Waiting\n",
      "dda151859818: Waiting\n",
      "fbd2732ad777: Waiting\n",
      "ba9de9d8475e: Waiting\n",
      "a144de6e67e6: Layer already exists\n",
      "4b8ec9124f1c: Layer already exists\n",
      "652cdcb17d30: Layer already exists\n",
      "dd7f77c80a16: Layer already exists\n",
      "31835e84bcc0: Layer already exists\n",
      "f4cb77175ac9: Layer already exists\n",
      "75e70aa52609: Layer already exists\n",
      "dda151859818: Layer already exists\n",
      "ba9de9d8475e: Layer already exists\n",
      "fbd2732ad777: Layer already exists\n",
      "262082ba234d: Pushed\n",
      "d87d92dda3d2: Pushed\n",
      "59994943fd9c: Pushed\n",
      "latest: digest: sha256:0f36b50bc7fc254a031ca0ca421b61a3440ae3d23b3b5a3d0e010e90bac8e1b8 size: 3038\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                                 STATUS\n",
      "0063c724-034e-4170-89db-e8409058745e  2020-08-14T17:35:30+00:00  1M23S     gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597426528.38-b8e745fc69f240339115355e3c4b60b5.tgz  gcr.io/kubeflow-pipeline-fantasy/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --tag $BASE_IMAGE_URI $BASE_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p tmp/aip_pipeline/covertype_training\n",
    "\n",
    "cat > ./tmp/aip_pipeline/covertype_training/requirements.txt <<EOF\n",
    "fire\n",
    "cloudml-hypertune\n",
    "gcsfs\n",
    "pandas==0.24.0\n",
    "google-api-python-client==1.7.8\n",
    "joblib==0.13.0\n",
    "scikit-learn==0.20.2\n",
    "EOF\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/aip_pipeline/covertype_training/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='traing_image'\n",
    "TAG='latest'\n",
    "\n",
    "TRAIN_IMAGE_URI=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "TRAIN_APP_FOLDER='./tmp/aip_pipeline/covertype_training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 2.8 KiB before compression.\n",
      "Uploading tarball of [./tmp/aip_pipeline/covertype_training/] to [gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597426616.35-f8bc287a0cfb46f080d927844d6cf743.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-pipeline-fantasy/builds/4e1155c1-a6c3-4239-8174-712d63f22b39].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/4e1155c1-a6c3-4239-8174-712d63f22b39?project=493831447550].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4e1155c1-a6c3-4239-8174-712d63f22b39\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597426616.35-f8bc287a0cfb46f080d927844d6cf743.tgz#1597426617490558\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597426616.35-f8bc287a0cfb46f080d927844d6cf743.tgz#1597426617490558...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "\n",
      "                   ***** NOTICE *****\n",
      "\n",
      "Alternative official `docker` images, including multiple versions across\n",
      "multiple platforms, are maintained by the Docker Team. For details, please\n",
      "visit https://hub.docker.com/_/docker.\n",
      "\n",
      "                ***** END OF NOTICE *****\n",
      "\n",
      "Sending build context to Docker daemon  6.656kB\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Pulling fs layer\n",
      "14ca88e9f672: Pulling fs layer\n",
      "a31c3b1caad4: Pulling fs layer\n",
      "b054a26005b7: Pulling fs layer\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "b054a26005b7: Waiting\n",
      "8832e3773578: Waiting\n",
      "5e671b828b2a: Waiting\n",
      "2b940936f993: Waiting\n",
      "016724bbd2c9: Waiting\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "a31c3b1caad4: Verifying Checksum\n",
      "a31c3b1caad4: Download complete\n",
      "14ca88e9f672: Verifying Checksum\n",
      "14ca88e9f672: Download complete\n",
      "5b7339215d1d: Verifying Checksum\n",
      "5b7339215d1d: Download complete\n",
      "b054a26005b7: Verifying Checksum\n",
      "b054a26005b7: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "5b7339215d1d: Pull complete\n",
      "14ca88e9f672: Pull complete\n",
      "a31c3b1caad4: Pull complete\n",
      "b054a26005b7: Pull complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/5 : WORKDIR /app\n",
      " ---> Running in 786b362978ce\n",
      "Removing intermediate container 786b362978ce\n",
      " ---> 0ff1c6c2c1fd\n",
      "Step 3/5 : COPY . /app\n",
      " ---> 98778fe9254a\n",
      "Step 4/5 : RUN pip install -r requirements.txt\n",
      " ---> Running in f44e4d35628d\n",
      "Collecting fire (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/34/a7/0e22e70778aca01a52b9c899d9c145c6396d7b613719cd63db97ffa13f2f/fire-0.3.1.tar.gz (81kB)\n",
      "Collecting cloudml-hypertune (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Collecting gcsfs (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/5c/bc61dbd2e5b61d84486a96a64ca43512c9ac085487464562182f58406290/gcsfs-0.6.2-py2.py3-none-any.whl\n",
      "Collecting pandas==0.24.0 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/e1/4a63ed31e1b1362d40ce845a5735c717a959bda992669468dae3420af2cd/pandas-0.24.0-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "Collecting google-api-python-client==1.7.8 (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/55/e9/e8fb2e3a031cb69b9524b80a92b126665d9a17421700a219555e3233ab6a/google_api_python_client-1.7.8-py3-none-any.whl (56kB)\n",
      "Collecting joblib==0.13.0 (from -r requirements.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/1b/995167f6c66848d4eb7eabc386aebe07a1571b397629b2eac3b7bebdc343/joblib-0.13.0-py2.py3-none-any.whl (276kB)\n",
      "Collecting scikit-learn==0.20.2 (from -r requirements.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/3a/b92670f5c368c20329ecc4c255993fae7934564d485c3ed7ea7b8da7f741/scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->-r requirements.txt (line 1)) (1.1.0)\n",
      "Collecting fsspec>=0.6.0 (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/66/974e01194980d9780cc09724315111f9cccba26b4351552fdb4d97eb842e/fsspec-0.8.0-py3-none-any.whl (85kB)\n",
      "Collecting requests (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
      "Collecting decorator (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting google-auth-oauthlib (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting google-auth>=1.2 (from gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/79/4c59796bb02535aee5e5d2e2c5e16008aaf48903c2ec2ff566a2774bb3e0/google_auth-1.20.1-py2.py3-none-any.whl (91kB)\n",
      "Collecting python-dateutil>=2.5.0 (from pandas==0.24.0->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas==0.24.0->-r requirements.txt (line 4)) (1.16.4)\n",
      "Collecting pytz>=2011k (from pandas==0.24.0->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\n",
      "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client==1.7.8->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/0c/60d82c077998feb631608dca3cc1fe19ac074e772bf0c24cf409b977b815/uritemplate-3.0.1-py2.py3-none-any.whl\n",
      "Collecting google-auth-httplib2>=0.0.3 (from google-api-python-client==1.7.8->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/4e/992849016f8b0c27fb604aafd0a7a724db16128906197bd1245c6f18e6a1/google_auth_httplib2-0.0.4-py2.py3-none-any.whl\n",
      "Collecting httplib2<1dev,>=0.9.2 (from google-api-python-client==1.7.8->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/ad/d9d9331850ea5bd4f5cb8c650c0bfa119a4abd6b0ad7c45b6506bc979fc0/httplib2-0.18.1-py3-none-any.whl (95kB)\n",
      "Collecting scipy>=0.13.3 (from scikit-learn==0.20.2->-r requirements.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/2b/a8/f4c66eb529bb252d50e83dbf2909c6502e2f857550f22571ed8556f62d95/scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->gcsfs->-r requirements.txt (line 3)) (2.6)\n",
      "Collecting chardet<4,>=3.0.2 (from requests->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2->gcsfs->-r requirements.txt (line 3)) (41.0.1)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.2->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth>=1.2->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\" (from google-auth>=1.2->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/61/df/768b03527bf006b546dce284eb4249b185669e65afc5fbb2ac\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "Successfully built fire cloudml-hypertune\n",
      "Installing collected packages: fire, cloudml-hypertune, fsspec, urllib3, chardet, certifi, requests, decorator, oauthlib, requests-oauthlib, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, google-auth-oauthlib, gcsfs, python-dateutil, pytz, pandas, uritemplate, httplib2, google-auth-httplib2, google-api-python-client, joblib, scipy, scikit-learn\n",
      "Successfully installed cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 cloudml-hypertune-0.1.0.dev6 decorator-4.4.2 fire-0.3.1 fsspec-0.8.0 gcsfs-0.6.2 google-api-python-client-1.7.8 google-auth-1.20.1 google-auth-httplib2-0.0.4 google-auth-oauthlib-0.4.1 httplib2-0.18.1 joblib-0.13.0 oauthlib-3.1.0 pandas-0.24.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.1 pytz-2020.1 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scikit-learn-0.20.2 scipy-1.5.2 uritemplate-3.0.1 urllib3-1.25.10\n",
      "\u001b[91mWARNING: You are using pip version 19.1.1, however version 20.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container f44e4d35628d\n",
      " ---> 4c3ee8d1d7d8\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 3328f30a5e22\n",
      "Removing intermediate container 3328f30a5e22\n",
      " ---> e24bfae7b645\n",
      "Successfully built e24bfae7b645\n",
      "Successfully tagged gcr.io/kubeflow-pipeline-fantasy/traing_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-pipeline-fantasy/traing_image:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/traing_image]\n",
      "272458016023: Preparing\n",
      "4bc0fd3d17ca: Preparing\n",
      "6f4c8ec0ce92: Preparing\n",
      "a144de6e67e6: Preparing\n",
      "4b8ec9124f1c: Preparing\n",
      "652cdcb17d30: Preparing\n",
      "dd7f77c80a16: Preparing\n",
      "f4cb77175ac9: Preparing\n",
      "31835e84bcc0: Preparing\n",
      "75e70aa52609: Preparing\n",
      "dda151859818: Preparing\n",
      "fbd2732ad777: Preparing\n",
      "ba9de9d8475e: Preparing\n",
      "652cdcb17d30: Waiting\n",
      "dd7f77c80a16: Waiting\n",
      "f4cb77175ac9: Waiting\n",
      "31835e84bcc0: Waiting\n",
      "75e70aa52609: Waiting\n",
      "dda151859818: Waiting\n",
      "fbd2732ad777: Waiting\n",
      "ba9de9d8475e: Waiting\n",
      "4b8ec9124f1c: Layer already exists\n",
      "a144de6e67e6: Layer already exists\n",
      "652cdcb17d30: Layer already exists\n",
      "dd7f77c80a16: Layer already exists\n",
      "f4cb77175ac9: Layer already exists\n",
      "31835e84bcc0: Layer already exists\n",
      "75e70aa52609: Layer already exists\n",
      "dda151859818: Layer already exists\n",
      "4bc0fd3d17ca: Pushed\n",
      "6f4c8ec0ce92: Pushed\n",
      "fbd2732ad777: Layer already exists\n",
      "ba9de9d8475e: Layer already exists\n",
      "272458016023: Pushed\n",
      "latest: digest: sha256:5fa1e607aae82758c7957fd704b3eab3d5f2b85b84fe3843fbcade524b7f0b0c size: 3039\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                               IMAGES                                                   STATUS\n",
      "4e1155c1-a6c3-4239-8174-712d63f22b39  2020-08-14T17:36:58+00:00  1M48S     gs://kubeflow-pipeline-fantasy_cloudbuild/source/1597426616.35-f8bc287a0cfb46f080d927844d6cf743.tgz  gcr.io/kubeflow-pipeline-fantasy/traing_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --tag $TRAIN_IMAGE_URI $TRAIN_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_URL_SEARCH_PREFIX='https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'\n",
    "# COMPONENT_URL_SEARCH_PREFIX='https://raw.githubusercontent.com/kubeflow/pipelines/3f4b80127f35e40760eeb1813ce1d3f641502222/components/gcp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_best_run_op = comp.func_to_container_op(retrieve_best_run, base_image=BASE_IMAGE_URI)\n",
    "evaluate_model_op = comp.func_to_container_op(evaluate_model, base_image=BASE_IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define deployment operation on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "    \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "    sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}` AS cover\n",
    "       WHERE \n",
    "       MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "       \"\"\"\n",
    "    query = Template(sampling_query_template).render(\n",
    "      source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_pipeline(\n",
    "    project_id,\n",
    "    region,\n",
    "    gcs_root,\n",
    "    source_table_name: str,\n",
    "    dataset_id: str,\n",
    "    evaluation_metric_name: str,\n",
    "    evaluation_metric_threshold: float,\n",
    "    model_id: str,\n",
    "    replace_existing_version: bool,\n",
    "    hypertune_settings = HYPERTUNE_SETTINGS,\n",
    "    dataset_location = 'US'\n",
    "):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "    training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "    create_training_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=training_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the validation split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "    validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "    create_validation_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=validation_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "    testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "    create_testing_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=testing_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                              kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAIN_IMAGE_URI,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAIN_IMAGE_URI,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "            model_uri=train_model.outputs['job_dir'],\n",
    "            project_id=project_id,\n",
    "            model_id=model_id,\n",
    "            runtime_version=\"1.14\",\n",
    "            python_version=\"3.5\",\n",
    "            replace_existing_version=True, \n",
    "            set_default=True)\n",
    "\n",
    "    kfp.dsl.get_pipeline_conf() # .add_op_transformer(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = covertype_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com/#/experiments/details/234211b9-abb0-46e2-b8a9-019cb0a6c3d8\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com/#/runs/details/9dbaee65-1638-4e2a-b6ed-82650780c6be\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'covertype_kubeflow'\n",
    "\n",
    "arguments = {\n",
    "    'project_id': PROJECT_ID,\n",
    "    'gcs_root': GCS_STAGING_PATH,\n",
    "    'region': AIP_REGION,\n",
    "    'source_table_name': 'covertype_dataset.covertype',\n",
    "    'dataset_id': 'splits',\n",
    "    'evaluation_metric_name': 'accuracy',\n",
    "    'evaluation_metric_threshold': 0.69,\n",
    "    'model_id': 'covertype_classifier',\n",
    "    'replace_existing_version': True\n",
    "}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Given `penalty`, (aka regularization term) to be used, is a parameter of SGDClassifier, the possible value can be set is {l2, l1, elasticnet}, default=l2.\n",
    "\n",
    "- Add `penalty` as the third hyperparameter besides `alpha` and `max_iter`.\n",
    "- Add the same pipeline as in this tutorial and find the best set of hyperparameter.\n",
    "- You may find the reference for setting categorical hyperparameter [here](https://cloud.google.com/ai-platform/training/docs/using-hyperparameter-tuning).\n",
    "\n",
    "Hints:\n",
    "- Add `penalty` as an additional parameter in `train_evaluate` function\n",
    "- Modify `HYPERTUNE_SETTINGS` to include `penalty` in the hyperparameter tuning process\n",
    "- Modify `retrieve_best_run_op` to retrieve best `penalty` as well\n",
    "- Modify `train_args` to pass the selected `penalty` to final model training as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
