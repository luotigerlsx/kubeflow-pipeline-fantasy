{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating model training and deployment with Kubeflow Pipelines (KFP) and Cloud AI Platform\n",
    "\n",
    "In this lab, you will develop, deploy, and run a KFP pipeline that orchestrates BigQuery and Cloud AI Platform services to train a scikit-learn model.\n",
    "\n",
    "The pipeline you develop in the lab orchestrates GCP managed services. The source data is in BigQuery. The pipeline uses:\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with KFP distribution:\n",
    "    - [BigQuery query component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/bigquery/query)\n",
    "    - [AI Platform Training component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/ml_engine/train)\n",
    "    - [AI Platform Deploy component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/ml_engine/deploy)\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's [Lightweight Python Components](https://www.kubeflow.org/docs/pipelines/sdk/lightweight-python-components/) mechanism. The code for the components is in the `helper_components.py` file:\n",
    "    - **Retrieve Best Run**. This component retrieves the tuning metric and hyperparameter values for the best run of the AI Platform Training hyperparameter tuning job.\n",
    "    - **Evaluate Model**. This component evaluates the *sklearn* trained model using a provided metric and a testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab dataset\n",
    "This lab uses the [Covertype Dat Set](https://archive.ics.uci.edu/ml/datasets/covertype). The pipeline developed in the lab sources the dataset from BigQuery. Before proceeding with the lab upload the dataset to BigQuery:\n",
    "\n",
    "1. Open new terminal in you **JupyterLab**\n",
    "\n",
    "2. Create the BigQuery dataset and upload the Cover Type csv file.\n",
    "\n",
    "```\n",
    "export PROJECT_ID=$(gcloud config get-value core/project)\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://workshop-datasets/covertype/full/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "from kfp.dsl import types\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s\n",
    "\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/luoshixin/LocalDevelop/kubeflow-pipeline/kubeflow-pipeline/kubeflow-pipeline-fantasy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "PROJECT_ID='kubeflow-pipeline-fantasy'\n",
    "GCS_BUCKET='gs://kubeflow-pipeline-ui'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX=PROJECT_ID\n",
    "NAMESPACE='kubeflow'\n",
    "AIP_REGION='us-central1'\n",
    "AIP_ZONE='us-central1-a'\n",
    "GCS_STAGING_PATH='{}/staging'.format(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "If you run this notebook **outside** of a Kubeflow cluster, run the following command:\n",
    "- `host`: The URL of your Kubeflow Pipelines instance, for example \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- `client_id`: The client ID used by Identity-Aware Proxy\n",
    "- `other_client_id`: The client ID used to obtain the auth codes and refresh tokens.\n",
    "- `other_client_secret`: The client secret used to obtain the auth codes and refresh tokens.\n",
    "\n",
    "```python\n",
    "client = kfp.Client(host, client_id, other_client_id, other_client_secret)\n",
    "```\n",
    "\n",
    "If you run this notebook **within** a Kubeflow cluster, run the following command:\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```\n",
    "\n",
    "You'll need to create OAuth client ID credentials of type `Other` to get `other_client_id` and `other_client_secret`. Learn more about [creating OAuth credentials](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside Kubeflow cluster\n",
    "HOST = 'https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline'\n",
    "# HOST = 'https://7c021d0340d296aa-dot-us-central2.pipelines.googleusercontent.com'\n",
    "CLIENT_ID = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_ID = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_SECRET = \"cB8Xj-rb9JWCYcCRDlpTMfhc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    if HOST.endswith('.com'):\n",
    "        client = kfp.Client(host=HOST)\n",
    "    else:\n",
    "        client = kfp.Client(host=HOST, \n",
    "                            client_id=CLIENT_ID,\n",
    "                            other_client_id=OTHER_CLIENT_ID, \n",
    "                            other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(\n",
    "    project_id: str, job_id: str\n",
    ") -> NamedTuple('Outputs', [('metric_value', float), ('alpha', float),\n",
    "                            ('max_iter', int)]):\n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "\n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "    alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "    max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "    return (metric_value, alpha, max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    dataset_path: str, model_path: str, metric_name: str\n",
    ") -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    #import joblib\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    from tensorflow import gfile\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Cover_Type', axis=1)\n",
    "    y_test = df_test['Cover_Type']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.pkl'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    \n",
    "    if gfile.Exists(model_filename):\n",
    "        gfile.Remove(model_filename)\n",
    "\n",
    "    gfile.Copy(gcs_model_filepath, model_filename)\n",
    "#     subprocess.check_call(['gsutil', 'cp', gcs_model_filepath, model_filename],\n",
    "#                         stderr=sys.stdout)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the program code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a file `train.py` that contains a Python script. The script use the Covertype Data Set to develop a multi-class classification model that predicts the type of forest cover from cartographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/aip_pipeline/covertype_training\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/aip_pipeline/covertype_training/train.py <<HERE\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "    \n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_feature_indexes),\n",
    "            ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log'))\n",
    "        ])\n",
    "    \n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "  \n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "  \n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "        y_validation = df_validation['Cover_Type']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        from tensorflow import gfile\n",
    "        model_filename = 'model.pkl'\n",
    "        with open(model_filename, 'wb') as model_file:\n",
    "            pickle.dump(pipeline, model_file)\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "\n",
    "        if gfile.Exists(gcs_model_path):\n",
    "            gfile.Remove(gcs_model_path)\n",
    "\n",
    "        gfile.Copy(model_filename, gcs_model_path)\n",
    "        \n",
    "#         subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path], stderr=sys.stdout)\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker container\n",
    "Create your own container image that includes your program. Now create a container that runs the script. Start by creating a Dockerfile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p tmp/aip_pipeline/covertype_base\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/aip_pipeline/covertype_base/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.15.0-py3\n",
    "RUN pip install -U pandas google-api-python-client scikit-learn gcsfs\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "\n",
    "BASE_IMAGE_URI=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "BASE_APP_FOLDER='./tmp/aip_pipeline/covertype_base/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 1 file(s) totalling 108 bytes before compression.\n",
      "Uploading tarball of [./tmp/aip_pipeline/covertype_base/] to [gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584349090.757195-6193082347a6413ab5c033b95c4c73dc.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-pipeline-fantasy/builds/fb84eaa4-033b-4a72-bb22-a2b8bef7f71f].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/fb84eaa4-033b-4a72-bb22-a2b8bef7f71f?project=493831447550].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"fb84eaa4-033b-4a72-bb22-a2b8bef7f71f\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584349090.757195-6193082347a6413ab5c033b95c4c73dc.tgz#1584349091765597\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584349090.757195-6193082347a6413ab5c033b95c4c73dc.tgz#1584349091765597...\n",
      "/ [1 files][  240.0 B/  240.0 B]                                                \n",
      "Operation completed over 1 objects/240.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  2.048kB\n",
      "Step 1/2 : FROM tensorflow/tensorflow:1.15.0-py3\n",
      "1.15.0-py3: Pulling from tensorflow/tensorflow\n",
      "22e816666fd6: Pulling fs layer\n",
      "079b6d2a1e53: Pulling fs layer\n",
      "11048ebae908: Pulling fs layer\n",
      "c58094023a2e: Pulling fs layer\n",
      "fb153ade6d14: Pulling fs layer\n",
      "0db149060649: Pulling fs layer\n",
      "138c908b7d99: Pulling fs layer\n",
      "094a8f5dd2cb: Pulling fs layer\n",
      "354ee6535f23: Pulling fs layer\n",
      "f5de9bda32bd: Pulling fs layer\n",
      "ac66bd508eff: Pulling fs layer\n",
      "c58094023a2e: Waiting\n",
      "fb153ade6d14: Waiting\n",
      "0db149060649: Waiting\n",
      "138c908b7d99: Waiting\n",
      "094a8f5dd2cb: Waiting\n",
      "354ee6535f23: Waiting\n",
      "f5de9bda32bd: Waiting\n",
      "ac66bd508eff: Waiting\n",
      "079b6d2a1e53: Verifying Checksum\n",
      "079b6d2a1e53: Download complete\n",
      "11048ebae908: Verifying Checksum\n",
      "11048ebae908: Download complete\n",
      "22e816666fd6: Verifying Checksum\n",
      "22e816666fd6: Download complete\n",
      "c58094023a2e: Verifying Checksum\n",
      "c58094023a2e: Download complete\n",
      "fb153ade6d14: Verifying Checksum\n",
      "fb153ade6d14: Download complete\n",
      "138c908b7d99: Verifying Checksum\n",
      "138c908b7d99: Download complete\n",
      "094a8f5dd2cb: Verifying Checksum\n",
      "094a8f5dd2cb: Download complete\n",
      "f5de9bda32bd: Verifying Checksum\n",
      "f5de9bda32bd: Download complete\n",
      "0db149060649: Verifying Checksum\n",
      "0db149060649: Download complete\n",
      "ac66bd508eff: Verifying Checksum\n",
      "ac66bd508eff: Download complete\n",
      "22e816666fd6: Pull complete\n",
      "354ee6535f23: Verifying Checksum\n",
      "354ee6535f23: Download complete\n",
      "079b6d2a1e53: Pull complete\n",
      "11048ebae908: Pull complete\n",
      "c58094023a2e: Pull complete\n",
      "fb153ade6d14: Pull complete\n",
      "0db149060649: Pull complete\n",
      "138c908b7d99: Pull complete\n",
      "094a8f5dd2cb: Pull complete\n",
      "354ee6535f23: Pull complete\n",
      "f5de9bda32bd: Pull complete\n",
      "ac66bd508eff: Pull complete\n",
      "Digest: sha256:ac8457b32f65b68ee2421eabaea70919b373ada9746d46a6d1566d66c75ff719\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.15.0-py3\n",
      " ---> f24a5ca8605f\n",
      "Step 2/2 : RUN pip install -U pandas google-api-python-client scikit-learn gcsfs\n",
      " ---> Running in 8121a4ac8ed5\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/b9/9ad570258ce4fe504bd23002154f9e6f09bf7110359d271e4ba1664f7281/pandas-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "Collecting google-api-python-client\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/d8/312e03adf4c78663e17d802fe2440072376fee46cada1404f1727ed77a32/scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
      "Collecting gcsfs\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/9f/864a9ff497ed4ba12502c4037db8c66fde0049d9dd0388bd55b67e5c4249/gcsfs-0.6.0-py2.py3-none-any.whl\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.3)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Collecting httplib2<1dev,>=0.9.2\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/4b/025a7338bb2d4a2c61f0e530b79aafc29d112ed8e61333a6dd9ba48f3bab/httplib2-0.17.0-py3-none-any.whl (95kB)\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/lib/python3/dist-packages (from google-api-python-client) (1.11.0)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/0c/60d82c077998feb631608dca3cc1fe19ac074e772bf0c24cf409b977b815/uritemplate-3.0.1-py2.py3-none-any.whl\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Collecting google-api-core<2dev,>=1.13.0\n",
      "  Downloading https://files.pythonhosted.org/packages/63/7e/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\n",
      "Collecting google-auth>=1.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl (76kB)\n",
      "Collecting scipy>=0.17.0\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "Collecting decorator\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/1f/7028dacd3c28f34ce48130aae73a88fa5cc27b6b0e494fcf2739f7954d9d/fsspec-0.6.2-py3-none-any.whl (62kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client) (3.10.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf/googleapis-common-protos-1.51.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client) (41.4.0)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->gcsfs) (2.6)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting pyasn1>=0.1.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Building wheels for collected packages: googleapis-common-protos\n",
      "  Building wheel for googleapis-common-protos (setup.py): started\n",
      "  Building wheel for googleapis-common-protos (setup.py): finished with status 'done'\n",
      "  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-cp36-none-any.whl size=74529 sha256=8bbbc87451afa32ac2bf2c70ce1fbc4b230b90429adde4ad7efc22343943eb0d\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f9/7f/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\n",
      "Successfully built googleapis-common-protos\n",
      "Installing collected packages: pytz, python-dateutil, pandas, httplib2, uritemplate, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, google-auth-httplib2, urllib3, certifi, chardet, requests, googleapis-common-protos, google-api-core, google-api-python-client, scipy, joblib, scikit-learn, decorator, fsspec, oauthlib, requests-oauthlib, google-auth-oauthlib, gcsfs\n",
      "Successfully installed cachetools-4.0.0 certifi-2019.11.28 chardet-3.0.4 decorator-4.4.2 fsspec-0.6.2 gcsfs-0.6.0 google-api-core-1.16.0 google-api-python-client-1.8.0 google-auth-1.11.3 google-auth-httplib2-0.0.3 google-auth-oauthlib-0.4.1 googleapis-common-protos-1.51.0 httplib2-0.17.0 joblib-0.14.1 oauthlib-3.1.0 pandas-1.0.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.2.post1 scipy-1.4.1 uritemplate-3.0.1 urllib3-1.25.8\n",
      "\u001b[91mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 8121a4ac8ed5\n",
      " ---> 74a3aa597bfc\n",
      "Successfully built 74a3aa597bfc\n",
      "Successfully tagged gcr.io/kubeflow-pipeline-fantasy/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-pipeline-fantasy/base_image:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/base_image]\n",
      "a6110a5ae22a: Preparing\n",
      "84c3bc63b701: Preparing\n",
      "56ec85ad394c: Preparing\n",
      "aefe991487a2: Preparing\n",
      "4a58ecdd995f: Preparing\n",
      "fa9f3f4bd775: Preparing\n",
      "2bf9e296738e: Preparing\n",
      "92486bede3ce: Preparing\n",
      "19331eff40f0: Preparing\n",
      "100ef12ce3a4: Preparing\n",
      "97e6b67a30f1: Preparing\n",
      "a090697502b8: Preparing\n",
      "fa9f3f4bd775: Waiting\n",
      "2bf9e296738e: Waiting\n",
      "92486bede3ce: Waiting\n",
      "19331eff40f0: Waiting\n",
      "100ef12ce3a4: Waiting\n",
      "97e6b67a30f1: Waiting\n",
      "a090697502b8: Waiting\n",
      "aefe991487a2: Layer already exists\n",
      "56ec85ad394c: Layer already exists\n",
      "84c3bc63b701: Layer already exists\n",
      "4a58ecdd995f: Layer already exists\n",
      "2bf9e296738e: Layer already exists\n",
      "92486bede3ce: Layer already exists\n",
      "fa9f3f4bd775: Layer already exists\n",
      "100ef12ce3a4: Layer already exists\n",
      "a090697502b8: Layer already exists\n",
      "19331eff40f0: Layer already exists\n",
      "97e6b67a30f1: Layer already exists\n",
      "a6110a5ae22a: Pushed\n",
      "latest: digest: sha256:8721b84c2550273883b81fe780f36b079ca075172cd20158547789c963f40cdf size: 2837\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                   IMAGES                                                 STATUS\n",
      "fb84eaa4-033b-4a72-bb22-a2b8bef7f71f  2020-03-16T08:58:12+00:00  1M53S     gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584349090.757195-6193082347a6413ab5c033b95c4c73dc.tgz  gcr.io/kubeflow-pipeline-fantasy/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --tag $BASE_IMAGE_URI $BASE_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/aip_pipeline/covertype_training/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.15.0-py3\n",
    "RUN pip install -U fire cloudml-hypertune pandas google-api-python-client scikit-learn gcsfs\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='traing_image'\n",
    "TAG='latest'\n",
    "\n",
    "TRAIN_IMAGE_URI=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "TRAIN_APP_FOLDER='./tmp/aip_pipeline/covertype_training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 2.8 KiB before compression.\n",
      "Uploading tarball of [./tmp/aip_pipeline/covertype_training/] to [gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584349213.128568-a2126532131e4f59b67ee5524924db90.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-pipeline-fantasy/builds/dee87275-641c-4e4c-b569-915dca4ff019].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/dee87275-641c-4e4c-b569-915dca4ff019?project=493831447550].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"dee87275-641c-4e4c-b569-915dca4ff019\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584349213.128568-a2126532131e4f59b67ee5524924db90.tgz#1584349214152815\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584349213.128568-a2126532131e4f59b67ee5524924db90.tgz#1584349214152815...\n",
      "/ [1 files][  1.3 KiB/  1.3 KiB]                                                \n",
      "Operation completed over 1 objects/1.3 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  5.632kB\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.15.0-py3\n",
      "1.15.0-py3: Pulling from tensorflow/tensorflow\n",
      "22e816666fd6: Pulling fs layer\n",
      "079b6d2a1e53: Pulling fs layer\n",
      "11048ebae908: Pulling fs layer\n",
      "c58094023a2e: Pulling fs layer\n",
      "fb153ade6d14: Pulling fs layer\n",
      "0db149060649: Pulling fs layer\n",
      "138c908b7d99: Pulling fs layer\n",
      "094a8f5dd2cb: Pulling fs layer\n",
      "354ee6535f23: Pulling fs layer\n",
      "f5de9bda32bd: Pulling fs layer\n",
      "ac66bd508eff: Pulling fs layer\n",
      "c58094023a2e: Waiting\n",
      "fb153ade6d14: Waiting\n",
      "0db149060649: Waiting\n",
      "138c908b7d99: Waiting\n",
      "094a8f5dd2cb: Waiting\n",
      "354ee6535f23: Waiting\n",
      "f5de9bda32bd: Waiting\n",
      "ac66bd508eff: Waiting\n",
      "079b6d2a1e53: Verifying Checksum\n",
      "079b6d2a1e53: Download complete\n",
      "11048ebae908: Verifying Checksum\n",
      "11048ebae908: Download complete\n",
      "22e816666fd6: Verifying Checksum\n",
      "22e816666fd6: Download complete\n",
      "c58094023a2e: Verifying Checksum\n",
      "c58094023a2e: Download complete\n",
      "fb153ade6d14: Verifying Checksum\n",
      "fb153ade6d14: Download complete\n",
      "138c908b7d99: Verifying Checksum\n",
      "138c908b7d99: Download complete\n",
      "094a8f5dd2cb: Verifying Checksum\n",
      "094a8f5dd2cb: Download complete\n",
      "f5de9bda32bd: Verifying Checksum\n",
      "f5de9bda32bd: Download complete\n",
      "0db149060649: Verifying Checksum\n",
      "0db149060649: Download complete\n",
      "ac66bd508eff: Verifying Checksum\n",
      "ac66bd508eff: Download complete\n",
      "354ee6535f23: Verifying Checksum\n",
      "354ee6535f23: Download complete\n",
      "22e816666fd6: Pull complete\n",
      "079b6d2a1e53: Pull complete\n",
      "11048ebae908: Pull complete\n",
      "c58094023a2e: Pull complete\n",
      "fb153ade6d14: Pull complete\n",
      "0db149060649: Pull complete\n",
      "138c908b7d99: Pull complete\n",
      "094a8f5dd2cb: Pull complete\n",
      "354ee6535f23: Pull complete\n",
      "f5de9bda32bd: Pull complete\n",
      "ac66bd508eff: Pull complete\n",
      "Digest: sha256:ac8457b32f65b68ee2421eabaea70919b373ada9746d46a6d1566d66c75ff719\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.15.0-py3\n",
      " ---> f24a5ca8605f\n",
      "Step 2/5 : RUN pip install -U fire cloudml-hypertune pandas google-api-python-client scikit-learn gcsfs\n",
      " ---> Running in 68efffcc9d8f\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/b9/9ad570258ce4fe504bd23002154f9e6f09bf7110359d271e4ba1664f7281/pandas-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "Collecting google-api-python-client\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/d8/312e03adf4c78663e17d802fe2440072376fee46cada1404f1727ed77a32/scikit_learn-0.22.2.post1-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
      "Collecting gcsfs\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/9f/864a9ff497ed4ba12502c4037db8c66fde0049d9dd0388bd55b67e5c4249/gcsfs-0.6.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/lib/python3/dist-packages (from fire) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from fire) (1.1.0)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.3)\n",
      "Collecting httplib2<1dev,>=0.9.2\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/4b/025a7338bb2d4a2c61f0e530b79aafc29d112ed8e61333a6dd9ba48f3bab/httplib2-0.17.0-py3-none-any.whl (95kB)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Collecting google-auth>=1.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl (76kB)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/0c/60d82c077998feb631608dca3cc1fe19ac074e772bf0c24cf409b977b815/uritemplate-3.0.1-py2.py3-none-any.whl\n",
      "Collecting google-api-core<2dev,>=1.13.0\n",
      "  Downloading https://files.pythonhosted.org/packages/63/7e/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "Collecting scipy>=0.17.0\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
      "Collecting decorator\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/1f/7028dacd3c28f34ce48130aae73a88fa5cc27b6b0e494fcf2739f7954d9d/fsspec-0.6.2-py3-none-any.whl (62kB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client) (41.4.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client) (3.10.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf/googleapis-common-protos-1.51.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->gcsfs) (2.6)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, googleapis-common-protos\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=104738 sha256=c2fa9e358d1060b23d8548a7f809816a8fdd4907488832cf6c9d2635ddafa966\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=5583 sha256=791a5709bf50189aed2066c0f1776e9357c41356ead2d7742d7a63d78ba69da5\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "  Building wheel for googleapis-common-protos (setup.py): started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for googleapis-common-protos (setup.py): finished with status 'done'\n",
      "  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-cp36-none-any.whl size=74529 sha256=e2000d3e7de9c7b491914026036f94c4a4eda9df69654eb09df50abd24ef36a4\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f9/7f/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\n",
      "Successfully built fire cloudml-hypertune googleapis-common-protos\n",
      "Installing collected packages: fire, cloudml-hypertune, python-dateutil, pytz, pandas, httplib2, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-httplib2, uritemplate, certifi, chardet, urllib3, requests, googleapis-common-protos, google-api-core, google-api-python-client, joblib, scipy, scikit-learn, decorator, oauthlib, requests-oauthlib, google-auth-oauthlib, fsspec, gcsfs\n",
      "Successfully installed cachetools-4.0.0 certifi-2019.11.28 chardet-3.0.4 cloudml-hypertune-0.1.0.dev6 decorator-4.4.2 fire-0.2.1 fsspec-0.6.2 gcsfs-0.6.0 google-api-core-1.16.0 google-api-python-client-1.8.0 google-auth-1.11.3 google-auth-httplib2-0.0.3 google-auth-oauthlib-0.4.1 googleapis-common-protos-1.51.0 httplib2-0.17.0 joblib-0.14.1 oauthlib-3.1.0 pandas-1.0.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.22.2.post1 scipy-1.4.1 uritemplate-3.0.1 urllib3-1.25.8\n",
      "\u001b[91mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 68efffcc9d8f\n",
      " ---> b1546b5fffa2\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in 4bce0a1db1a2\n",
      "Removing intermediate container 4bce0a1db1a2\n",
      " ---> 5144f7c2f1b8\n",
      "Step 4/5 : COPY . /app\n",
      " ---> de540a1879ca\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 8f113cc0904b\n",
      "Removing intermediate container 8f113cc0904b\n",
      " ---> 1c6c52d35cca\n",
      "Successfully built 1c6c52d35cca\n",
      "Successfully tagged gcr.io/kubeflow-pipeline-fantasy/traing_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-pipeline-fantasy/traing_image:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/traing_image]\n",
      "845c6546a507: Preparing\n",
      "98e49b7dea76: Preparing\n",
      "9288abc69327: Preparing\n",
      "84c3bc63b701: Preparing\n",
      "56ec85ad394c: Preparing\n",
      "aefe991487a2: Preparing\n",
      "4a58ecdd995f: Preparing\n",
      "fa9f3f4bd775: Preparing\n",
      "2bf9e296738e: Preparing\n",
      "92486bede3ce: Preparing\n",
      "19331eff40f0: Preparing\n",
      "100ef12ce3a4: Preparing\n",
      "97e6b67a30f1: Preparing\n",
      "a090697502b8: Preparing\n",
      "aefe991487a2: Waiting\n",
      "4a58ecdd995f: Waiting\n",
      "fa9f3f4bd775: Waiting\n",
      "2bf9e296738e: Waiting\n",
      "92486bede3ce: Waiting\n",
      "19331eff40f0: Waiting\n",
      "100ef12ce3a4: Waiting\n",
      "97e6b67a30f1: Waiting\n",
      "a090697502b8: Waiting\n",
      "84c3bc63b701: Layer already exists\n",
      "56ec85ad394c: Layer already exists\n",
      "4a58ecdd995f: Layer already exists\n",
      "aefe991487a2: Layer already exists\n",
      "2bf9e296738e: Layer already exists\n",
      "fa9f3f4bd775: Layer already exists\n",
      "92486bede3ce: Layer already exists\n",
      "19331eff40f0: Layer already exists\n",
      "100ef12ce3a4: Layer already exists\n",
      "97e6b67a30f1: Layer already exists\n",
      "a090697502b8: Layer already exists\n",
      "98e49b7dea76: Pushed\n",
      "845c6546a507: Pushed\n",
      "9288abc69327: Pushed\n",
      "latest: digest: sha256:22498710dbaac11ec2f8e4eeb99d032d8b34ed0874e2588f7daad1e355a6b94c size: 3252\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                   IMAGES                                                   STATUS\n",
      "dee87275-641c-4e4c-b569-915dca4ff019  2020-03-16T09:00:14+00:00  1M52S     gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584349213.128568-a2126532131e4f59b67ee5524924db90.tgz  gcr.io/kubeflow-pipeline-fantasy/traing_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --tag $TRAIN_IMAGE_URI $TRAIN_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_URL_SEARCH_PREFIX='https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'\n",
    "RUNTIME_VERSION='1.15'\n",
    "PYTHON_VERSION='3.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_best_run_op = comp.func_to_container_op(retrieve_best_run, base_image=BASE_IMAGE_URI)\n",
    "evaluate_model_op = comp.func_to_container_op(evaluate_model, base_image=BASE_IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define deployment operation on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "    \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "    sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}` AS cover\n",
    "       WHERE \n",
    "       MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "       \"\"\"\n",
    "    query = Template(sampling_query_template).render(\n",
    "      source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_pipeline(\n",
    "    project_id: types.GCPProjectID,\n",
    "    region: types.GCPRegion,\n",
    "    source_table_name: types.String,\n",
    "    gcs_root: types.GCSPath,\n",
    "    dataset_id: str,\n",
    "    evaluation_metric_name: str,\n",
    "    evaluation_metric_threshold: float,\n",
    "    model_id: str,\n",
    "    version_id: str,\n",
    "    replace_existing_version: bool,\n",
    "    hypertune_settings: types.Dict = HYPERTUNE_SETTINGS,\n",
    "    dataset_location: str = 'US'\n",
    "):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "    training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "    create_training_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=training_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the validation split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "    validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "    create_validation_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=validation_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "    testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "    create_testing_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=testing_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                              kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAIN_IMAGE_URI,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAIN_IMAGE_URI,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "            model_uri=train_model.outputs['job_dir'],\n",
    "            project_id=project_id,\n",
    "            model_id=model_id,\n",
    "            version_id=version_id,\n",
    "            runtime_version=RUNTIME_VERSION,\n",
    "            python_version=PYTHON_VERSION,\n",
    "            replace_existing_version=replace_existing_version)\n",
    "\n",
    "    kfp.dsl.get_pipeline_conf().add_op_transformer(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = covertype_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/experiments/details/5068b555-c9ee-42f0-89a2-e3eb1be8f382\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/runs/details/80035050-18c6-444a-bc6b-454144c0eec5\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'covertype_kubeflow'\n",
    "\n",
    "arguments = {\n",
    "    'project_id': PROJECT_ID,\n",
    "    'gcs_root': GCS_STAGING_PATH,\n",
    "    'region': AIP_REGION,\n",
    "    'source_table_name': 'covertype_dataset.covertype',\n",
    "    'dataset_id': 'splits',\n",
    "    'evaluation_metric_name': 'accuracy',\n",
    "    'evaluation_metric_threshold': 0.69,\n",
    "    'model_id': 'covertype_classifier',\n",
    "    'version_id': 'v01',\n",
    "    'replace_existing_version': True\n",
    "}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualPython37PT",
   "language": "python",
   "name": "virtualpython37pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
