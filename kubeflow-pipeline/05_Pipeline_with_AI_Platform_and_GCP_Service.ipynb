{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating model training and deployment with Kubeflow Pipelines (KFP) and Cloud AI Platform\n",
    "\n",
    "In this lab, you will develop, deploy, and run a KFP pipeline that orchestrates BigQuery and Cloud AI Platform services to train a scikit-learn model.\n",
    "\n",
    "The pipeline you develop in the lab orchestrates GCP managed services. The source data is in BigQuery. The pipeline uses:\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with KFP distribution:\n",
    "    - [BigQuery query component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/bigquery/query)\n",
    "    - [AI Platform Training component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/ml_engine/train)\n",
    "    - [AI Platform Deploy component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/ml_engine/deploy)\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's [Lightweight Python Components](https://www.kubeflow.org/docs/pipelines/sdk/lightweight-python-components/) mechanism. The code for the components is in the `helper_components.py` file:\n",
    "    - **Retrieve Best Run**. This component retrieves the tuning metric and hyperparameter values for the best run of the AI Platform Training hyperparameter tuning job.\n",
    "    - **Evaluate Model**. This component evaluates the *sklearn* trained model using a provided metric and a testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab dataset\n",
    "This lab uses the [Covertype Dat Set](https://archive.ics.uci.edu/ml/datasets/covertype). The pipeline developed in the lab sources the dataset from BigQuery. Before proceeding with the lab upload the dataset to BigQuery:\n",
    "\n",
    "1. Open new terminal in you **JupyterLab**\n",
    "\n",
    "2. Create the BigQuery dataset and upload the Cover Type csv file.\n",
    "\n",
    "```\n",
    "export PROJECT_ID=$(gcloud config get-value core/project)\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://workshop-datasets/covertype/full/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "from kfp.dsl import types\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s\n",
    "\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/luoshixin/LocalDevelop/kubeflow-pipeline/kubeflow-pipeline/kubeflow-pipeline-fantasy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "PROJECT_ID='kubeflow-pipeline-fantasy'\n",
    "GCS_BUCKET='gs://kubeflow-pipeline-ui'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX=PROJECT_ID\n",
    "NAMESPACE='kubeflow'\n",
    "AIP_REGION='us-central1'\n",
    "AIP_ZONE='us-central1-a'\n",
    "GCS_STAGING_PATH='{}/staging'.format(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "If you run this notebook **outside** of a Kubeflow cluster, run the following command:\n",
    "- `host`: The URL of your Kubeflow Pipelines instance, for example \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- `client_id`: The client ID used by Identity-Aware Proxy\n",
    "- `other_client_id`: The client ID used to obtain the auth codes and refresh tokens.\n",
    "- `other_client_secret`: The client secret used to obtain the auth codes and refresh tokens.\n",
    "\n",
    "```python\n",
    "client = kfp.Client(host, client_id, other_client_id, other_client_secret)\n",
    "```\n",
    "\n",
    "If you run this notebook **within** a Kubeflow cluster, run the following command:\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```\n",
    "\n",
    "You'll need to create OAuth client ID credentials of type `Other` to get `other_client_id` and `other_client_secret`. Learn more about [creating OAuth credentials](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside Kubeflow cluster\n",
    "HOST = 'https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline'\n",
    "# HOST = 'https://7c021d0340d296aa-dot-us-central2.pipelines.googleusercontent.com'\n",
    "CLIENT_ID = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_ID = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_SECRET = \"cB8Xj-rb9JWCYcCRDlpTMfhc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    if HOST.endswith('.com'):\n",
    "        client = kfp.Client(host=HOST)\n",
    "    else:\n",
    "        client = kfp.Client(host=HOST, \n",
    "                            client_id=CLIENT_ID,\n",
    "                            other_client_id=OTHER_CLIENT_ID, \n",
    "                            other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(\n",
    "    project_id: str, job_id: str\n",
    ") -> NamedTuple('Outputs', [('metric_value', float), ('alpha', float),\n",
    "                            ('max_iter', int)]):\n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "\n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "    alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "    max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "    return (metric_value, alpha, max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    dataset_path: str, model_path: str, metric_name: str\n",
    ") -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    import pickle\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    from tensorflow import gfile\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Cover_Type', axis=1)\n",
    "    y_test = df_test['Cover_Type']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.joblib'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    \n",
    "    if gfile.Exists(model_filename):\n",
    "        gfile.Remove(model_filename)\n",
    "\n",
    "    gfile.Copy(gcs_model_filepath, model_filename)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = pickle.load(model_file)\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the program code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a file `train.py` that contains a Python script. The script use the Covertype Data Set to develop a multi-class classification model that predicts the type of forest cover from cartographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/aip_pipeline/covertype_training\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/aip_pipeline/covertype_training/train.py <<HERE\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from tensorflow import gfile\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path,\n",
    "                   alpha, max_iter, hptune):\n",
    "    with gfile.Open(training_dataset_path, 'r') as f:\n",
    "        # Assume there is no header\n",
    "        df_train = pd.read_csv(f)\n",
    "\n",
    "    with gfile.Open(validation_dataset_path, 'r') as f:\n",
    "        # Assume there is no header\n",
    "        df_validation = pd.read_csv(f)\n",
    "\n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_feature_indexes),\n",
    "            ('cat', OneHotEncoder(), categorical_feature_indexes)\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log'))\n",
    "    ])\n",
    "\n",
    "    num_features_type_map = {feature: 'float64' for feature in\n",
    "                             df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map)\n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "\n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "        y_validation = df_validation['Cover_Type']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='accuracy',\n",
    "            metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "\n",
    "        model_filename = 'model.pkl'\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "\n",
    "        if gfile.Exists(gcs_model_path):\n",
    "            gfile.Remove(gcs_model_path)\n",
    "\n",
    "        with gfile.Open(gcs_model_path, 'w') as wf:\n",
    "            pickle.dump(pipeline, wf)\n",
    "\n",
    "        print(\"Saved model in: {}\".format(gcs_model_path))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker container\n",
    "Create your own container image that includes your program. Now create a container that runs the script. Start by creating a Dockerfile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p tmp/aip_pipeline/covertype_base\n",
    "\n",
    "cat > ./tmp/aip_pipeline/covertype_base/requirements.txt <<EOF\n",
    "fire\n",
    "cloudml-hypertune\n",
    "gcsfs\n",
    "pandas==0.24.0\n",
    "google-api-python-client==1.7.8\n",
    "joblib==0.13.0\n",
    "scikit-learn==0.20.2\n",
    "EOF\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/aip_pipeline/covertype_base/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "RUN pip install -r requirements.txt\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "\n",
    "BASE_IMAGE_URI=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "BASE_APP_FOLDER='./tmp/aip_pipeline/covertype_base/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 211 bytes before compression.\n",
      "Uploading tarball of [./tmp/aip_pipeline/covertype_base/] to [gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584428228.977243-fcee4428cdc647c6a807d695d53c3507.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-pipeline-fantasy/builds/319675bf-008f-46d9-98b4-b6e7cf640d7b].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/319675bf-008f-46d9-98b4-b6e7cf640d7b?project=493831447550].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"319675bf-008f-46d9-98b4-b6e7cf640d7b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584428228.977243-fcee4428cdc647c6a807d695d53c3507.tgz#1584428230141965\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584428228.977243-fcee4428cdc647c6a807d695d53c3507.tgz#1584428230141965...\n",
      "/ [1 files][  341.0 B/  341.0 B]                                                \n",
      "Operation completed over 1 objects/341.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.072kB\n",
      "Step 1/4 : FROM tensorflow/tensorflow:1.14.0-py3\n",
      "1.14.0-py3: Pulling from tensorflow/tensorflow\n",
      "5b7339215d1d: Pulling fs layer\n",
      "14ca88e9f672: Pulling fs layer\n",
      "a31c3b1caad4: Pulling fs layer\n",
      "b054a26005b7: Pulling fs layer\n",
      "8832e3773578: Pulling fs layer\n",
      "5e671b828b2a: Pulling fs layer\n",
      "2b940936f993: Pulling fs layer\n",
      "016724bbd2c9: Pulling fs layer\n",
      "5bd1cb597025: Pulling fs layer\n",
      "68543864d644: Pulling fs layer\n",
      "b054a26005b7: Waiting\n",
      "8832e3773578: Waiting\n",
      "5e671b828b2a: Waiting\n",
      "2b940936f993: Waiting\n",
      "016724bbd2c9: Waiting\n",
      "5bd1cb597025: Waiting\n",
      "68543864d644: Waiting\n",
      "a31c3b1caad4: Verifying Checksum\n",
      "a31c3b1caad4: Download complete\n",
      "14ca88e9f672: Verifying Checksum\n",
      "14ca88e9f672: Download complete\n",
      "5b7339215d1d: Verifying Checksum\n",
      "5b7339215d1d: Download complete\n",
      "b054a26005b7: Verifying Checksum\n",
      "b054a26005b7: Download complete\n",
      "2b940936f993: Verifying Checksum\n",
      "2b940936f993: Download complete\n",
      "5e671b828b2a: Verifying Checksum\n",
      "5e671b828b2a: Download complete\n",
      "8832e3773578: Verifying Checksum\n",
      "8832e3773578: Download complete\n",
      "5bd1cb597025: Verifying Checksum\n",
      "5bd1cb597025: Download complete\n",
      "68543864d644: Verifying Checksum\n",
      "68543864d644: Download complete\n",
      "016724bbd2c9: Verifying Checksum\n",
      "016724bbd2c9: Download complete\n",
      "5b7339215d1d: Pull complete\n",
      "14ca88e9f672: Pull complete\n",
      "a31c3b1caad4: Pull complete\n",
      "b054a26005b7: Pull complete\n",
      "8832e3773578: Pull complete\n",
      "5e671b828b2a: Pull complete\n",
      "2b940936f993: Pull complete\n",
      "016724bbd2c9: Pull complete\n",
      "5bd1cb597025: Pull complete\n",
      "68543864d644: Pull complete\n",
      "Digest: sha256:7c05dfab9ea82c95b571ca7dc3d92d4e0b289eeec6b3abf51f189caca0684488\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.14.0-py3\n",
      " ---> 4cc892a3babd\n",
      "Step 2/4 : WORKDIR /app\n",
      " ---> Running in 39882eb911ac\n",
      "Removing intermediate container 39882eb911ac\n",
      " ---> cdf35a58d4dd\n",
      "Step 3/4 : COPY . /app\n",
      " ---> 23184ba1e04b\n",
      "Step 4/4 : RUN pip install -r requirements.txt\n",
      " ---> Running in 49bda9482997\n",
      "Collecting fire (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Collecting gcsfs (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/9f/864a9ff497ed4ba12502c4037db8c66fde0049d9dd0388bd55b67e5c4249/gcsfs-0.6.0-py2.py3-none-any.whl\n",
      "Collecting pandas==0.24.0 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/e1/4a63ed31e1b1362d40ce845a5735c717a959bda992669468dae3420af2cd/pandas-0.24.0-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "Collecting google-api-python-client==1.7.8 (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/55/e9/e8fb2e3a031cb69b9524b80a92b126665d9a17421700a219555e3233ab6a/google_api_python_client-1.7.8-py3-none-any.whl (56kB)\n",
      "Collecting joblib==0.13.0 (from -r requirements.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/1b/995167f6c66848d4eb7eabc386aebe07a1571b397629b2eac3b7bebdc343/joblib-0.13.0-py2.py3-none-any.whl (276kB)\n",
      "Collecting scikit-learn==0.20.2 (from -r requirements.txt (line 7))\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/3a/b92670f5c368c20329ecc4c255993fae7934564d485c3ed7ea7b8da7f741/scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->-r requirements.txt (line 1)) (1.1.0)\n",
      "Collecting google-auth>=1.2 (from gcsfs->-r requirements.txt (line 3))\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --tag $BASE_IMAGE_URI $BASE_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p tmp/aip_pipeline/covertype_training\n",
    "\n",
    "cat > ./tmp/aip_pipeline/covertype_training/requirements.txt <<EOF\n",
    "fire\n",
    "cloudml-hypertune\n",
    "gcsfs\n",
    "pandas==0.24.0\n",
    "google-api-python-client==1.7.8\n",
    "joblib==0.13.0\n",
    "scikit-learn==0.20.2\n",
    "EOF\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/aip_pipeline/covertype_training/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.14.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='traing_image'\n",
    "TAG='latest'\n",
    "\n",
    "TRAIN_IMAGE_URI=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "TRAIN_APP_FOLDER='./tmp/aip_pipeline/covertype_training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud builds submit --tag $TRAIN_IMAGE_URI $TRAIN_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_URL_SEARCH_PREFIX='https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'\n",
    "# COMPONENT_URL_SEARCH_PREFIX='https://raw.githubusercontent.com/kubeflow/pipelines/3f4b80127f35e40760eeb1813ce1d3f641502222/components/gcp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "mlengine_deploy_op = component_store.load_component('ml_engine/deploy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_best_run_op = comp.func_to_container_op(retrieve_best_run, base_image=BASE_IMAGE_URI)\n",
    "evaluate_model_op = comp.func_to_container_op(evaluate_model, base_image=BASE_IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define deployment operation on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "    \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "    sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}` AS cover\n",
    "       WHERE \n",
    "       MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "       \"\"\"\n",
    "    query = Template(sampling_query_template).render(\n",
    "      source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_pipeline(\n",
    "    project_id: types.GCPProjectID,\n",
    "    region: types.GCPRegion,\n",
    "    source_table_name: types.String,\n",
    "    gcs_root: types.GCSPath,\n",
    "    dataset_id: str,\n",
    "    evaluation_metric_name: str,\n",
    "    evaluation_metric_threshold: float,\n",
    "    model_id: str,\n",
    "    replace_existing_version: bool,\n",
    "    hypertune_settings: types.Dict = HYPERTUNE_SETTINGS,\n",
    "    dataset_location: str = 'US'\n",
    "):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "    training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "    create_training_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=training_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the validation split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "    validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "    create_validation_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=validation_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "    testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "    create_testing_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=testing_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                              kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAIN_IMAGE_URI,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAIN_IMAGE_URI,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "            model_uri=train_model.outputs['job_dir'],\n",
    "            project_id=project_id,\n",
    "            model_id=model_id,\n",
    "            runtime_version=\"1.14\",\n",
    "            python_version=\"3.5\",\n",
    "            replace_existing_version=True, \n",
    "            set_default=True)\n",
    "\n",
    "    kfp.dsl.get_pipeline_conf().add_op_transformer(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = covertype_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'covertype_kubeflow'\n",
    "\n",
    "arguments = {\n",
    "    'project_id': PROJECT_ID,\n",
    "    'gcs_root': GCS_STAGING_PATH,\n",
    "    'region': AIP_REGION,\n",
    "    'source_table_name': 'covertype_dataset.covertype',\n",
    "    'dataset_id': 'splits',\n",
    "    'evaluation_metric_name': 'accuracy',\n",
    "    'evaluation_metric_threshold': 0.69,\n",
    "    'model_id': 'covertype_classifier',\n",
    "    'replace_existing_version': True\n",
    "}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualPython35",
   "language": "python",
   "name": "virtualpython35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
