{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating model training and deployment with Kubeflow Pipelines (KFP) and Cloud AI Platform\n",
    "\n",
    "In this lab, you will develop, deploy, and run a KFP pipeline that orchestrates BigQuery and Cloud AI Platform services to train a scikit-learn model.\n",
    "\n",
    "The pipeline you develop in the lab orchestrates GCP managed services. The source data is in BigQuery. The pipeline uses:\n",
    "- Pre-build components. The pipeline uses the following pre-build components that are included with KFP distribution:\n",
    "    - [BigQuery query component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/bigquery/query)\n",
    "    - [AI Platform Training component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/ml_engine/train)\n",
    "    - [AI Platform Deploy component](https://github.com/kubeflow/pipelines/tree/0.1.36/components/gcp/ml_engine/deploy)\n",
    "- Custom components. The pipeline uses two custom helper components that encapsulate functionality not available in any of the pre-build components. The components are implemented using the KFP SDK's [Lightweight Python Components](https://www.kubeflow.org/docs/pipelines/sdk/lightweight-python-components/) mechanism. The code for the components is in the `helper_components.py` file:\n",
    "    - **Retrieve Best Run**. This component retrieves the tuning metric and hyperparameter values for the best run of the AI Platform Training hyperparameter tuning job.\n",
    "    - **Evaluate Model**. This component evaluates the *sklearn* trained model using a provided metric and a testing dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab dataset\n",
    "This lab uses the [Covertype Dat Set](https://archive.ics.uci.edu/ml/datasets/covertype). The pipeline developed in the lab sources the dataset from BigQuery. Before proceeding with the lab upload the dataset to BigQuery:\n",
    "\n",
    "1. Open new terminal in you **JupyterLab**\n",
    "\n",
    "2. Create the BigQuery dataset and upload the Cover Type csv file.\n",
    "\n",
    "```\n",
    "export PROJECT_ID=$(gcloud config get-value core/project)\n",
    "\n",
    "DATASET_LOCATION=US\n",
    "DATASET_ID=covertype_dataset\n",
    "TABLE_ID=covertype\n",
    "DATA_SOURCE=gs://workshop-datasets/covertype/full/dataset.csv\n",
    "SCHEMA=Elevation:INTEGER,\\\n",
    "Aspect:INTEGER,\\\n",
    "Slope:INTEGER,\\\n",
    "Horizontal_Distance_To_Hydrology:INTEGER,\\\n",
    "Vertical_Distance_To_Hydrology:INTEGER,\\\n",
    "Horizontal_Distance_To_Roadways:INTEGER,\\\n",
    "Hillshade_9am:INTEGER,\\\n",
    "Hillshade_Noon:INTEGER,\\\n",
    "Hillshade_3pm:INTEGER,\\\n",
    "Horizontal_Distance_To_Fire_Points:INTEGER,\\\n",
    "Wilderness_Area:STRING,\\\n",
    "Soil_Type:STRING,\\\n",
    "Cover_Type:INTEGER\n",
    "\n",
    "bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID\n",
    "\n",
    "bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TABLE_ID \\\n",
    "$DATA_SOURCE \\\n",
    "$SCHEMA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "from kfp.dsl import types\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s\n",
    "\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/luoshixin/LocalDevelop/kubeflow-pipeline/kubeflow-pipeline/kubeflow-pipeline-fantasy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "PROJECT_ID='kubeflow-pipeline-fantasy'\n",
    "GCS_BUCKET='gs://kubeflow-pipeline-ui'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX=PROJECT_ID\n",
    "NAMESPACE='kubeflow'\n",
    "AIP_REGION='us-central1'\n",
    "AIP_ZONE='us-central1-a'\n",
    "GCS_STAGING_PATH='{}/staging'.format(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "If you run this notebook **outside** of a Kubeflow cluster, run the following command:\n",
    "- `host`: The URL of your Kubeflow Pipelines instance, for example \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- `client_id`: The client ID used by Identity-Aware Proxy\n",
    "- `other_client_id`: The client ID used to obtain the auth codes and refresh tokens.\n",
    "- `other_client_secret`: The client secret used to obtain the auth codes and refresh tokens.\n",
    "\n",
    "```python\n",
    "client = kfp.Client(host, client_id, other_client_id, other_client_secret)\n",
    "```\n",
    "\n",
    "If you run this notebook **within** a Kubeflow cluster, run the following command:\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```\n",
    "\n",
    "You'll need to create OAuth client ID credentials of type `Other` to get `other_client_id` and `other_client_secret`. Learn more about [creating OAuth credentials](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside Kubeflow cluster\n",
    "HOST = 'https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline'\n",
    "# HOST = 'https://7c021d0340d296aa-dot-us-central2.pipelines.googleusercontent.com'\n",
    "CLIENT_ID = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_ID = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_SECRET = \"cB8Xj-rb9JWCYcCRDlpTMfhc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    if HOST.endswith('.com'):\n",
    "        client = kfp.Client(host=HOST)\n",
    "    else:\n",
    "        client = kfp.Client(host=HOST, \n",
    "                            client_id=CLIENT_ID,\n",
    "                            other_client_id=OTHER_CLIENT_ID, \n",
    "                            other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "\n",
    "def retrieve_best_run(\n",
    "    project_id: str, job_id: str\n",
    ") -> NamedTuple('Outputs', [('metric_value', float), ('alpha', float),\n",
    "                            ('max_iter', int)]):\n",
    "    \"\"\"Retrieves the parameters of the best Hypertune run.\"\"\"\n",
    "\n",
    "    from googleapiclient import discovery\n",
    "    from googleapiclient import errors\n",
    "\n",
    "    ml = discovery.build('ml', 'v1')\n",
    "\n",
    "    job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)\n",
    "    request = ml.projects().jobs().get(name=job_name)\n",
    "\n",
    "    try:\n",
    "        response = request.execute()\n",
    "    except errors.HttpError as err:\n",
    "        print(err)\n",
    "    except:\n",
    "        print('Unexpected error')\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    best_trial = response['trainingOutput']['trials'][0]\n",
    "\n",
    "    metric_value = best_trial['finalMetric']['objectiveValue']\n",
    "    alpha = float(best_trial['hyperparameters']['alpha'])\n",
    "    max_iter = int(best_trial['hyperparameters']['max_iter'])\n",
    "\n",
    "    return (metric_value, alpha, max_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    dataset_path: str, model_path: str, metric_name: str\n",
    ") -> NamedTuple('Outputs', [('metric_name', str), ('metric_value', float),\n",
    "                            ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \"\"\"Evaluates a trained sklearn model.\"\"\"\n",
    "    import joblib\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    from tensorflow import gfile\n",
    "    from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "    df_test = pd.read_csv(dataset_path)\n",
    "\n",
    "    X_test = df_test.drop('Cover_Type', axis=1)\n",
    "    y_test = df_test['Cover_Type']\n",
    "\n",
    "    # Copy the model from GCS\n",
    "    model_filename = 'model.joblib'\n",
    "    gcs_model_filepath = '{}/{}'.format(model_path, model_filename)\n",
    "    print(gcs_model_filepath)\n",
    "    \n",
    "    if gfile.Exists(model_filename):\n",
    "        gfile.Remove(model_filename)\n",
    "\n",
    "    gfile.Copy(gcs_model_filepath, model_filename)\n",
    "\n",
    "    with open(model_filename, 'rb') as model_file:\n",
    "        model = joblib.load(model_file)\n",
    "\n",
    "    y_hat = model.predict(X_test)\n",
    "\n",
    "    if metric_name == 'accuracy':\n",
    "        metric_value = accuracy_score(y_test, y_hat)\n",
    "    elif metric_name == 'recall':\n",
    "        metric_value = recall_score(y_test, y_hat)\n",
    "    else:\n",
    "        metric_name = 'N/A'\n",
    "        metric_value = 0\n",
    "\n",
    "    # Export the metric\n",
    "    metrics = {\n",
    "      'metrics': [{\n",
    "          'name': metric_name,\n",
    "          'numberValue': float(metric_value)\n",
    "      }]\n",
    "    }\n",
    "\n",
    "    return (metric_name, metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the program code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a file `train.py` that contains a Python script. The script use the Covertype Data Set to develop a multi-class classification model that predicts the type of forest cover from cartographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/aip_pipeline/covertype_training\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/aip_pipeline/covertype_training/train.py <<HERE\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "import hypertune\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def train_evaluate(job_dir, training_dataset_path, validation_dataset_path, alpha, max_iter, hptune):\n",
    "    \n",
    "    df_train = pd.read_csv(training_dataset_path)\n",
    "    df_validation = pd.read_csv(validation_dataset_path)\n",
    "    \n",
    "    if not hptune:\n",
    "        df_train = pd.concat([df_train, df_validation])\n",
    "\n",
    "    numeric_feature_indexes = slice(0, 10)\n",
    "    categorical_feature_indexes = slice(10, 12)\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numeric_feature_indexes),\n",
    "            ('cat', OneHotEncoder(), categorical_feature_indexes) \n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SGDClassifier(loss='log'))\n",
    "        ])\n",
    "    \n",
    "    num_features_type_map = {feature: 'float64' for feature in df_train.columns[numeric_feature_indexes]}\n",
    "    df_train = df_train.astype(num_features_type_map)\n",
    "    df_validation = df_validation.astype(num_features_type_map) \n",
    "\n",
    "    print('Starting training: alpha={}, max_iter={}'.format(alpha, max_iter))\n",
    "    X_train = df_train.drop('Cover_Type', axis=1)\n",
    "    y_train = df_train['Cover_Type']\n",
    "  \n",
    "    pipeline.set_params(classifier__alpha=alpha, classifier__max_iter=max_iter)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "  \n",
    "    if hptune:\n",
    "        X_validation = df_validation.drop('Cover_Type', axis=1)\n",
    "        y_validation = df_validation['Cover_Type']\n",
    "        accuracy = pipeline.score(X_validation, y_validation)\n",
    "        print('Model accuracy: {}'.format(accuracy))\n",
    "        # Log it with hypertune\n",
    "        hpt = hypertune.HyperTune()\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='accuracy',\n",
    "        metric_value=accuracy\n",
    "        )\n",
    "\n",
    "    # Save the model\n",
    "    if not hptune:\n",
    "        from tensorflow import gfile\n",
    "        model_filename = 'model.joblib'\n",
    "        gcs_model_path = \"{}/{}\".format(job_dir, model_filename)\n",
    "        \n",
    "        if gfile.Exists(gcs_model_path):\n",
    "            gfile.Remove(gcs_model_path)\n",
    "        \n",
    "        with gfile.Open(gcs_model_path, 'w') as wf:\n",
    "            joblib.dump(pipeline, wf)\n",
    "        \n",
    "        print(\"Saved model in: {}\".format(gcs_model_path)) \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(train_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Docker container\n",
    "Create your own container image that includes your program. Now create a container that runs the script. Start by creating a Dockerfile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p tmp/aip_pipeline/covertype_base\n",
    "\n",
    "cat > ./tmp/aip_pipeline/covertype_base/requirements.txt <<EOF\n",
    "fire\n",
    "cloudml-hypertune\n",
    "pandas\n",
    "google-api-python-client\n",
    "gcsfs\n",
    "joblib\n",
    "scikit-learn==0.20.2\n",
    "EOF\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/aip_pipeline/covertype_base/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.15.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "RUN pip install -r requirements.txt\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='base_image'\n",
    "TAG='latest'\n",
    "\n",
    "BASE_IMAGE_URI=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "BASE_APP_FOLDER='./tmp/aip_pipeline/covertype_base/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 188 bytes before compression.\n",
      "Uploading tarball of [./tmp/aip_pipeline/covertype_base/] to [gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584410684.426886-987e5cd5bc5f497fa20bc5151cde2f69.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-pipeline-fantasy/builds/927428a3-2174-402f-b3bc-e6f4d64d357d].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/927428a3-2174-402f-b3bc-e6f4d64d357d?project=493831447550].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"927428a3-2174-402f-b3bc-e6f4d64d357d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584410684.426886-987e5cd5bc5f497fa20bc5151cde2f69.tgz#1584410685560067\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584410684.426886-987e5cd5bc5f497fa20bc5151cde2f69.tgz#1584410685560067...\n",
      "/ [1 files][  326.0 B/  326.0 B]                                                \n",
      "Operation completed over 1 objects/326.0 B.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.072kB\n",
      "Step 1/4 : FROM tensorflow/tensorflow:1.15.0-py3\n",
      "1.15.0-py3: Pulling from tensorflow/tensorflow\n",
      "22e816666fd6: Pulling fs layer\n",
      "079b6d2a1e53: Pulling fs layer\n",
      "11048ebae908: Pulling fs layer\n",
      "c58094023a2e: Pulling fs layer\n",
      "fb153ade6d14: Pulling fs layer\n",
      "0db149060649: Pulling fs layer\n",
      "138c908b7d99: Pulling fs layer\n",
      "094a8f5dd2cb: Pulling fs layer\n",
      "354ee6535f23: Pulling fs layer\n",
      "f5de9bda32bd: Pulling fs layer\n",
      "ac66bd508eff: Pulling fs layer\n",
      "c58094023a2e: Waiting\n",
      "fb153ade6d14: Waiting\n",
      "0db149060649: Waiting\n",
      "138c908b7d99: Waiting\n",
      "094a8f5dd2cb: Waiting\n",
      "354ee6535f23: Waiting\n",
      "f5de9bda32bd: Waiting\n",
      "ac66bd508eff: Waiting\n",
      "079b6d2a1e53: Verifying Checksum\n",
      "079b6d2a1e53: Download complete\n",
      "11048ebae908: Verifying Checksum\n",
      "11048ebae908: Download complete\n",
      "22e816666fd6: Verifying Checksum\n",
      "22e816666fd6: Download complete\n",
      "c58094023a2e: Verifying Checksum\n",
      "c58094023a2e: Download complete\n",
      "fb153ade6d14: Verifying Checksum\n",
      "fb153ade6d14: Download complete\n",
      "138c908b7d99: Verifying Checksum\n",
      "138c908b7d99: Download complete\n",
      "094a8f5dd2cb: Verifying Checksum\n",
      "094a8f5dd2cb: Download complete\n",
      "f5de9bda32bd: Verifying Checksum\n",
      "f5de9bda32bd: Download complete\n",
      "ac66bd508eff: Verifying Checksum\n",
      "ac66bd508eff: Download complete\n",
      "0db149060649: Verifying Checksum\n",
      "0db149060649: Download complete\n",
      "22e816666fd6: Pull complete\n",
      "354ee6535f23: Verifying Checksum\n",
      "354ee6535f23: Download complete\n",
      "079b6d2a1e53: Pull complete\n",
      "11048ebae908: Pull complete\n",
      "c58094023a2e: Pull complete\n",
      "fb153ade6d14: Pull complete\n",
      "0db149060649: Pull complete\n",
      "138c908b7d99: Pull complete\n",
      "094a8f5dd2cb: Pull complete\n",
      "354ee6535f23: Pull complete\n",
      "f5de9bda32bd: Pull complete\n",
      "ac66bd508eff: Pull complete\n",
      "Digest: sha256:ac8457b32f65b68ee2421eabaea70919b373ada9746d46a6d1566d66c75ff719\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.15.0-py3\n",
      " ---> f24a5ca8605f\n",
      "Step 2/4 : WORKDIR /app\n",
      " ---> Running in 6191b7ccee09\n",
      "Removing intermediate container 6191b7ccee09\n",
      " ---> a38ea81e961d\n",
      "Step 3/4 : COPY . /app\n",
      " ---> 328153faba9d\n",
      "Step 4/4 : RUN pip install -r requirements.txt\n",
      " ---> Running in fc39bdc94cc9\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/b9/9ad570258ce4fe504bd23002154f9e6f09bf7110359d271e4ba1664f7281/pandas-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "Collecting google-api-python-client\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
      "Collecting gcsfs\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/9f/864a9ff497ed4ba12502c4037db8c66fde0049d9dd0388bd55b67e5c4249/gcsfs-0.6.0-py2.py3-none-any.whl\n",
      "Collecting joblib\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "Collecting scikit-learn==0.20.2\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/3a/b92670f5c368c20329ecc4c255993fae7934564d485c3ed7ea7b8da7f741/scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (1.17.3)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/0c/60d82c077998feb631608dca3cc1fe19ac074e772bf0c24cf409b977b815/uritemplate-3.0.1-py2.py3-none-any.whl\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Collecting httplib2<1dev,>=0.9.2\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/4b/025a7338bb2d4a2c61f0e530b79aafc29d112ed8e61333a6dd9ba48f3bab/httplib2-0.17.0-py3-none-any.whl (95kB)\n",
      "Collecting google-auth>=1.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl (76kB)\n",
      "Collecting google-api-core<2dev,>=1.13.0\n",
      "  Downloading https://files.pythonhosted.org/packages/63/7e/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/1f/7028dacd3c28f34ce48130aae73a88fa5cc27b6b0e494fcf2739f7954d9d/fsspec-0.6.2-py3-none-any.whl (62kB)\n",
      "Collecting decorator\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
      "Collecting scipy>=0.13.3\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->-r requirements.txt (line 4)) (41.4.0)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf/googleapis-common-protos-1.51.0.tar.gz\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client->-r requirements.txt (line 4)) (3.10.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting certifi>=2017.4.17\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->gcsfs->-r requirements.txt (line 5)) (2.6)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, googleapis-common-protos\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=104738 sha256=56ba27e7d8275681b39014266da5cd5d4afce192d4ce1d786dd524737a9b2ad1\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=5583 sha256=5b3379a824f5a9cb2ef4570fb84ce1935dedd1853d834ddeb5aa23fccefd3bb9\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "  Building wheel for googleapis-common-protos (setup.py): started\n",
      "  Building wheel for googleapis-common-protos (setup.py): finished with status 'done'\n",
      "  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-cp36-none-any.whl size=74529 sha256=d4f8e3f23308e0bf58c681ed091e4e91479dabb5e1a94fd363c3b2432f8e302b\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f9/7f/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\n",
      "Successfully built fire cloudml-hypertune googleapis-common-protos\n",
      "Installing collected packages: fire, cloudml-hypertune, python-dateutil, pytz, pandas, uritemplate, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, httplib2, google-auth-httplib2, googleapis-common-protos, certifi, urllib3, chardet, requests, google-api-core, google-api-python-client, fsspec, decorator, oauthlib, requests-oauthlib, google-auth-oauthlib, gcsfs, joblib, scipy, scikit-learn\n",
      "Successfully installed cachetools-4.0.0 certifi-2019.11.28 chardet-3.0.4 cloudml-hypertune-0.1.0.dev6 decorator-4.4.2 fire-0.2.1 fsspec-0.6.2 gcsfs-0.6.0 google-api-core-1.16.0 google-api-python-client-1.8.0 google-auth-1.11.3 google-auth-httplib2-0.0.3 google-auth-oauthlib-0.4.1 googleapis-common-protos-1.51.0 httplib2-0.17.0 joblib-0.14.1 oauthlib-3.1.0 pandas-1.0.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.20.2 scipy-1.4.1 uritemplate-3.0.1 urllib3-1.25.8\n",
      "\u001b[91mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container fc39bdc94cc9\n",
      " ---> 595286fa1ad9\n",
      "Successfully built 595286fa1ad9\n",
      "Successfully tagged gcr.io/kubeflow-pipeline-fantasy/base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-pipeline-fantasy/base_image:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/base_image]\n",
      "56b58c2e18b2: Preparing\n",
      "fdb16059b410: Preparing\n",
      "b5bb0d0155bd: Preparing\n",
      "84c3bc63b701: Preparing\n",
      "56ec85ad394c: Preparing\n",
      "aefe991487a2: Preparing\n",
      "4a58ecdd995f: Preparing\n",
      "fa9f3f4bd775: Preparing\n",
      "2bf9e296738e: Preparing\n",
      "92486bede3ce: Preparing\n",
      "19331eff40f0: Preparing\n",
      "100ef12ce3a4: Preparing\n",
      "97e6b67a30f1: Preparing\n",
      "a090697502b8: Preparing\n",
      "aefe991487a2: Waiting\n",
      "4a58ecdd995f: Waiting\n",
      "fa9f3f4bd775: Waiting\n",
      "2bf9e296738e: Waiting\n",
      "92486bede3ce: Waiting\n",
      "19331eff40f0: Waiting\n",
      "100ef12ce3a4: Waiting\n",
      "97e6b67a30f1: Waiting\n",
      "a090697502b8: Waiting\n",
      "56ec85ad394c: Layer already exists\n",
      "84c3bc63b701: Layer already exists\n",
      "4a58ecdd995f: Layer already exists\n",
      "aefe991487a2: Layer already exists\n",
      "fa9f3f4bd775: Layer already exists\n",
      "2bf9e296738e: Layer already exists\n",
      "92486bede3ce: Layer already exists\n",
      "19331eff40f0: Layer already exists\n",
      "100ef12ce3a4: Layer already exists\n",
      "97e6b67a30f1: Layer already exists\n",
      "a090697502b8: Layer already exists\n",
      "b5bb0d0155bd: Pushed\n",
      "fdb16059b410: Pushed\n",
      "56b58c2e18b2: Pushed\n",
      "latest: digest: sha256:f4a38313bdbe427192916b6e447a28584e340a812bd92c52227945f33e9aa552 size: 3250\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                   IMAGES                                                 STATUS\n",
      "927428a3-2174-402f-b3bc-e6f4d64d357d  2020-03-17T02:04:45+00:00  2M1S      gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584410684.426886-987e5cd5bc5f497fa20bc5151cde2f69.tgz  gcr.io/kubeflow-pipeline-fantasy/base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --tag $BASE_IMAGE_URI $BASE_APP_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p tmp/aip_pipeline/covertype_training\n",
    "\n",
    "cat > ./tmp/aip_pipeline/covertype_training/requirements.txt <<EOF\n",
    "fire\n",
    "cloudml-hypertune\n",
    "pandas\n",
    "google-api-python-client\n",
    "gcsfs\n",
    "joblib\n",
    "scikit-learn==0.20.2\n",
    "EOF\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/aip_pipeline/covertype_training/Dockerfile <<EOF\n",
    "FROM tensorflow/tensorflow:1.15.0-py3\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='traing_image'\n",
    "TAG='latest'\n",
    "\n",
    "TRAIN_IMAGE_URI=\"gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{TAG}\".format(\n",
    "    PROJECT_ID=PROJECT_ID,\n",
    "    IMAGE_NAME=IMAGE_NAME,\n",
    "    TAG=TAG\n",
    ")\n",
    "\n",
    "TRAIN_APP_FOLDER='./tmp/aip_pipeline/covertype_training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 3 file(s) totalling 2.7 KiB before compression.\n",
      "Uploading tarball of [./tmp/aip_pipeline/covertype_training/] to [gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584410811.528946-2614ac2b9c90435f8af3b9b6550edd44.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-pipeline-fantasy/builds/3f9f3335-d81a-4eff-b9d8-01ae911ae303].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/3f9f3335-d81a-4eff-b9d8-01ae911ae303?project=493831447550].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"3f9f3335-d81a-4eff-b9d8-01ae911ae303\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584410811.528946-2614ac2b9c90435f8af3b9b6550edd44.tgz#1584410812580632\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584410811.528946-2614ac2b9c90435f8af3b9b6550edd44.tgz#1584410812580632...\n",
      "/ [1 files][  1.2 KiB/  1.2 KiB]                                                \n",
      "Operation completed over 1 objects/1.2 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  6.144kB\n",
      "Step 1/5 : FROM tensorflow/tensorflow:1.15.0-py3\n",
      "1.15.0-py3: Pulling from tensorflow/tensorflow\n",
      "22e816666fd6: Pulling fs layer\n",
      "079b6d2a1e53: Pulling fs layer\n",
      "11048ebae908: Pulling fs layer\n",
      "c58094023a2e: Pulling fs layer\n",
      "fb153ade6d14: Pulling fs layer\n",
      "0db149060649: Pulling fs layer\n",
      "138c908b7d99: Pulling fs layer\n",
      "094a8f5dd2cb: Pulling fs layer\n",
      "354ee6535f23: Pulling fs layer\n",
      "f5de9bda32bd: Pulling fs layer\n",
      "ac66bd508eff: Pulling fs layer\n",
      "c58094023a2e: Waiting\n",
      "fb153ade6d14: Waiting\n",
      "0db149060649: Waiting\n",
      "138c908b7d99: Waiting\n",
      "094a8f5dd2cb: Waiting\n",
      "354ee6535f23: Waiting\n",
      "f5de9bda32bd: Waiting\n",
      "ac66bd508eff: Waiting\n",
      "11048ebae908: Verifying Checksum\n",
      "11048ebae908: Download complete\n",
      "079b6d2a1e53: Verifying Checksum\n",
      "079b6d2a1e53: Download complete\n",
      "22e816666fd6: Verifying Checksum\n",
      "22e816666fd6: Download complete\n",
      "c58094023a2e: Verifying Checksum\n",
      "c58094023a2e: Download complete\n",
      "fb153ade6d14: Verifying Checksum\n",
      "fb153ade6d14: Download complete\n",
      "138c908b7d99: Verifying Checksum\n",
      "138c908b7d99: Download complete\n",
      "094a8f5dd2cb: Verifying Checksum\n",
      "094a8f5dd2cb: Download complete\n",
      "f5de9bda32bd: Verifying Checksum\n",
      "f5de9bda32bd: Download complete\n",
      "ac66bd508eff: Verifying Checksum\n",
      "ac66bd508eff: Download complete\n",
      "0db149060649: Verifying Checksum\n",
      "0db149060649: Download complete\n",
      "354ee6535f23: Verifying Checksum\n",
      "354ee6535f23: Download complete\n",
      "22e816666fd6: Pull complete\n",
      "079b6d2a1e53: Pull complete\n",
      "11048ebae908: Pull complete\n",
      "c58094023a2e: Pull complete\n",
      "fb153ade6d14: Pull complete\n",
      "0db149060649: Pull complete\n",
      "138c908b7d99: Pull complete\n",
      "094a8f5dd2cb: Pull complete\n",
      "354ee6535f23: Pull complete\n",
      "f5de9bda32bd: Pull complete\n",
      "ac66bd508eff: Pull complete\n",
      "Digest: sha256:ac8457b32f65b68ee2421eabaea70919b373ada9746d46a6d1566d66c75ff719\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.15.0-py3\n",
      " ---> f24a5ca8605f\n",
      "Step 2/5 : WORKDIR /app\n",
      " ---> Running in 938ff39bc6b2\n",
      "Removing intermediate container 938ff39bc6b2\n",
      " ---> 158c6282debd\n",
      "Step 3/5 : COPY . /app\n",
      " ---> e0b99b01cbdb\n",
      "Step 4/5 : RUN pip install -r requirements.txt\n",
      " ---> Running in b84442be3684\n",
      "Collecting fire\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading https://files.pythonhosted.org/packages/84/54/142a00a29d1c51dcf8c93b305f35554c947be2faa0d55de1eabcc0a9023c/cloudml-hypertune-0.1.0.dev6.tar.gz\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/f9/b9/9ad570258ce4fe504bd23002154f9e6f09bf7110359d271e4ba1664f7281/pandas-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (10.1MB)\n",
      "Collecting google-api-python-client\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
      "Collecting gcsfs\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/9f/864a9ff497ed4ba12502c4037db8c66fde0049d9dd0388bd55b67e5c4249/gcsfs-0.6.0-py2.py3-none-any.whl\n",
      "Collecting joblib\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\n",
      "Collecting scikit-learn==0.20.2\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/3a/b92670f5c368c20329ecc4c255993fae7934564d485c3ed7ea7b8da7f741/scikit_learn-0.20.2-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire->-r requirements.txt (line 1)) (1.1.0)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (1.17.3)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/0c/60d82c077998feb631608dca3cc1fe19ac074e772bf0c24cf409b977b815/uritemplate-3.0.1-py2.py3-none-any.whl\n",
      "Collecting httplib2<1dev,>=0.9.2\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/4b/025a7338bb2d4a2c61f0e530b79aafc29d112ed8e61333a6dd9ba48f3bab/httplib2-0.17.0-py3-none-any.whl (95kB)\n",
      "Collecting google-auth>=1.4.1\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl (76kB)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Collecting google-api-core<2dev,>=1.13.0\n",
      "  Downloading https://files.pythonhosted.org/packages/63/7e/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl (58kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/1f/7028dacd3c28f34ce48130aae73a88fa5cc27b6b0e494fcf2739f7954d9d/fsspec-0.6.2-py3-none-any.whl (62kB)\n",
      "Collecting decorator\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/1b/72a1821152d07cf1d8b6fce298aeb06a7eb90f4d6d41acec9861e7cc6df0/decorator-4.4.2-py2.py3-none-any.whl\n",
      "Collecting scipy>=0.13.3\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client->-r requirements.txt (line 4)) (41.4.0)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client->-r requirements.txt (line 4)) (3.10.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf/googleapis-common-protos-1.51.0.tar.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl (125kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl (156kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->gcsfs->-r requirements.txt (line 5)) (2.6)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Building wheels for collected packages: fire, cloudml-hypertune, googleapis-common-protos\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=104738 sha256=7b79f7f802acccc061971e7906841b9dc5dd99dc89756f839a221e3f84dbd548\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=5583 sha256=8fec1d644592f3ef6f943065b3e89e73ad31aee88e4c4eed506584b09be155ab\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/ac/62/80b621f3fe2994f3f367a36123d8351d75e3ea5591b4a62c85\n",
      "  Building wheel for googleapis-common-protos (setup.py): started\n",
      "  Building wheel for googleapis-common-protos (setup.py): finished with status 'done'\n",
      "  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-cp36-none-any.whl size=74529 sha256=b944bbb35cc4b866a4469c6051a8ea758c02c202c6cc0e0735318c0ac2dc1260\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/f9/7f/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\n",
      "Successfully built fire cloudml-hypertune googleapis-common-protos\n",
      "Installing collected packages: fire, cloudml-hypertune, python-dateutil, pytz, pandas, uritemplate, httplib2, pyasn1, rsa, cachetools, pyasn1-modules, google-auth, google-auth-httplib2, googleapis-common-protos, urllib3, certifi, chardet, requests, google-api-core, google-api-python-client, oauthlib, requests-oauthlib, google-auth-oauthlib, fsspec, decorator, gcsfs, joblib, scipy, scikit-learn\n",
      "Successfully installed cachetools-4.0.0 certifi-2019.11.28 chardet-3.0.4 cloudml-hypertune-0.1.0.dev6 decorator-4.4.2 fire-0.2.1 fsspec-0.6.2 gcsfs-0.6.0 google-api-core-1.16.0 google-api-python-client-1.8.0 google-auth-1.11.3 google-auth-httplib2-0.0.3 google-auth-oauthlib-0.4.1 googleapis-common-protos-1.51.0 httplib2-0.17.0 joblib-0.14.1 oauthlib-3.1.0 pandas-1.0.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 python-dateutil-2.8.1 pytz-2019.3 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scikit-learn-0.20.2 scipy-1.4.1 uritemplate-3.0.1 urllib3-1.25.8\n",
      "\u001b[91mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container b84442be3684\n",
      " ---> 0eec5c12a1ff\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 26738ec03306\n",
      "Removing intermediate container 26738ec03306\n",
      " ---> 77fcb4fca903\n",
      "Successfully built 77fcb4fca903\n",
      "Successfully tagged gcr.io/kubeflow-pipeline-fantasy/traing_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-pipeline-fantasy/traing_image:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/traing_image]\n",
      "059d544d302e: Preparing\n",
      "8696a3ae48bf: Preparing\n",
      "5686c642f71f: Preparing\n",
      "84c3bc63b701: Preparing\n",
      "56ec85ad394c: Preparing\n",
      "aefe991487a2: Preparing\n",
      "4a58ecdd995f: Preparing\n",
      "fa9f3f4bd775: Preparing\n",
      "2bf9e296738e: Preparing\n",
      "92486bede3ce: Preparing\n",
      "19331eff40f0: Preparing\n",
      "100ef12ce3a4: Preparing\n",
      "97e6b67a30f1: Preparing\n",
      "a090697502b8: Preparing\n",
      "aefe991487a2: Waiting\n",
      "4a58ecdd995f: Waiting\n",
      "fa9f3f4bd775: Waiting\n",
      "2bf9e296738e: Waiting\n",
      "92486bede3ce: Waiting\n",
      "19331eff40f0: Waiting\n",
      "100ef12ce3a4: Waiting\n",
      "97e6b67a30f1: Waiting\n",
      "a090697502b8: Waiting\n",
      "84c3bc63b701: Layer already exists\n",
      "56ec85ad394c: Layer already exists\n",
      "4a58ecdd995f: Layer already exists\n",
      "aefe991487a2: Layer already exists\n",
      "2bf9e296738e: Layer already exists\n",
      "fa9f3f4bd775: Layer already exists\n",
      "19331eff40f0: Layer already exists\n",
      "92486bede3ce: Layer already exists\n",
      "100ef12ce3a4: Layer already exists\n",
      "97e6b67a30f1: Layer already exists\n",
      "a090697502b8: Layer already exists\n",
      "5686c642f71f: Pushed\n",
      "8696a3ae48bf: Pushed\n",
      "059d544d302e: Pushed\n",
      "latest: digest: sha256:90b2df9342aeeee6ec3eb12112995f68395265163aec36d86938f364789e0d57 size: 3251\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                   IMAGES                                                   STATUS\n",
      "3f9f3335-d81a-4eff-b9d8-01ae911ae303  2020-03-17T02:06:53+00:00  1M46S     gs://kubeflow-pipeline-fantasy_cloudbuild/source/1584410811.528946-2614ac2b9c90435f8af3b9b6550edd44.tgz  gcr.io/kubeflow-pipeline-fantasy/traing_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --tag $TRAIN_IMAGE_URI $TRAIN_APP_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPONENT_URL_SEARCH_PREFIX='https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'\n",
    "# COMPONENT_URL_SEARCH_PREFIX='https://raw.githubusercontent.com/kubeflow/pipelines/3f4b80127f35e40760eeb1813ce1d3f641502222/components/gcp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "bigquery_query_op = component_store.load_component('bigquery/query')\n",
    "mlengine_train_op = component_store.load_component('ml_engine/train')\n",
    "\n",
    "mlengine_deploy_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/3f4b80127f35e40760eeb1813ce1d3f641502222/components/gcp/ml_engine/deploy/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_best_run_op = comp.func_to_container_op(retrieve_best_run, base_image=BASE_IMAGE_URI)\n",
    "evaluate_model_op = comp.func_to_container_op(evaluate_model, base_image=BASE_IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define deployment operation on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def generate_sampling_query(source_table_name, num_lots, lots):\n",
    "    \"\"\"Prepares the data sampling query.\"\"\"\n",
    "\n",
    "    sampling_query_template = \"\"\"\n",
    "       SELECT *\n",
    "       FROM \n",
    "           `{{ source_table }}` AS cover\n",
    "       WHERE \n",
    "       MOD(ABS(FARM_FINGERPRINT(TO_JSON_STRING(cover))), {{ num_lots }}) IN ({{ lots }})\n",
    "       \"\"\"\n",
    "    query = Template(sampling_query_template).render(\n",
    "      source_table=source_table_name, num_lots=num_lots, lots=str(lots)[1:-1])\n",
    "\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_FILE_PATH = 'datasets/training/data.csv'\n",
    "VALIDATION_FILE_PATH = 'datasets/validation/data.csv'\n",
    "TESTING_FILE_PATH = 'datasets/testing/data.csv'\n",
    "\n",
    "# Parameter defaults\n",
    "SPLITS_DATASET_ID = 'splits'\n",
    "HYPERTUNE_SETTINGS = \"\"\"\n",
    "{\n",
    "    \"hyperparameters\":  {\n",
    "        \"goal\": \"MAXIMIZE\",\n",
    "        \"maxTrials\": 6,\n",
    "        \"maxParallelTrials\": 3,\n",
    "        \"hyperparameterMetricTag\": \"accuracy\",\n",
    "        \"enableTrialEarlyStopping\": True,\n",
    "        \"params\": [\n",
    "            {\n",
    "                \"parameterName\": \"max_iter\",\n",
    "                \"type\": \"DISCRETE\",\n",
    "                \"discreteValues\": [500, 1000]\n",
    "            },\n",
    "            {\n",
    "                \"parameterName\": \"alpha\",\n",
    "                \"type\": \"DOUBLE\",\n",
    "                \"minValue\": 0.0001,\n",
    "                \"maxValue\": 0.001,\n",
    "                \"scaleType\": \"UNIT_LINEAR_SCALE\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='Covertype Classifier Training',\n",
    "    description='The pipeline training and deploying the Covertype classifierpipeline_yaml'\n",
    ")\n",
    "def covertype_pipeline(\n",
    "    project_id: types.GCPProjectID,\n",
    "    region: types.GCPRegion,\n",
    "    source_table_name: types.String,\n",
    "    gcs_root: types.GCSPath,\n",
    "    dataset_id: str,\n",
    "    evaluation_metric_name: str,\n",
    "    evaluation_metric_threshold: float,\n",
    "    model_id: str,\n",
    "    replace_existing_version: bool,\n",
    "    hypertune_settings: types.Dict = HYPERTUNE_SETTINGS,\n",
    "    dataset_location: str = 'US'\n",
    "):\n",
    "    \"\"\"Orchestrates training and deployment of an sklearn model.\"\"\"\n",
    "\n",
    "    # Create the training split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[1, 2, 3, 4])\n",
    "\n",
    "    training_file_path = '{}/{}'.format(gcs_root, TRAINING_FILE_PATH)\n",
    "\n",
    "    create_training_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=training_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the validation split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[8])\n",
    "\n",
    "    validation_file_path = '{}/{}'.format(gcs_root, VALIDATION_FILE_PATH)\n",
    "\n",
    "    create_validation_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=validation_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Create the testing split\n",
    "    query = generate_sampling_query(\n",
    "        source_table_name=source_table_name, num_lots=10, lots=[9])\n",
    "\n",
    "    testing_file_path = '{}/{}'.format(gcs_root, TESTING_FILE_PATH)\n",
    "\n",
    "    create_testing_split = bigquery_query_op(\n",
    "        query=query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=dataset_id,\n",
    "        table_id='',\n",
    "        output_gcs_path=testing_file_path,\n",
    "        dataset_location=dataset_location)\n",
    "\n",
    "    # Tune hyperparameters\n",
    "    tune_args = [\n",
    "      '--training_dataset_path',\n",
    "      create_training_split.outputs['output_gcs_path'],\n",
    "      '--validation_dataset_path',\n",
    "      create_validation_split.outputs['output_gcs_path'], '--hptune', 'True'\n",
    "    ]\n",
    "\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir/hypertune',\n",
    "                              kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "\n",
    "    hypertune = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAIN_IMAGE_URI,\n",
    "        job_dir=job_dir,\n",
    "        args=tune_args,\n",
    "        training_input=hypertune_settings)\n",
    "\n",
    "    # Retrieve the best trial\n",
    "    get_best_trial = retrieve_best_run_op(project_id, hypertune.outputs['job_id'])\n",
    "\n",
    "    # Train the model on a combined training and validation datasets\n",
    "    job_dir = '{}/{}/{}'.format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    train_args = [\n",
    "        '--training_dataset_path',\n",
    "        create_training_split.outputs['output_gcs_path'],\n",
    "        '--validation_dataset_path',\n",
    "        create_validation_split.outputs['output_gcs_path'], '--alpha',\n",
    "        get_best_trial.outputs['alpha'], '--max_iter',\n",
    "        get_best_trial.outputs['max_iter'], '--hptune', 'False'\n",
    "    ]\n",
    "\n",
    "    train_model = mlengine_train_op(\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        master_image_uri=TRAIN_IMAGE_URI,\n",
    "        job_dir=job_dir,\n",
    "        args=train_args)\n",
    "\n",
    "    # Evaluate the model on the testing split\n",
    "    eval_model = evaluate_model_op(\n",
    "        dataset_path=str(create_testing_split.outputs['output_gcs_path']),\n",
    "        model_path=str(train_model.outputs['job_dir']),\n",
    "        metric_name=evaluation_metric_name)\n",
    "\n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(eval_model.outputs['metric_value'] > evaluation_metric_threshold):\n",
    "        deploy_model = mlengine_deploy_op(\n",
    "            model_uri=train_model.outputs['job_dir'],\n",
    "            project_id=project_id,\n",
    "            model_id=model_id,\n",
    "            runtime_version=\"1.14\",\n",
    "            python_version=\"3.5\",\n",
    "            replace_existing_version=True, \n",
    "            set_default=True)\n",
    "\n",
    "    kfp.dsl.get_pipeline_conf().add_op_transformer(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = covertype_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/kfp/components/_data_passing.py:154: UserWarning: There are no registered serializers from type \"bool\" to type \"Bool\", so the value will be serializers as string \"True\".\n",
      "  serialized_value),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/experiments/details/5068b555-c9ee-42f0-89a2-e3eb1be8f382\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/runs/details/227e2fc9-2414-4c7d-944b-f0ed214c139f\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'covertype_kubeflow'\n",
    "\n",
    "arguments = {\n",
    "    'project_id': PROJECT_ID,\n",
    "    'gcs_root': GCS_STAGING_PATH,\n",
    "    'region': AIP_REGION,\n",
    "    'source_table_name': 'covertype_dataset.covertype',\n",
    "    'dataset_id': 'splits',\n",
    "    'evaluation_metric_name': 'accuracy',\n",
    "    'evaluation_metric_threshold': 0.69,\n",
    "    'model_id': 'covertype_classifier',\n",
    "    'replace_existing_version': True\n",
    "}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualPython35",
   "language": "python",
   "name": "virtualpython35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
