{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KubeFlow Pipeline local development quickstart\n",
    "\n",
    "In this notebook, we will demo: \n",
    "\n",
    "* Author components with the lightweight method and ContainerOp based on existing images.\n",
    "* Author pipelines.\n",
    "\n",
    "**Note: Make sure that you have docker installed in the local environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# PROJECT_ID is used to construct the docker image registry. We will use Google Container Registry, \n",
    "# but any other accessible registry works as well. \n",
    "PROJECT_ID='trykube-248403'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pipeline SDK\n",
    "# !pip3 install kfp --upgrade\n",
    "# !mkdir -p tmp/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two ways to author a component to list blobs in a GCS bucket\n",
    "A pipeline is composed of one or more components. In this section, you will build a single component that lists the blobs in a GCS bucket. Then you build a pipeline that consists of this component. There are two ways to author a component. In the following sections we will go through each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a lightweight python component from a Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Define component function\n",
    "The requirements for the component function:\n",
    "* The function must be stand-alone.\n",
    "* The function can only import packages that are available in the base image.\n",
    "* If the function operates on numbers, the parameters must have type hints. Supported types are `int`, `float`, `bool`. Everything else is passed as `str`, that is, string.\n",
    "* To build a component with multiple output values, use Pythonâ€™s `typing.NamedTuple` type hint syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name: str) -> str:\n",
    "    '''Lists all the blobs in the bucket.'''\n",
    "    import subprocess\n",
    "\n",
    "    subprocess.call(['pip', 'install', '--upgrade', 'google-cloud-storage'])\n",
    "    from google.cloud import storage\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    list_blobs_response = bucket.list_blobs()\n",
    "    blobs = ','.join([blob.name for blob in list_blobs_response])\n",
    "    print(blobs)\n",
    "    return blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create a lightweight Python component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as comp\n",
    "\n",
    "# Converts the function to a lightweight Python component.\n",
    "list_blobs_op = comp.func_to_container_op(list_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "\n",
    "# Defines the pipeline.\n",
    "@dsl.pipeline(name='List GCS blobs', description='Lists GCS blobs.')\n",
    "def pipeline_func(bucket_name):\n",
    "    list_blobs_task = list_blobs_op(bucket_name)\n",
    "\n",
    "# Compile the pipeline to a file.\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(pipeline_func, 'list_blobs.pipeline.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Wrap an existing Docker container image using `ContainerOp`\n",
    "Since **docker** is not installed in the kubeflow notebook, the following cells cannot be run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a Docker container\n",
    "Create your own container image that includes your program. If your component creates some outputs to be fed as inputs to the downstream components, each separate output must be written as a string to a separate local text file inside the container image. For example, if a trainer component needs to output the trained model path, it can write the path to a local file `/output.txt`. The string written to an output file cannot be too big. If it is too big (>> 100 kB), it is recommended to save the output to an external persistent storage and pass the storage path to the next component.\n",
    "\n",
    "Start by entering the value of your Google Cloud Platform Project ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates a file `app.py` that contains a Python script. The script takes a GCS bucket name as an input argument, gets the lists of blobs in that bucket, prints the list of blobs and also writes them to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create folders if they don't exist.\n",
    "mkdir -p tmp/components/list-gcs-blobs\n",
    "\n",
    "# Create the Python file that lists GCS blobs.\n",
    "cat > ./tmp/components/list-gcs-blobs/app.py <<HERE\n",
    "import argparse\n",
    "from google.cloud import storage\n",
    "# Parse agruments.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--bucket', type=str, required=True, help='GCS bucket name.')\n",
    "args = parser.parse_args()\n",
    "# Create a client.\n",
    "storage_client = storage.Client()\n",
    "# List blobs.\n",
    "bucket = storage_client.get_bucket(args.bucket)\n",
    "list_blobs_response = bucket.list_blobs()\n",
    "blobs = ','.join([blob.name for blob in list_blobs_response])\n",
    "print(blobs)\n",
    "with open('/blobs.txt', 'w') as f:\n",
    "  f.write(blobs)\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a container that runs the script. Start by creating a `Dockerfile`. A `Dockerfile` contains the instructions to assemble a Docker image. The `FROM` statement specifies the Base Image from which you are building. `WORKDIR` sets the working directory. When you assemble the Docker image, `COPY` will copy the required files and directories (for example, `app.py`) to the filesystem of the container. `RUN` will execute a command (for example, install the dependencies) and commits the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create Dockerfile.\n",
    "cat > ./tmp/components/list-gcs-blobs/Dockerfile <<EOF\n",
    "FROM python:3.6-slim\n",
    "WORKDIR /app\n",
    "COPY . /app\n",
    "RUN pip install --upgrade google-cloud-storage\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our Dockerfile we can create our Docker image. Then we need to push the image to a registry to host the image. Now create a Shell script that builds a container image and stores it in the Google Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"{PROJECT_ID}\"\n",
    "\n",
    "IMAGE_NAME=\"listgcsblobs\"\n",
    "TAG=\"latest\" # \"v_$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "# Create script to build docker image and push it.\n",
    "cat > ./tmp/components/list-gcs-blobs/build_image.sh <<HERE\n",
    "PROJECT_ID=\"${1}\"\n",
    "IMAGE_NAME=\"${IMAGE_NAME}\"\n",
    "TAG=\"${TAG}\"\n",
    "GCR_IMAGE=\"gcr.io/\\${PROJECT_ID}/\\${IMAGE_NAME}:\\${TAG}\"\n",
    "docker build -t \\${IMAGE_NAME} .\n",
    "docker tag \\${IMAGE_NAME} \\${GCR_IMAGE}\n",
    "docker push \\${GCR_IMAGE}\n",
    "docker image rm \\${IMAGE_NAME}\n",
    "docker image rm \\${GCR_IMAGE}\n",
    "HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Build and push the image.\n",
    "cd tmp/components/list-gcs-blobs\n",
    "bash build_image.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define each component\n",
    "Define a component by creating an instance of `kfp.dsl.ContainerOp` that describes the interactions with the Docker container image created in the previous step. You need to specify the component name, the image to use, the command to run after the container starts, the input arguments, and the file outputs. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl\n",
    "\n",
    "def list_gcs_blobs_op(name, bucket):\n",
    "    return kfp.dsl.ContainerOp(\n",
    "      name=name,\n",
    "      image='gcr.io/{}/listgcsblobs:latest'.format(PROJECT_ID),\n",
    "      command=['python', '/app/app.py'],\n",
    "      file_outputs={'blobs': '/blobs.txt'},\n",
    "      arguments=['--bucket', bucket]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Create your workflow as a Python function\n",
    "Start by creating a folder to store the pipeline file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders if they don't exist.\n",
    "!mkdir -p tmp/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your pipeline as a Python function. ` @kfp.dsl.pipeline` is a required decoration including `name` and `description` properties. Then compile the pipeline function. After the compilation is completed, a pipeline file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import kfp.compiler as compiler\n",
    "\n",
    "# Define the pipeline\n",
    "@kfp.dsl.pipeline(\n",
    "  name='List GCS Blobs',\n",
    "  description='Takes a GCS bucket name as input and lists the blobs.'\n",
    ")\n",
    "def pipeline_func(bucket='Enter your bucket name here.'):\n",
    "    list_blobs_task = list_gcs_blobs_op('List', bucket)\n",
    "\n",
    "# Compile the pipeline to a file.\n",
    "filename = 'tmp/pipelines/list_blobs{dt:%Y%m%d_%H%M%S}.pipeline.tar.gz'.format(\n",
    "    dt=datetime.datetime.now())\n",
    "compiler.Compiler().compile(pipeline_func, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [instructions](https://www.kubeflow.org/docs/other-guides/accessing-uis/) on kubeflow.org to access Kubeflow UIs. Upload the created pipeline and run it.\n",
    "\n",
    "**Warning:** When the pipeline is run, it pulls the image from the repository to the Kubernetes cluster to create a container. Kubernetes caches pulled images. One solution is to use the image digest instead of the tag in your component dsl, for example, `s/v1/sha256:9509182e27dcba6d6903fccf444dc6188709cc094a018d5dd4211573597485c9/g`. Alternatively, if you don't want to update the digest every time, you can try `:latest` tag, which will force the k8s to always pull the latest image.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
