{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFX Pipeline from Pre-build Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline demonstrates the TFX capabilities at scale. The pipeline uses a public BigQuery dataset and uses GCP services to preprocess data (Dataflow) and train the model (Cloud ML Engine). The model is then deployed to kubeflow for prediction service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './config/kubeflow-pipeline-fantasy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameter"
    ]
   },
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "PROJECT_ID='kubeflow-pipeline-fantasy'\n",
    "GCS_BUCKET='gs://kubeflow-pipeline-ui'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "If you run this notebook **outside** of a Kubeflow cluster, run the following command:\n",
    "- `host`: The URL of your Kubeflow Pipelines instance, for example \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- `client_id`: The client ID used by Identity-Aware Proxy\n",
    "- `other_client_id`: The client ID used to obtain the auth codes and refresh tokens.\n",
    "- `other_client_secret`: The client secret used to obtain the auth codes and refresh tokens.\n",
    "\n",
    "```python\n",
    "client = kfp.Client(host, client_id, other_client_id, other_client_secret)\n",
    "```\n",
    "\n",
    "If you run this notebook **within** a Kubeflow cluster, run the following command:\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```\n",
    "\n",
    "You'll need to create OAuth client ID credentials of type `Other` to get `other_client_id` and `other_client_secret`. Learn more about [creating OAuth credentials](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside Kubeflow cluster\n",
    "\n",
    "# The host for full deployment of Kubeflow ends with '/pipeline'\n",
    "HOST = 'https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline'\n",
    "# Full deployment of Kubeflow on GCP is usually protected through IAP, therefore the following \n",
    "# will be needed to access the endpoint\n",
    "CLIENT_ID = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_ID = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_SECRET = \"cB8Xj-rb9JWCYcCRDlpTMfhc\"\n",
    "\n",
    "# # The host for managed 'AI Platform Pipeline' ends with 'pipelines.googleusercontent.com'\n",
    "# HOST = 'https://7c021d0340d296aa-dot-us-central2.pipelines.googleusercontent.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya29.a0Adw1xeVU2xFIGr6OPo10YNC7n04mElu4u1hadTABZuRBnVL-AXJ26XqLW8TpP09xYQebVmlae92PC6-DKBi6DFWV9eIjd8hdnY3qvIQfH2WAbTuk_d3SFc9Eyaw8SH425GMYAh0Ty9djbg3Bz9m4BHfMg6YeTT9NF_9sFyEJGPkz\r\n"
     ]
    }
   ],
   "source": [
    "# This is to ensure the proper access token is present to reach the end point for managed 'AI Platform Pipeline'\n",
    "# If you are not working with managed 'AI Platform Pipeline', this step is not necessary\n",
    "! gcloud auth print-access-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    client = kfp.Client(host=HOST, \n",
    "                        client_id=CLIENT_ID,\n",
    "                        other_client_id=OTHER_CLIENT_ID, \n",
    "                        other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-build components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_tf_data_validation_op  = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/0b07e456b1f319d8b7a7301274f55c00fda9f537/components/dataflow/tfdv/component.yaml')\n",
    "dataflow_tf_transform_op        = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/0b07e456b1f319d8b7a7301274f55c00fda9f537/components/dataflow/tft/component.yaml')\n",
    "tf_train_op                     = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/0b07e456b1f319d8b7a7301274f55c00fda9f537/components/kubeflow/dnntrainer/component.yaml')\n",
    "dataflow_tf_model_analyze_op    = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/0b07e456b1f319d8b7a7301274f55c00fda9f537/components/dataflow/tfma/component.yaml')\n",
    "dataflow_tf_predict_op          = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/0b07e456b1f319d8b7a7301274f55c00fda9f537/components/dataflow/predict/component.yaml')\n",
    "\n",
    "confusion_matrix_op             = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/0b07e456b1f319d8b7a7301274f55c00fda9f537/components/local/confusion_matrix/component.yaml')\n",
    "roc_op                          = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/0b07e456b1f319d8b7a7301274f55c00fda9f537/components/local/roc/component.yaml')\n",
    "\n",
    "kubeflow_deploy_op              = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/0b07e456b1f319d8b7a7301274f55c00fda9f537/components/kubeflow/deployer/component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  name='TFX Taxi Cab Classification Pipeline Example',\n",
    "  description='Example pipeline that does classification with model analysis based on a public BigQuery dataset.'\n",
    ")\n",
    "def taxi_cab_classification(\n",
    "    output,\n",
    "    project,\n",
    "    column_names='gs://ml-pipeline-playground/tfx/taxi-cab-classification/column-names.json',\n",
    "    key_columns='trip_start_timestamp',\n",
    "    train='gs://ml-pipeline-playground/tfx/taxi-cab-classification/train.csv',\n",
    "    evaluation='gs://ml-pipeline-playground/tfx/taxi-cab-classification/eval.csv',\n",
    "    mode='local',\n",
    "    preprocess_module='gs://ml-pipeline-playground/tfx/taxi-cab-classification/preprocessing.py',\n",
    "    learning_rate=0.1,\n",
    "    hidden_layer_size='1500',\n",
    "    steps=3000,\n",
    "    analyze_slice_column='trip_start_hour'\n",
    "):\n",
    "    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\n",
    "    target_lambda = \"\"\"lambda x: (x['target'] > x['fare'] * 0.2)\"\"\"\n",
    "    target_class_lambda = \"\"\"lambda x: 1 if (x['target'] > x['fare'] * 0.2) else 0\"\"\"\n",
    "\n",
    "    tf_server_name = 'taxi-cab-classification-model-{{workflow.uid}}'\n",
    "\n",
    "    validation = dataflow_tf_data_validation_op(\n",
    "        inference_data=train,\n",
    "        validation_data=evaluation,\n",
    "        column_names=column_names,\n",
    "        key_columns=key_columns,\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        validation_output=output_template,\n",
    "    )\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(\n",
    "        training_data_file_pattern=train,\n",
    "        evaluation_data_file_pattern=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        preprocessing_module=preprocess_module,\n",
    "        transformed_data_dir=output_template\n",
    "    )\n",
    "\n",
    "    training = tf_train_op(\n",
    "        transformed_data_dir=preprocess.output,\n",
    "        schema=validation.outputs['schema'],\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        steps=steps,\n",
    "        optimizer='Adagrad',\n",
    "        target='tips',\n",
    "        preprocessing_module=preprocess_module,\n",
    "        training_output_dir=output_template\n",
    "    )\n",
    "\n",
    "    analysis = dataflow_tf_model_analyze_op(\n",
    "        model=training.output,\n",
    "        evaluation_data=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        slice_columns=analyze_slice_column,\n",
    "        analysis_results_dir=output_template\n",
    "    )\n",
    "\n",
    "    prediction = dataflow_tf_predict_op(\n",
    "        data_file_pattern=evaluation,\n",
    "        schema=validation.outputs['schema'],\n",
    "        target_column='tips',\n",
    "        model=training.output,\n",
    "        batch_size=32,\n",
    "        run_mode=mode,\n",
    "        gcp_project=project,\n",
    "        predictions_dir=output_template\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix_op(\n",
    "        predictions=prediction.output,\n",
    "        target_lambda=target_lambda,\n",
    "        output_dir=output_template\n",
    "    )\n",
    "\n",
    "\n",
    "    deploy = kubeflow_deploy_op(\n",
    "        model_dir=str(training.output) + '/export/export',\n",
    "        server_name=tf_server_name,\n",
    "        cluster_name='kubeflow', \n",
    "        namespace='kubeflow',\n",
    "        pvc_name='', \n",
    "        service_type='ClusterIP'\n",
    "    )\n",
    "\n",
    "\n",
    "    steps = [validation, preprocess, training, analysis, prediction, cm, deploy]\n",
    "    for step in steps:\n",
    "        step.apply(gcp.use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = taxi_cab_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/kfp/components/_data_passing.py:133: UserWarning: Missing type name was inferred as \"Float\" based on the value \"0.1\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/Users/luoshixin/LocalSim/virtualPython35/lib/python3.5/site-packages/kfp/components/_data_passing.py:133: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"3000\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "INFO:root:Creating experiment TFX-From-Components.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/experiments/details/29af70b3-7794-44ac-ab43-aebacb5fc4b4\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/runs/details/e075f37a-9092-4052-b4a7-e73d67c1e271\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'TFX-From-Components'\n",
    "\n",
    "arguments = {\"project\":PROJECT_ID,\n",
    "             \"output\": GCS_BUCKET}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualPython35",
   "language": "python",
   "name": "virtualpython35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
