{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and XGboost Pipeline\n",
    "\n",
    "This tutorial demonstrate building a machine learning pipeling with spark and XGBoost. The pipeline \n",
    "- starts by creating an Google DataProc cluster, and then running analysis, transformation, distributed training and prediction in the created cluster. \n",
    "- Then a single node confusion-matrix and ROC aggregator is used (for classification case) to provide the confusion matrix data, and ROC data to the front end, respectively. \n",
    "- Finally, a delete cluster operation runs to destroy the cluster it creates in the beginning. The delete cluster operation is used as an exit handler, meaning it will run regardless of whether the pipeline fails or not.\n",
    "\n",
    "**Please do not forget to enable the Dataproc API in your cluster** https://console.developers.google.com/apis/api/dataproc.googleapis.com/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import kfp\n",
    "import kfp.gcp as gcp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "import kfp.components as comp\n",
    "import datetime\n",
    "\n",
    "import kubernetes as k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = './config/kubeflow-pipeline-fantasy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameter"
    ]
   },
   "outputs": [],
   "source": [
    "# Required Parameters\n",
    "PROJECT_ID='kubeflow-pipeline-fantasy'\n",
    "GCS_BUCKET='gs://kubeflow-pipeline-ui'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create client\n",
    "\n",
    "If you run this notebook **outside** of a Kubeflow cluster, run the following command:\n",
    "- `host`: The URL of your Kubeflow Pipelines instance, for example \"https://`<your-deployment>`.endpoints.`<your-project>`.cloud.goog/pipeline\"\n",
    "- `client_id`: The client ID used by Identity-Aware Proxy\n",
    "- `other_client_id`: The client ID used to obtain the auth codes and refresh tokens.\n",
    "- `other_client_secret`: The client secret used to obtain the auth codes and refresh tokens.\n",
    "\n",
    "```python\n",
    "client = kfp.Client(host, client_id, other_client_id, other_client_secret)\n",
    "```\n",
    "\n",
    "If you run this notebook **within** a Kubeflow cluster, run the following command:\n",
    "```python\n",
    "client = kfp.Client()\n",
    "```\n",
    "\n",
    "You'll need to create OAuth client ID credentials of type `Other` to get `other_client_id` and `other_client_secret`. Learn more about [creating OAuth credentials](\n",
    "https://cloud.google.com/iap/docs/authentication-howto#authenticating_from_a_desktop_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside Kubeflow cluster\n",
    "\n",
    "# # The host for full deployment of Kubeflow ends with '/pipeline'\n",
    "# HOST = ''\n",
    "# # Full deployment of Kubeflow on GCP is usually protected through IAP, therefore the following \n",
    "# # will be needed to access the endpoint\n",
    "# CLIENT_ID = ''\n",
    "# OTHER_CLIENT_ID = ''\n",
    "# OTHER_CLIENT_SECRET = ''\n",
    "\n",
    "# The host for managed 'AI Platform Pipeline' ends with 'pipelines.googleusercontent.com'\n",
    "HOST = 'https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya29.c.KoAB4Ae2Ka0xwH7PMJheJ-5Hc8IY3CMcxJpR0dyQv2TWe9ysM_OwfF2egxVENos0J7N-LOjgMxyE2yMl4HC9jXD18w2JLT-ltL6YnrWoQjevBoy7BVbjFnDbbZccQDyecP87oJKVGu8hl-aNu0LoKCXDgCFw_K3BvHpcZXVy0ZMb19I\n"
     ]
    }
   ],
   "source": [
    "# This is to ensure the proper access token is present to reach the end point for managed 'AI Platform Pipeline'\n",
    "# If you are not working with managed 'AI Platform Pipeline', this step is not necessary\n",
    "! gcloud auth print-access-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    if HOST.endswith('googleusercontent.com'):\n",
    "        CLIENT_ID = None\n",
    "        OTHER_CLIENT_ID = None\n",
    "        OTHER_CLIENT_SECRET = None\n",
    "\n",
    "    client = kfp.Client(host=HOST, \n",
    "                        client_id=CLIENT_ID,\n",
    "                        other_client_id=OTHER_CLIENT_ID, \n",
    "                        other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reusable components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnose_me_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/566dddfdfc0a6a725b6e50ea85e73d8d5578bbb9/components/diagnostics/diagnose_me/component.yaml')\n",
    "\n",
    "confusion_matrix_op = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/local/confusion_matrix/component.yaml')\n",
    "\n",
    "roc_op = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/1.0.0/components/local/roc/component.yaml')\n",
    "\n",
    "dataproc_create_cluster_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/38771da09094640cd2786a4b5130b26ea140f864/components/gcp/dataproc/create_cluster/component.yaml')\n",
    "\n",
    "dataproc_delete_cluster_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/38771da09094640cd2786a4b5130b26ea140f864/components/gcp/dataproc/delete_cluster/component.yaml')\n",
    "\n",
    "dataproc_submit_pyspark_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/38771da09094640cd2786a4b5130b26ea140f864/components/gcp/dataproc/submit_pyspark_job/component.yaml'\n",
    ")\n",
    "\n",
    "dataproc_submit_spark_op = comp.load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/38771da09094640cd2786a4b5130b26ea140f864/components/gcp/dataproc/submit_spark_job/component.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PYSRC_PREFIX = 'gs://ml-pipeline/sample-pipeline/xgboost' # Common path to python src.\n",
    "\n",
    "_XGBOOST_PKG = 'gs://ml-pipeline/sample-pipeline/xgboost/xgboost4j-example-0.8-SNAPSHOT-jar-with-dependencies.jar'\n",
    "\n",
    "_TRAINER_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostTrainer'\n",
    "\n",
    "_PREDICTOR_MAIN_CLS = 'ml.dmlc.xgboost4j.scala.example.spark.XGBoostPredictor'\n",
    "\n",
    "\n",
    "def delete_directory_from_gcs(dir_path):\n",
    "  \"\"\"Delete a GCS dir recursively. Ignore errors.\"\"\"\n",
    "  try:\n",
    "    subprocess.call(['gsutil', '-m', 'rm', '-r', dir_path])\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data analyze and transform operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc_analyze_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    schema,\n",
    "    train_data,\n",
    "    output):\n",
    "\n",
    "    return dataproc_submit_pyspark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_python_file_uri=os.path.join(_PYSRC_PREFIX, 'analyze_run.py'),\n",
    "      args=['--output', str(output), '--train', str(train_data), '--schema', str(schema)]\n",
    "    )\n",
    "\n",
    "\n",
    "def dataproc_transform_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    target,\n",
    "    analysis,\n",
    "    output\n",
    "):\n",
    "\n",
    "    # Remove existing [output]/train and [output]/eval if they exist.\n",
    "    delete_directory_from_gcs(os.path.join(output, 'train'))\n",
    "    delete_directory_from_gcs(os.path.join(output, 'eval'))\n",
    "\n",
    "    return dataproc_submit_pyspark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_python_file_uri=os.path.join(_PYSRC_PREFIX,\n",
    "                                        'transform_run.py'),\n",
    "      args=[\n",
    "        '--output',\n",
    "        str(output),\n",
    "        '--analysis',\n",
    "        str(analysis),\n",
    "        '--target',\n",
    "        str(target),\n",
    "        '--train',\n",
    "        str(train_data),\n",
    "        '--eval',\n",
    "        str(eval_data)\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training and prediction operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataproc_train_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    target,\n",
    "    analysis,\n",
    "    workers,\n",
    "    rounds,\n",
    "    output,\n",
    "    is_classification=True\n",
    "):\n",
    "\n",
    "    if is_classification:\n",
    "        config='gs://ml-pipeline/sample-data/xgboost-config/trainconfcla.json'\n",
    "    else:\n",
    "        config='gs://ml-pipeline/sample-data/xgboost-config/trainconfreg.json'\n",
    "\n",
    "    return dataproc_submit_spark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_class=_TRAINER_MAIN_CLS,\n",
    "      spark_job=json.dumps({'jarFileUris': [_XGBOOST_PKG]}),\n",
    "      args=json.dumps([\n",
    "        str(config),\n",
    "        str(rounds),\n",
    "        str(workers),\n",
    "        str(analysis),\n",
    "        str(target),\n",
    "        str(train_data),\n",
    "        str(eval_data),\n",
    "        str(output)\n",
    "      ]))\n",
    "\n",
    "\n",
    "def dataproc_predict_op(\n",
    "    project,\n",
    "    region,\n",
    "    cluster_name,\n",
    "    data,\n",
    "    model,\n",
    "    target,\n",
    "    analysis,\n",
    "    output\n",
    "):\n",
    "\n",
    "    return dataproc_submit_spark_op(\n",
    "      project_id=project,\n",
    "      region=region,\n",
    "      cluster_name=cluster_name,\n",
    "      main_class=_PREDICTOR_MAIN_CLS,\n",
    "      spark_job=json.dumps({'jarFileUris': [_XGBOOST_PKG]}),\n",
    "      args=json.dumps([\n",
    "        str(model),\n",
    "        str(data),\n",
    "        str(analysis),\n",
    "        str(target),\n",
    "        str(output)\n",
    "      ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='XGBoost Trainer',\n",
    "    description='A trainer that does end-to-end distributed training for XGBoost models.'\n",
    ")\n",
    "def xgb_train_pipeline(\n",
    "    output=GCS_BUCKET,\n",
    "    project=PROJECT_ID,\n",
    "    diagnostic_mode='HALT_ON_ERROR',\n",
    "    rounds=5,\n",
    "):\n",
    "    output_template = str(output) + '/' + dsl.RUN_ID_PLACEHOLDER + '/data'\n",
    "    region='us-central1'\n",
    "    workers=2\n",
    "    quota_check=[{'region':region,'metric':'CPUS','quota_needed':12.0}]\n",
    "    train_data='gs://ml-pipeline/sample-data/sfpd/train.csv'\n",
    "    eval_data='gs://ml-pipeline/sample-data/sfpd/eval.csv'\n",
    "    schema='gs://ml-pipeline/sample-data/sfpd/schema.json'\n",
    "    true_label='ACTION'\n",
    "    target='resolution'\n",
    "    required_apis='dataproc.googleapis.com'\n",
    "    cluster_name='xgb-%s' % dsl.RUN_ID_PLACEHOLDER\n",
    "\n",
    "    # Current GCP pyspark/spark op do not provide outputs as return values, instead,\n",
    "    # we need to use strings to pass the uri around.\n",
    "    analyze_output = output_template\n",
    "    transform_output_train = os.path.join(output_template, 'train', 'part-*')\n",
    "    transform_output_eval = os.path.join(output_template, 'eval', 'part-*')\n",
    "    train_output = os.path.join(output_template, 'train_output')\n",
    "    predict_output = os.path.join(output_template, 'predict_output')\n",
    "    \n",
    "    _diagnose_me_op = diagnose_me_op(\n",
    "        bucket=output,\n",
    "        execution_mode=diagnostic_mode,\n",
    "        project_id=project, \n",
    "        target_apis=required_apis,\n",
    "        quota_check=quota_check)\n",
    "    \n",
    "    with dsl.ExitHandler(exit_op=dataproc_delete_cluster_op(\n",
    "        project_id=project,\n",
    "        region=region,\n",
    "        name=cluster_name\n",
    "    )):\n",
    "        _create_cluster_op = dataproc_create_cluster_op(\n",
    "            project_id=project,\n",
    "            region=region,\n",
    "            name=cluster_name,\n",
    "            initialization_actions=[\n",
    "              os.path.join(_PYSRC_PREFIX,\n",
    "                           'initialization_actions.sh'),\n",
    "            ],\n",
    "            image_version='1.2'\n",
    "        ).after(_diagnose_me_op)\n",
    "\n",
    "        _analyze_op = dataproc_analyze_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            schema=schema,\n",
    "            train_data=train_data,\n",
    "            output=output_template\n",
    "        ).after(_create_cluster_op).set_display_name('Analyzer')\n",
    "\n",
    "        _transform_op = dataproc_transform_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            train_data=train_data,\n",
    "            eval_data=eval_data,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            output=output_template\n",
    "        ).after(_analyze_op).set_display_name('Transformer')\n",
    "\n",
    "        _train_op = dataproc_train_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            train_data=transform_output_train,\n",
    "            eval_data=transform_output_eval,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            workers=workers,\n",
    "            rounds=rounds,\n",
    "            output=train_output\n",
    "        ).after(_transform_op).set_display_name('Trainer')\n",
    "\n",
    "        _predict_op = dataproc_predict_op(\n",
    "            project=project,\n",
    "            region=region,\n",
    "            cluster_name=cluster_name,\n",
    "            data=transform_output_eval,\n",
    "            model=train_output,\n",
    "            target=target,\n",
    "            analysis=analyze_output,\n",
    "            output=predict_output\n",
    "        ).after(_train_op).set_display_name('Predictor')\n",
    "\n",
    "        _cm_op = confusion_matrix_op(\n",
    "            predictions=os.path.join(predict_output, 'part-*.csv'),\n",
    "            output_dir=output_template\n",
    "        ).after(_predict_op)\n",
    "\n",
    "        _roc_op = roc_op(\n",
    "            predictions_dir=os.path.join(predict_output, 'part-*.csv'),\n",
    "            true_class=true_label,\n",
    "            true_score_column=true_label,\n",
    "            output_dir=output_template\n",
    "        ).after(_predict_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = xgb_train_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"5\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com/#/experiments/details/caf9d059-4605-48e4-a39b-8aff6e29eda7\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://69a95965149a4145-dot-asia-east1.pipelines.googleusercontent.com/#/runs/details/1c9f9234-5303-4a24-a2d1-da7928900293\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'Spark-and-XGBoost'\n",
    "\n",
    "arguments = {\"project\":PROJECT_ID,\n",
    "             \"output\": GCS_BUCKET}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
