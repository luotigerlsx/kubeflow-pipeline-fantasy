{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is largely based on that from https://github.com/jarokaz/mlops-labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating AutoML Tables training and deployment with Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you develop a continous training and deployment pipeline using Kubeflow Pipelines, BigQuery and AutoML Tables.\n",
    "\n",
    "The scenario used in the lab is  predicting customer lifetime value (CLV).\n",
    "\n",
    "The goal of CLV modeling is to identify the most valuable customers - customers that are going to generate the highest value in a given future time range. The CLV models are built from a variety of data sources - historical sales data being the most important one and in many cases the only one. \n",
    "\n",
    "Predicting Customer Lifetime Value (CLV)  is a representative example of a use case where you may need to fine-tune and re-train a predictive model on a frequent basis. As there is a constant flow of new sales transactions that constitute the core of training data, models have to be kept up to date with evolving purchase patterns. Automation of model training and deployment is critical. \n",
    "\n",
    "In the CLV model developed in this lab, the historical sales transactions are preprocessed and aggregated to engineer a set of latent features  representing the so-called RFM characteristics of your customers:\n",
    "- Recency: How active have they been recently?\n",
    "- Frequency: How often do they buy?\n",
    "- Monetary: What amount do they spend?\n",
    "\n",
    "The following diagram shows a succession of past sales for a set of four customers.\n",
    "\n",
    "![clv_timeline](../images/clv-timeline.png)\n",
    "\n",
    "The diagram illustrates the RFM values for the customers, showing for each customer:\n",
    "- Recency: The time between the last purchase and today, represented by the distance between the leftmost circle and the vertical dotted line that's labeled **Now**.\n",
    "- Frequency: The time between purchaes, represented by the distance between the circles on a single line.\n",
    "- Monetary: The amount of money spent on each purchase, represented by the size of the circle.\n",
    "\n",
    "As demonstrated in the lab you usually create multiple features per each characteristic. For example, in the lab, Recency is captured by two features: *recency* and *T*.\n",
    "\n",
    "The RFM input features and the target label are engineered using the following process:\n",
    "- A time series of of historical sales transactions for a given customer is divided into two time periods: *the features period* and *the predict period*. A point in time that is used to divide the time series is referred two as *the threshold date*. \n",
    "- The transactions in *the features period* are aggregated to create the latent RFM input features \n",
    "- The transactions in *the predict period* are aggregated to calculate the target label representing the expected total value of the customer\n",
    "\n",
    "This process results in a single example per customer and a set of examples across a customer population constitutes a training set.\n",
    "\n",
    "The pipeline implemented in the lab, uses BigQuery as a source of historical sales transactions. BigQuery is also used to engineer RFM features. The model is then trained and deployed using AutoML Tables. The below diagram represents the control and data flow implemented by the pipeline.\n",
    "\n",
    "\n",
    "![Training pipeline](../images/clv_train.png)\n",
    "\n",
    "1. The BQ query is run to process sales transactions in the *transactions* table into RFM features in the *features* table. \n",
    "1. The data from the *features* table is imported into the AutoML dataset\n",
    "1. The AutoML model is trained on the imported dataset\n",
    "1. After the training completes the evaluation metrics are retrieved and compared against the performance threshold\n",
    "1. If the model performs better than the threshold the model is deployed to AutoML Deployment\n",
    "\n",
    "The sample dataset used in the lab is based on the publicaly available [Online Retail Data Set](http://archive.ics.uci.edu/ml/datasets/Online+Retail) from the UCI Machine Learning Repository. The original dataset was preprocessed to conform to the following schema:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| customer_id | string | A unique customer ID |\n",
    "| order_date | date (yyyy-MM-dd) | The date of a transaction. Transactions (potentially from multiple invoices) are grouped by day |\n",
    "| quantity | integer | A number of items of a single SKU in a transaction |\n",
    "| unit_price | float | A unit price of a SKU |\n",
    "\n",
    "The feature engineering query generates the features table with the below schema. \n",
    "\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| monetary | Float | The total spend by a customer in the features period|\n",
    "| frequency | Integer | The number of transactions placed by a customer in the features period |\n",
    "| recency | Integer |  The time (in days) between the first and the last orders in the features period |\n",
    "| T | Integer | The time between the first order placed and in the threshold date|\n",
    "| time_between | Float |  The average time betwee orders in the features period |\n",
    "| avg_basket_value | Float |  The averate monetary value of the customer's basket in the features period |\n",
    "| avg_basket_size | Float |  The average number of items in a basket in the features perio|\n",
    "| cnt_returns | Integer |  The number of returns in the features period|\n",
    "| target_monetary | Float | The total amount spent in the predict period. This is the label for predictions|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare lab environment\n",
    "Let's start with configuring your GCP environment settings and uploading the sales transactions into BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from jinja2 import Template\n",
    "\n",
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/luoshixin/LocalDevelop/kubeflow-pipeline/kubeflow-pipeline/config/kubeflow-pipeline-fantasy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! gcloud services enable automl.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Automl permission to \"Service Account used for Kubeflow user actions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set lab settings\n",
    "Make sure to update the following values with you environment settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'kubeflow-pipeline-fantasy'\n",
    "DATASET_LOCATION = 'US'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = 'lab_automl'\n",
    "TRANSACTIONS_TABLE_ID = 'transactions'\n",
    "TRANSACTIONS_TABLE_SCHEMA = 'customer_id:STRING,order_date:DATE,quantity:INTEGER,unit_price:FLOAT'\n",
    "TRANSACTIONS_SOURCE_FILE='gs://kubeflow-pipeline-ui/cl_data/transactions.csv'\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/gcp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional Parameters, but required for running outside Kubeflow cluster\n",
    "\n",
    "# The host for full deployment of Kubeflow ends with '/pipeline'\n",
    "HOST = 'https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline'\n",
    "# Full deployment of Kubeflow on GCP is usually protected through IAP, therefore the following \n",
    "# will be needed to access the endpoint\n",
    "CLIENT_ID = \"493831447550-os23o55235htd9v45a9lsejv8d1plhd0.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_ID = \"493831447550-iu24vv6id3ng5smhf2lboovv5qukuhbh.apps.googleusercontent.com\"\n",
    "OTHER_CLIENT_SECRET = \"cB8Xj-rb9JWCYcCRDlpTMfhc\"\n",
    "\n",
    "# # The host for managed 'AI Platform Pipeline' ends with 'pipelines.googleusercontent.com'\n",
    "# HOST = 'https://7c021d0340d296aa-dot-us-central2.pipelines.googleusercontent.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya29.a0Adw1xeUZ7rsarGqP72F8duIt5a5RNJfMJ153FV6VvouDTB1HGCO1uJe_sFCxKQuu1NDsV50Zd3dGv5_08aD_bToCWURSUTekSm5mdDuBwKapAp3AN1nR0Ob2wzJizhf-Ei4x8jJ47HxsFoWTo1yHsC6m9VONXbqoPafBnXEHqu_8\n"
     ]
    }
   ],
   "source": [
    "# This is to ensure the proper access token is present to reach the end point for managed 'AI Platform Pipeline'\n",
    "# If you are not working with managed 'AI Platform Pipeline', this step is not necessary\n",
    "! gcloud auth print-access-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create kfp client\n",
    "in_cluster = True\n",
    "try:\n",
    "  k8s.config.load_incluster_config()\n",
    "except:\n",
    "  in_cluster = False\n",
    "  pass\n",
    "\n",
    "if in_cluster:\n",
    "    client = kfp.Client()\n",
    "else:\n",
    "    if HOST.endswith('googleusercontent.com'):\n",
    "        CLIENT_ID = None\n",
    "        OTHER_CLIENT_ID = None\n",
    "        OTHER_CLIENT_SECRET = None\n",
    "\n",
    "    client = kfp.Client(host=HOST, \n",
    "                        client_id=CLIENT_ID,\n",
    "                        other_client_id=OTHER_CLIENT_ID, \n",
    "                        other_client_secret=OTHER_CLIENT_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a BigQuery dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'kubeflow-pipeline-fantasy:lab_automl'\n",
      "already exists.\n"
     ]
    }
   ],
   "source": [
    "!bq --location=$DATASET_LOCATION --project_id=$PROJECT_ID mk --dataset $DATASET_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sale transactions data to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r5379bf337e10ab6f_000001711a7e4cef_1 ... (3s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq --project_id=$PROJECT_ID --dataset_id=$DATASET_ID load \\\n",
    "--source_format=CSV \\\n",
    "--skip_leading_rows=1 \\\n",
    "--replace \\\n",
    "$TRANSACTIONS_TABLE_ID \\\n",
    "$TRANSACTIONS_SOURCE_FILE \\\n",
    "$TRANSACTIONS_TABLE_SCHEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the dataset\n",
    "To query data in BigQuery you can use BigQuery Python client library ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-05-12</td>\n",
       "      <td>256</td>\n",
       "      <td>1.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16701</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13941</td>\n",
       "      <td>2011-06-21</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14258</td>\n",
       "      <td>2011-07-11</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18102</td>\n",
       "      <td>2011-07-28</td>\n",
       "      <td>256</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>256</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>256</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-08-09</td>\n",
       "      <td>256</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14646</td>\n",
       "      <td>2011-08-11</td>\n",
       "      <td>256</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customer_id  order_date  quantity  unit_price\n",
       "0       14646  2011-05-12       256        1.65\n",
       "1       16553  2011-05-18       256        0.36\n",
       "2       16701  2011-05-18       256        0.36\n",
       "3       13941  2011-06-21       256        0.36\n",
       "4       14258  2011-07-11       256        0.36\n",
       "5       18102  2011-07-28       256        5.88\n",
       "6       14646  2011-08-09       256        0.72\n",
       "7       14646  2011-08-09       256        0.72\n",
       "8       14646  2011-08-09       256        0.72\n",
       "9       14646  2011-08-11       256        2.55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_template = \"\"\"\n",
    "SELECT *\n",
    "FROM `{{ source_table }}`\n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "\n",
    "query = Template(query_template).render(\n",
    "    source_table='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TRANSACTIONS_TABLE_ID))\n",
    "\n",
    "df = bq_client.query(query).to_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or Jupyter the `%%bigquery` magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>256</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-03-10</td>\n",
       "      <td>2</td>\n",
       "      <td>12.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-02-08</td>\n",
       "      <td>4</td>\n",
       "      <td>12.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-03-10</td>\n",
       "      <td>4</td>\n",
       "      <td>5.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-05-18</td>\n",
       "      <td>6</td>\n",
       "      <td>2.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-03-10</td>\n",
       "      <td>120</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-06-29</td>\n",
       "      <td>144</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>16553</td>\n",
       "      <td>2010-12-14</td>\n",
       "      <td>192</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>16553</td>\n",
       "      <td>2011-04-12</td>\n",
       "      <td>-4</td>\n",
       "      <td>12.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>16553</td>\n",
       "      <td>2010-12-08</td>\n",
       "      <td>-1</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  order_date  quantity  unit_price\n",
       "0        16553  2011-05-18       256        0.36\n",
       "1        16553  2011-03-10         2       12.75\n",
       "2        16553  2011-02-08         4       12.75\n",
       "3        16553  2011-03-10         4        5.95\n",
       "4        16553  2011-05-18         6        2.95\n",
       "..         ...         ...       ...         ...\n",
       "81       16553  2011-03-10       120        0.85\n",
       "82       16553  2011-06-29       144        0.53\n",
       "83       16553  2010-12-14       192        0.85\n",
       "84       16553  2011-04-12        -4       12.75\n",
       "85       16553  2010-12-08        -1        4.25\n",
       "\n",
       "[86 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT_ID\n",
    "SELECT *\n",
    "FROM `lab_automl.transactions`\n",
    "WHERE customer_id='16553'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are multiple sales transactions per customer. They represent the purchasing history and behavior of a given customer. For example, \n",
    "the customer identified by 16553 has 85 orders. Most of them are new purchases. Some of them are returns - the records with a negative quantity.\n",
    "\n",
    "The feature engineering query converts these 85 records into a single record representing the RFM charateristics of this customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the KFP training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create component factories for the pre-defined GCP components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None,\n",
    "    url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "    \n",
    "automl_create_dataset_op = component_store.load_component('automl/create_dataset_for_tables')\n",
    "automl_import_data_from_bq_op = component_store.load_component('automl/import_data_from_bigquery')\n",
    "automl_create_model__op = component_store.load_component('automl/create_model_for_tables')\n",
    "automl_split_dataset_table_column_names_op = component_store.load_component('automl/split_dataset_table_column_names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base docker image for the custom components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.7\n",
    "RUN pip3 install --upgrade google-cloud-bigquery google-api-core google-cloud-automl grpcio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME=\"lab_automl_components\"\n",
    "IMAGE_URI=\"gcr.io/{}/{}:latest\".format(PROJECT_ID, IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 6 file(s) totalling 253.1 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://kubeflow-pipeline-fantasy_cloudbuild/source/1585287430.8344-1c4a4024cba74738af739d15350603b6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/kubeflow-pipeline-fantasy/builds/7f6df094-f111-408b-8925-196cc00d7971].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/7f6df094-f111-408b-8925-196cc00d7971?project=493831447550].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"7f6df094-f111-408b-8925-196cc00d7971\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://kubeflow-pipeline-fantasy_cloudbuild/source/1585287430.8344-1c4a4024cba74738af739d15350603b6.tgz#1585287432916754\n",
      "Copying gs://kubeflow-pipeline-fantasy_cloudbuild/source/1585287430.8344-1c4a4024cba74738af739d15350603b6.tgz#1585287432916754...\n",
      "/ [1 files][138.3 KiB/138.3 KiB]                                                \n",
      "Operation completed over 1 objects/138.3 KiB.                                    \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  266.2kB\n",
      "Step 1/2 : FROM python:3.7\n",
      "3.7: Pulling from library/python\n",
      "50e431f79093: Already exists\n",
      "dd8c6d374ea5: Already exists\n",
      "c85513200d84: Already exists\n",
      "55769680e827: Already exists\n",
      "f5e195d50b88: Already exists\n",
      "94cdd3612287: Already exists\n",
      "e01b42ee5411: Pulling fs layer\n",
      "044dee574de1: Pulling fs layer\n",
      "f6d53d5a5c86: Pulling fs layer\n",
      "044dee574de1: Verifying Checksum\n",
      "044dee574de1: Download complete\n",
      "f6d53d5a5c86: Verifying Checksum\n",
      "f6d53d5a5c86: Download complete\n",
      "e01b42ee5411: Verifying Checksum\n",
      "e01b42ee5411: Download complete\n",
      "e01b42ee5411: Pull complete\n",
      "044dee574de1: Pull complete\n",
      "f6d53d5a5c86: Pull complete\n",
      "Digest: sha256:d1e0f5f81e107d01da2a5bcb83fb8644cd00886c7a04644dcaa6da7b9f4ffed4\n",
      "Status: Downloaded newer image for python:3.7\n",
      " ---> 8e3336637d81\n",
      "Step 2/2 : RUN pip3 install --upgrade google-cloud-bigquery google-api-core google-cloud-automl grpcio\n",
      " ---> Running in 1a6fa43c6f41\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-1.24.0-py2.py3-none-any.whl (165 kB)\n",
      "Collecting google-api-core\n",
      "  Downloading google_api_core-1.16.0-py2.py3-none-any.whl (70 kB)\n",
      "Collecting google-cloud-automl\n",
      "  Downloading google_cloud_automl-0.10.0-py2.py3-none-any.whl (372 kB)\n",
      "Collecting grpcio\n",
      "  Downloading grpcio-1.27.2-cp37-cp37m-manylinux2010_x86_64.whl (2.7 MB)\n",
      "Collecting google-auth<2.0dev,>=1.9.0\n",
      "  Downloading google_auth-1.12.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting google-cloud-core<2.0dev,>=1.1.0\n",
      "  Downloading google_cloud_core-1.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.11.3-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
      "Collecting six<2.0.0dev,>=1.13.0\n",
      "  Downloading six-1.14.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0\n",
      "  Downloading google_resumable_media-0.5.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in /usr/local/lib/python3.7/site-packages (from google-api-core) (46.0.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis-common-protos-1.51.0.tar.gz (35 kB)\n",
      "Collecting pytz\n",
      "  Downloading pytz-2019.3-py2.py3-none-any.whl (509 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.0.0-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<4.1,>=3.1.4\n",
      "  Downloading rsa-4.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2019.11.28-py2.py3-none-any.whl (156 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.8-py2.py3-none-any.whl (125 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Building wheels for collected packages: googleapis-common-protos\n",
      "  Building wheel for googleapis-common-protos (setup.py): started\n",
      "  Building wheel for googleapis-common-protos (setup.py): finished with status 'done'\n",
      "  Created wheel for googleapis-common-protos: filename=googleapis_common_protos-1.51.0-py3-none-any.whl size=77592 sha256=031ec9c45aca2c26966c63aff46013a09b4d65142a33d00138805a6e8ec91f4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/a1/71/5e427276ceeff277fd76878d1b19fbf4587a2845015d86864b\n",
      "Successfully built googleapis-common-protos\n",
      "Installing collected packages: cachetools, pyasn1, rsa, six, pyasn1-modules, google-auth, certifi, urllib3, idna, chardet, requests, protobuf, googleapis-common-protos, pytz, google-api-core, google-cloud-core, google-resumable-media, google-cloud-bigquery, google-cloud-automl, grpcio\n",
      "Successfully installed cachetools-4.0.0 certifi-2019.11.28 chardet-3.0.4 google-api-core-1.16.0 google-auth-1.12.0 google-cloud-automl-0.10.0 google-cloud-bigquery-1.24.0 google-cloud-core-1.3.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 grpcio-1.27.2 idna-2.9 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytz-2019.3 requests-2.23.0 rsa-4.0 six-1.14.0 urllib3-1.25.8\n",
      "Removing intermediate container 1a6fa43c6f41\n",
      " ---> 8bf10b66c7fb\n",
      "Successfully built 8bf10b66c7fb\n",
      "Successfully tagged gcr.io/kubeflow-pipeline-fantasy/lab_automl_components:latest\n",
      "PUSH\n",
      "Pushing gcr.io/kubeflow-pipeline-fantasy/lab_automl_components:latest\n",
      "The push refers to repository [gcr.io/kubeflow-pipeline-fantasy/lab_automl_components]\n",
      "9f07b98b3fd8: Preparing\n",
      "7b467e610997: Preparing\n",
      "72e018dfb28d: Preparing\n",
      "a6b1c5949826: Preparing\n",
      "3dffd131f01f: Preparing\n",
      "271910c4c150: Preparing\n",
      "6670e930ed33: Preparing\n",
      "c7f27a4eb870: Preparing\n",
      "e70dfb4c3a48: Preparing\n",
      "1c76bd0dc325: Preparing\n",
      "271910c4c150: Waiting\n",
      "6670e930ed33: Waiting\n",
      "c7f27a4eb870: Waiting\n",
      "e70dfb4c3a48: Waiting\n",
      "1c76bd0dc325: Waiting\n",
      "7b467e610997: Layer already exists\n",
      "72e018dfb28d: Layer already exists\n",
      "a6b1c5949826: Layer already exists\n",
      "3dffd131f01f: Layer already exists\n",
      "e70dfb4c3a48: Layer already exists\n",
      "271910c4c150: Layer already exists\n",
      "6670e930ed33: Layer already exists\n",
      "c7f27a4eb870: Layer already exists\n",
      "1c76bd0dc325: Layer already exists\n",
      "9f07b98b3fd8: Pushed\n",
      "latest: digest: sha256:b41f722e773eaac8e60dfa276d08b209ee9f177764d34b6dfaa86e0c941c2a4f size: 2429\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                 IMAGES                                                            STATUS\n",
      "7f6df094-f111-408b-8925-196cc00d7971  2020-03-27T05:37:13+00:00  33S       gs://kubeflow-pipeline-fantasy_cloudbuild/source/1585287430.8344-1c4a4024cba74738af739d15350603b6.tgz  gcr.io/kubeflow-pipeline-fantasy/lab_automl_components (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create BQ query component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_query(query: str, \n",
    "             project_id:str, \n",
    "             dataset_id: str, \n",
    "             table_id: str, \n",
    "             location: str) -> NamedTuple('Outputs', [('table_uri', str), ('job_id', str)]):\n",
    "\n",
    "    from google.cloud import bigquery\n",
    "    from google.api_core import exceptions\n",
    "    import logging\n",
    "    import os\n",
    "    import uuid\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    client = bigquery.Client(project=project_id, location=location)\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    job_config.create_disposition = bigquery.job.CreateDisposition.CREATE_IF_NEEDED\n",
    "    job_config.write_disposition = bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "    job_id = 'query_' + os.environ.get('KFP_POD_NAME', uuid.uuid1().hex)\n",
    "    \n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    try:\n",
    "        dataset = client.get_dataset(dataset_ref)\n",
    "    except exceptions.NotFound:\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = location\n",
    "        logging.info('Creating dataset {}'.format(dataset_id))\n",
    "        client.create_dataset(dataset)\n",
    "     \n",
    "    table_id = table_id if table_id else job_id\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    job_config.destination = table_ref\n",
    "    logging.info('Submitting the job {}'.format(job_id))\n",
    "    query_job = client.query(query, job_config, job_id=job_id)\n",
    "    query_job.result() # Wait for query to finish\n",
    "            \n",
    "    table_uri = 'bq://{}.{}.{}'.format(project_id, dataset_id, table_id)\n",
    "    \n",
    "    return (table_uri, job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_query_op = func_to_container_op(bq_query, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a component that retrieves and logs AutoML evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_log_regression_metrics(\n",
    "    model_path: str,\n",
    "    primary_metric:str) -> NamedTuple('Outputs', [('primary_metric_value', float), \n",
    "                                                  ('mlpipeline_metrics', 'Metrics')]):\n",
    "    \n",
    "    import logging\n",
    "    import json\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    from collections import namedtuple\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient()\n",
    "\n",
    "    # Retrieve evaluation metrics\n",
    "    for evaluation in client.list_model_evaluations(model_name=model_path):\n",
    "        if evaluation.regression_evaluation_metrics.ListFields():\n",
    "            evaluation_metrics = evaluation.regression_evaluation_metrics      \n",
    "    primary_metric_value = getattr(evaluation_metrics, primary_metric)\n",
    "    \n",
    "    # Write the primary metric as a KFP pipeline metric\n",
    "    metrics = {\n",
    "        'metrics': [{\n",
    "            'name': primary_metric.replace('_', '-'),\n",
    "            'numberValue': primary_metric_value\n",
    "        }]\n",
    "    }\n",
    "    divmod_output = namedtuple('AutoMLMetricsOutput', ['primary_metric_value', 'mlpipeline_metrics'])\n",
    "    return divmod_output(primary_metric_value, json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_regression_metrics_op = func_to_container_op(automl_log_regression_metrics, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a component that deploys an AutoML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automl_deploy_model(model_path: str):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import automl_v1beta1 as automl\n",
    "    from google.cloud.automl_v1beta1 import enums\n",
    "    \n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    client = automl.TablesClient()\n",
    "    \n",
    "    model = client.get_model(model_name=model_path)\n",
    "    if model.deployment_state != enums.Model.DeploymentState.DEPLOYED:\n",
    "        logging.info(\"Starting model deployment: {}\".format(model_path))\n",
    "        response = client.deploy_model(model_name=model_path)\n",
    "        response.result() # Wait for operation to complete\n",
    "        logging.info(\"Deployment completed\")\n",
    "    else:\n",
    "         logging.info(\"Model already deployed\")\n",
    "    \n",
    "    \n",
    "deploy_model_op = func_to_container_op(automl_deploy_model, base_image=IMAGE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='CLV Training',\n",
    "    description='CLV Training Pipeline using BigQuery for feature engineering and Automl Tables for model training'\n",
    ")\n",
    "def clv_train(\n",
    "    project_id:str,\n",
    "    \n",
    "    feature_engineering_query:str,\n",
    "    features_dataset_id:str,\n",
    "    features_dataset_location:str,    \n",
    "    features_table_id:str ='features',\n",
    "\n",
    "    aml_compute_region:str ='us-central1',\n",
    "    aml_dataset_name:str ='clv_features',\n",
    "    target_column_name:str ='target_monetary',\n",
    "    aml_model_name:str ='clv_regression',\n",
    "    train_budget:'Integer' =1000,\n",
    "    optimization_objective:str ='MINIMIZE_MAE',\n",
    "    primary_metric:str ='mean_absolute_error',\n",
    "    deployment_threshold:'Float' ='900'\n",
    "    ):\n",
    "    \"\"\"Trains a Customer Lifetime Value model\"\"\"\n",
    "    \n",
    "    # Use BigQuery to engineer features from transaction data\n",
    "    engineer_features = bq_query_op(\n",
    "        query=feature_engineering_query,\n",
    "        project_id=project_id,\n",
    "        dataset_id=features_dataset_id,\n",
    "        table_id=features_table_id,\n",
    "        location=features_dataset_location)\n",
    "    \n",
    "    # Create an AML Dataset\n",
    "    create_dataset = automl_create_dataset_op(\n",
    "        gcp_project_id=project_id,\n",
    "        gcp_region=aml_compute_region,\n",
    "        display_name=aml_dataset_name\n",
    "    )\n",
    "    \n",
    "    # Import the features from BigQuery to AML Dataset\n",
    "    import_data = automl_import_data_from_bq_op(\n",
    "        dataset_path=create_dataset.outputs['dataset_path'],\n",
    "        input_uri=engineer_features.outputs['table_uri']\n",
    "    )\n",
    "    \n",
    "    # Set the target and feature columns\n",
    "    split_column_specs = automl_split_dataset_table_column_names_op(\n",
    "        dataset_path=import_data.outputs['dataset_path'],\n",
    "        table_index=0,\n",
    "        target_column_name=target_column_name\n",
    "    )\n",
    "    \n",
    "    # Create a model\n",
    "    create_model = automl_create_model__op(\n",
    "        gcp_project_id=project_id,\n",
    "        gcp_region=aml_compute_region,\n",
    "        display_name=aml_model_name,\n",
    "        dataset_id=create_dataset.outputs['dataset_id'],\n",
    "        target_column_path=split_column_specs.outputs['target_column_path'],\n",
    "        input_feature_column_paths=split_column_specs.outputs['feature_column_paths'],\n",
    "        optimization_objective=optimization_objective,\n",
    "        train_budget_milli_node_hours=train_budget\n",
    "    )\n",
    "    \n",
    "    # Retrieve the primary metric from the model evaluations\n",
    "    log_regression_metrics = log_regression_metrics_op(create_model.outputs['model_path'], primary_metric)\n",
    "    \n",
    "    # Deploy the model if the primary metric is better than threshold\n",
    "    with kfp.dsl.Condition(log_regression_metrics.outputs['primary_metric_value'] < deployment_threshold):\n",
    "        deploy_model = deploy_model_op(create_model.outputs['model_path'])\n",
    "    \n",
    "    kfp.dsl.get_pipeline_conf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = clv_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_template = '''\n",
    "WITH\n",
    "  order_summaries as (\n",
    "    SELECT\n",
    "      a.customer_id,\n",
    "      a.order_date,\n",
    "      a.order_value,\n",
    "      a.order_qty_articles\n",
    "    FROM\n",
    "    (\n",
    "      SELECT\n",
    "        customer_id,\n",
    "        order_date,\n",
    "        ROUND(SUM(unit_price * quantity), 2) AS order_value,\n",
    "        SUM(quantity) AS order_qty_articles,\n",
    "        (\n",
    "          SELECT\n",
    "            MAX(order_date)\n",
    "          FROM\n",
    "            `{{ data_source_id }}` tl\n",
    "          WHERE\n",
    "            tl.customer_id = t.customer_id\n",
    "        ) latest_order\n",
    "      FROM\n",
    "        `{{ data_source_id }}` t\n",
    "      GROUP BY\n",
    "          customer_id,\n",
    "          order_date\n",
    "    ) a\n",
    "\n",
    "    INNER JOIN (\n",
    "      -- Only customers with more than one positive order values before threshold.\n",
    "      SELECT\n",
    "        customer_id\n",
    "      FROM (\n",
    "        -- Customers and how many positive order values  before threshold.\n",
    "        SELECT\n",
    "          customer_id,\n",
    "          SUM(positive_value) cnt_positive_value\n",
    "        FROM (\n",
    "          -- Customer with whether order was positive or not at each date.\n",
    "          SELECT\n",
    "            customer_id,\n",
    "            (\n",
    "              CASE\n",
    "                WHEN SUM(unit_price * quantity) > 0 THEN 1\n",
    "                ELSE 0\n",
    "              END ) positive_value\n",
    "          FROM\n",
    "            `{{ data_source_id }}`\n",
    "          WHERE\n",
    "            order_date < DATE(\"{{ threshold_date }}\")\n",
    "          GROUP BY\n",
    "            customer_id,\n",
    "            order_date)\n",
    "        GROUP BY\n",
    "          customer_id )\n",
    "      WHERE\n",
    "        cnt_positive_value > 1\n",
    "      ) b\n",
    "    ON\n",
    "      a.customer_id = b.customer_id\n",
    "    --[START common_clean]\n",
    "    WHERE\n",
    "      -- Bought in the past 3 months\n",
    "      DATE_DIFF(DATE(\"{{ predict_end }}\"), latest_order, DAY) <= 90\n",
    "      -- Make sure returns are consistent.\n",
    "      AND (\n",
    "        (order_qty_articles > 0 and order_Value > 0) OR\n",
    "        (order_qty_articles < 0 and order_Value < 0)\n",
    "      ))\n",
    "          \n",
    "SELECT\n",
    "--  tf.customer_id,\n",
    "  ROUND(tf.monetary, 2) as monetary,\n",
    "  tf.cnt_orders AS frequency,\n",
    "  tf.recency,\n",
    "  tf.T,\n",
    "  ROUND(tf.recency/cnt_orders, 2) AS time_between,\n",
    "  ROUND(tf.avg_basket_value, 2) AS avg_basket_value,\n",
    "  ROUND(tf.avg_basket_size, 2) AS avg_basket_size,\n",
    "  tf.cnt_returns,\n",
    "  -- Target calculated for overall period\n",
    "  ROUND(tt.target_monetary, 2) as target_monetary\n",
    "FROM\n",
    "  -- This SELECT uses only data before threshold to make features.\n",
    "  (\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      SUM(order_value) AS monetary,\n",
    "      DATE_DIFF(MAX(order_date), MIN(order_date), DAY) AS recency,\n",
    "      DATE_DIFF(DATE('{{ threshold_date }}'), MIN(order_date), DAY) AS T,\n",
    "      COUNT(DISTINCT order_date) AS cnt_orders,\n",
    "      AVG(order_qty_articles) avg_basket_size,\n",
    "      AVG(order_value) avg_basket_value,\n",
    "      SUM(CASE\n",
    "          WHEN order_value < 1 THEN 1\n",
    "          ELSE 0 END) AS cnt_returns\n",
    "    FROM\n",
    "      order_summaries a\n",
    "    WHERE\n",
    "      order_date <= DATE('{{ threshold_date }}')\n",
    "    GROUP BY\n",
    "      customer_id) tf,\n",
    "\n",
    "  -- This SELECT uses data after threshold to calculate the target )\n",
    "  (\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      SUM(order_value) target_monetary\n",
    "    FROM\n",
    "      order_summaries\n",
    "      WHERE order_date > DATE('{{ threshold_date }}')\n",
    "    GROUP BY\n",
    "      customer_id) tt\n",
    "WHERE\n",
    "  tf.customer_id = tt.customer_id\n",
    "  AND tf.monetary > 0\n",
    "  AND tf.monetary <= {{ max_monetary }}\n",
    "'''\n",
    "    \n",
    "query = Template(query_template).render(\n",
    "    data_source_id='{}.{}.{}'.format(PROJECT_ID, DATASET_ID, TRANSACTIONS_TABLE_ID),\n",
    "    threshold_date='2011-08-08',\n",
    "    predict_end='2011-12-12',\n",
    "    max_monetary=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/experiments/details/e98b79c4-f6b8-4f29-8cce-ca7647fb996d\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"https://kubeflow-st-ui.endpoints.kubeflow-pipeline-fantasy.cloud.goog/pipeline/#/runs/details/9496f5b8-8df9-436f-9d17-3bf7fd3e0bb6\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'clv_kubeflow'\n",
    "\n",
    "arguments = {\n",
    "    'project_id': PROJECT_ID,\n",
    "    'features_dataset_id': DATASET_ID,\n",
    "    'features_dataset_location':  DATASET_LOCATION,\n",
    "    'feature_engineering_query': query}\n",
    "\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}