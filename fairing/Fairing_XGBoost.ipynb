{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and deploy Xgboost (Scikit-learn) on Kubeflow from Notebooks\n",
    "\n",
    "This notebook introduces you the usage of Kubeflow Fairing to train and deploy a model to Kubeflow on Google Kubernetes Engine (GKE), and Google Cloud AI Platform training. This notebook demonstrate how to:\n",
    " \n",
    "* Train an XGBoost model in a local notebook,\n",
    "* Use Kubeflow Fairing to train an XGBoost model remotely on Kubeflow cluster,\n",
    "* Use Kubeflow Fairing to train an XGBoost model remotely on AI Platform training,\n",
    "* Use Kubeflow Fairing to deploy a trained model to Kubeflow, and Call the deployed endpoint for predictions.\n",
    "\n",
    "**You need Python 3.6 to use Kubeflow Fairing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setups\n",
    "\n",
    "* Pre-conditions\n",
    "    - Deployed a kubeflow cluster through https://deploy.kubeflow.cloud/\n",
    "    - Have the following environment variable ready: \n",
    "        - PROJECT_ID # project host the kubeflow cluster or for running AI platform training\n",
    "        - DEPLOYMENT_NAME # kubeflow deployment name, the same the cluster name after delpoyed\n",
    "        - GCP_BUCKET # google cloud storage bucket\n",
    "\n",
    "* Create service account\n",
    "```bash\n",
    "export SA_NAME = [service account name]\n",
    "gcloud iam service-accounts create ${SA_NAME}\n",
    "gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\n",
    "    --member serviceAccount:${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com \\\n",
    "    --role 'roles/editor'\n",
    "gcloud iam service-accounts keys create ~/key.json \\\n",
    "    --iam-account ${SA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\n",
    "```\n",
    "\n",
    "* Authorize for Source Repository\n",
    "```bash\n",
    "gcloud auth configure-docker\n",
    "```\n",
    "\n",
    "* Update local kubeconfig (for submiting job to kubeflow cluster)\n",
    "```bash\n",
    "export CLUSTER_NAME=${DEPLOYMENT_NAME} # this is the deployment name or the kubenete cluster name\n",
    "export ZONE=us-central1-c\n",
    "gcloud container clusters get-credentials ${CLUSTER_NAME} --region ${ZONE}\n",
    "```\n",
    "\n",
    "* Set the environmental variable: GOOGLE_APPLICATION_CREDENTIALS\n",
    "```bash\n",
    "export GOOGLE_APPLICATION_CREDENTIALS = ....\n",
    "```\n",
    "```python\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS']=...\n",
    "```\n",
    "\n",
    "* Install the lastest version of fairing\n",
    "```python\n",
    "pip install git+https://github.com/kubeflow/fairing@master\n",
    "```\n",
    "\n",
    "* Upload training file\n",
    "```bash\n",
    "# upload the train.csv to GCS bucket that can be accessed from both CMLE and Kubeflow cluster\n",
    "gsutil cp ./train.csv ${GCP_Bucket}/train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please not that the above configuration is required for notebook service running outside Kubeflow environment. And the examples demonstrated in the notebook is fully tested on notebook service outside Kubeflow cluster also.**\n",
    "\n",
    "**The environemt variables, e.g. service account, projects and etc, should have been pre-configured while setting up the cluster.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your notebook for training an XGBoost model\n",
    "\n",
    "Import the libraries required to train this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install joblib\n",
    "# ! pip3 install pandas scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import joblib\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(message)s')\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trykube-248403\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fairing\n",
    "\n",
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "# You can use any docker container registry istead of GCR\n",
    "# For local notebook, GCP_PROJECT should be set explicitly\n",
    "GCP_PROJECT = fairing.cloud.gcp.guess_project_name()\n",
    "GCP_Bucket ='gs://kubeflow-trykube/'\n",
    "print(GCP_PROJECT)\n",
    "# This is for local notebook instead of that in kubeflow cluster\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS']="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to split the input file into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcs_copy(src_path, dst_path):\n",
    "    import subprocess\n",
    "    print(subprocess.run(['gsutil', 'cp', src_path, dst_path], stdout=subprocess.PIPE).stdout[:-1].decode('utf-8'))\n",
    "    \n",
    "def gcs_download(src_path, file_name):\n",
    "    import subprocess\n",
    "    print(subprocess.run(['gsutil', 'cp', src_path, file_name], stdout=subprocess.PIPE).stdout[:-1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gcs_copy('train_fraud.csv', GCP_Bucket + \"train_fraud.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(source_path, test_size=0.25):\n",
    "    \"\"\"Read input data and split it into train and test.\"\"\"\n",
    "    \n",
    "    file_name = source_path.split('/')[-1]\n",
    "    gcs_download(source_path, file_name)\n",
    "    data = pd.read_csv(file_name)\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "\n",
    "    y = data.Class\n",
    "    X = data.drop(['Class', 'Amount', 'Time'], axis=1).select_dtypes(exclude=['object'])\n",
    "\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X.values,\n",
    "                                                      y.values,\n",
    "                                                      test_size=test_size,\n",
    "                                                      shuffle=True)\n",
    "\n",
    "    imputer = SimpleImputer()\n",
    "    train_X = imputer.fit_transform(train_X)\n",
    "    test_X = imputer.transform(test_X)\n",
    "\n",
    "    return (train_X, train_y), (test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to train, evaluate, and save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_X,\n",
    "                train_y,\n",
    "                test_X,\n",
    "                test_y,\n",
    "                n_estimators,\n",
    "                learning_rate):\n",
    "    \"\"\"Train the model using XGBRegressor.\"\"\"\n",
    "    model = XGBClassifier(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "\n",
    "    model.fit(train_X,\n",
    "            train_y,\n",
    "            early_stopping_rounds=40,\n",
    "            eval_set=[(test_X, test_y)])\n",
    "\n",
    "    print(\"Best loss on eval: %.2f with %d rounds\",\n",
    "               model.best_score,\n",
    "               model.best_iteration+1)\n",
    "    return model\n",
    "\n",
    "def eval_model(model, test_X, test_y):\n",
    "    \"\"\"Evaluate the model performance.\"\"\"\n",
    "    predictions = model.predict_proba(test_X)\n",
    "    logging.info(\"auc=%.2f\", roc_auc_score(test_y, predictions[:,1]))\n",
    "\n",
    "def save_model(model, model_file):\n",
    "    \"\"\"Save XGBoost model for serving.\"\"\"\n",
    "    joblib.dump(model, model_file)\n",
    "    gcs_copy(model_file, GCP_Bucket + model_file)\n",
    "    logging.info(\"Model export success: %s\", model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a class for your model, with methods for training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudServe(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_input = GCP_Bucket + \"train_fraud.csv\"\n",
    "        self.n_estimators = 50\n",
    "        self.learning_rate = 0.1\n",
    "        self.model_file = \"trained_fraud_model.joblib\"\n",
    "        self.model = None\n",
    "\n",
    "    def train(self):\n",
    "        (train_X, train_y), (test_X, test_y) = read_input(self.train_input)\n",
    "        model = train_model(train_X,\n",
    "                          train_y,\n",
    "                          test_X,\n",
    "                          test_y,\n",
    "                          self.n_estimators,\n",
    "                          self.learning_rate)\n",
    "\n",
    "        eval_model(model, test_X, test_y)\n",
    "        save_model(model, self.model_file)\n",
    "\n",
    "    def predict(self, X, feature_names):\n",
    "        \"\"\"Predict using the model for given ndarray.\"\"\"\n",
    "        if not self.model:\n",
    "            self.model = joblib.load(self.model_file)\n",
    "        # Do any preprocessing\n",
    "        prediction = self.model.predict(data=X)\n",
    "        # Do any postprocessing\n",
    "        return [[prediction.item(0), prediction.item(0)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an XGBoost model in a notebook\n",
    "\n",
    "Call `FraudServe().train()` to train your model, and then evaluate and save your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0]\tvalidation_0-error:0.034853\n",
      "Will train until validation_0-error hasn't improved in 40 rounds.\n",
      "[1]\tvalidation_0-error:0.029491\n",
      "[2]\tvalidation_0-error:0.032172\n",
      "[3]\tvalidation_0-error:0.032172\n",
      "[4]\tvalidation_0-error:0.032172\n",
      "[5]\tvalidation_0-error:0.029491\n",
      "[6]\tvalidation_0-error:0.029491\n",
      "[7]\tvalidation_0-error:0.032172\n",
      "[8]\tvalidation_0-error:0.029491\n",
      "[9]\tvalidation_0-error:0.029491\n",
      "[10]\tvalidation_0-error:0.029491\n",
      "[11]\tvalidation_0-error:0.029491\n",
      "[12]\tvalidation_0-error:0.02681\n",
      "[13]\tvalidation_0-error:0.024129\n",
      "[14]\tvalidation_0-error:0.021448\n",
      "[15]\tvalidation_0-error:0.024129\n",
      "[16]\tvalidation_0-error:0.021448\n",
      "[17]\tvalidation_0-error:0.021448\n",
      "[18]\tvalidation_0-error:0.021448\n",
      "[19]\tvalidation_0-error:0.021448\n",
      "[20]\tvalidation_0-error:0.021448\n",
      "[21]\tvalidation_0-error:0.021448\n",
      "[22]\tvalidation_0-error:0.021448\n",
      "[23]\tvalidation_0-error:0.021448\n",
      "[24]\tvalidation_0-error:0.021448\n",
      "[25]\tvalidation_0-error:0.021448\n",
      "[26]\tvalidation_0-error:0.021448\n",
      "[27]\tvalidation_0-error:0.021448\n",
      "[28]\tvalidation_0-error:0.021448\n",
      "[29]\tvalidation_0-error:0.021448\n",
      "[30]\tvalidation_0-error:0.018767\n",
      "[31]\tvalidation_0-error:0.018767\n",
      "[32]\tvalidation_0-error:0.018767\n",
      "[33]\tvalidation_0-error:0.021448\n",
      "[34]\tvalidation_0-error:0.018767\n",
      "[35]\tvalidation_0-error:0.024129\n",
      "[36]\tvalidation_0-error:0.021448\n",
      "[37]\tvalidation_0-error:0.021448\n",
      "[38]\tvalidation_0-error:0.021448\n",
      "[39]\tvalidation_0-error:0.021448\n",
      "[40]\tvalidation_0-error:0.024129\n",
      "[41]\tvalidation_0-error:0.021448\n",
      "[42]\tvalidation_0-error:0.021448\n",
      "[43]\tvalidation_0-error:0.021448\n",
      "[44]\tvalidation_0-error:0.021448\n",
      "[45]\tvalidation_0-error:0.021448\n",
      "[46]\tvalidation_0-error:0.021448\n",
      "[47]\tvalidation_0-error:0.024129\n",
      "[48]\tvalidation_0-error:0.024129\n",
      "[49]\tvalidation_0-error:0.021448\n",
      "Best loss on eval: %.2f with %d rounds 0.018767 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "auc=0.99\n",
      "Model export success: trained_fraud_model.joblib\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "FraudServe().train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Use of Fairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spicify a image registry that will hold the image built by fairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this demo, I use gsutil, therefore i compile a special image to install GoogleCloudSDK as based image\n",
    "base_image = 'gcr.io/{}/fairing-predict-example:latest'.format(GCP_PROJECT)\n",
    "# !docker build --build-arg PY_VERSION=3.6.4 . -t {base_image}\n",
    "# !docker push {base_image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCKER_REGISTRY = 'gcr.io/{}/fairing-job-xgboost'.format(GCP_PROJECT)\n",
    "BASE_IMAGE = base_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an XGBoost model remotely on Kubeflow\n",
    "\n",
    "Import the `TrainJob` and `GKEBackend` classes. Kubeflow Fairing packages the `FraudServe` class, the training data, and the training job's software prerequisites as a Docker image. Then Kubeflow Fairing deploys and runs the training job on Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using preprocessor: <class 'fairing.preprocessors.function.FunctionPreProcessor'>\n",
      "Using docker registry: gcr.io/trykube-248403/fairing-job-xgboost\n",
      "Using builder: <class 'fairing.builders.cluster.cluster.ClusterBuilder'>\n",
      "/opt/conda/lib/python3.6/site-packages/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "/opt/conda/lib/python3.6/site-packages/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Waiting for fairing-builder-27sbv to start...\n",
      "Waiting for fairing-builder-27sbv to start...\n",
      "Waiting for fairing-builder-27sbv to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image gcr.io/trykube-248403/fairing-predict-example:latest\n",
      "\u001b[36mINFO\u001b[0m[0001] Executing 0 build triggers\n",
      "\u001b[36mINFO\u001b[0m[0001] Unpacking rootfs as cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi requires it.\n",
      "\u001b[36mINFO\u001b[0m[0049] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0064] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0064] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0064] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0064] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0064] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0064] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0084] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0084] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0084] Changed working directory to /app\n",
      "\u001b[36mINFO\u001b[0m[0084] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0105] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0105] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0105] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0105] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0105] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0105] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0118] No files were changed, appending empty layer to config. No layer added to image.\n",
      "\u001b[36mINFO\u001b[0m[0118] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0118] Using files from context: [/kaniko/buildcontext/app/requirements.txt]\n",
      "\u001b[36mINFO\u001b[0m[0118] COPY /app//requirements.txt /app/\n",
      "\u001b[36mINFO\u001b[0m[0118] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0118] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0118] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0118] args: [-c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.25.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.90)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.0)\n",
      "Requirement already satisfied: seldon-core in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2019.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from xgboost->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 5)) (0.21.3)\n",
      "Requirement already satisfied: jaeger-client==3.13.0 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.13.0)\n",
      "Requirement already satisfied: opentracing<2,>=1.2.2 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: flask-cors in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.0.8)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (2.22.0)\n",
      "Requirement already satisfied: Flask-OpenTracing==0.2.0 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: redis in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.3.4)\n",
      "Requirement already satisfied: grpcio-opentracing in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.1.4)\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.22.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.9.0)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.11)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (5.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.32.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.8.0)\n",
      "Requirement already satisfied: threadloop<2,>=1 in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (1.0.2)\n",
      "Requirement already satisfied: tornado<5,>=4.3 in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (4.5.3)\n",
      "Requirement already satisfied: thrift in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (7.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (0.15.5)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (2.10.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (2019.6.16)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from protobuf->seldon-core->-r requirements.txt (line 6)) (40.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->-r requirements.txt (line 7)) (3.1.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow->-r requirements.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/site-packages (from Jinja2>=2.10.1->flask->seldon-core->-r requirements.txt (line 6)) (1.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0120] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0120] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0120] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0120] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0120] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0120] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0121] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0133] Using files from context: [/kaniko/buildcontext/app]\n",
      "\u001b[36mINFO\u001b[0m[0133] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0133] Taking snapshot of files...\n",
      "2019/07/31 07:17:19 existing blob: sha256:087a57faf9491b1b82a83e26bc8cc90c90c30e4a4d858b57ddd5b4c2c90095f6\n",
      "2019/07/31 07:17:19 existing blob: sha256:2eeb5ce9b9240a928b0a799f9f2601027e2c6b7525394ae5c371f124058489d7\n",
      "2019/07/31 07:17:19 existing blob: sha256:620aea26e85367b08cdf1f6768491fb44df6a2a71f7d663f835b1692e849c3ee\n",
      "2019/07/31 07:17:19 existing blob: sha256:54f7e8ac135a5f502a6ee9537ef3d64b1cd2fa570dc0a40b4d3b6f7ac81e7486\n",
      "2019/07/31 07:17:19 existing blob: sha256:a8c5303780550b746a4781e5e4cd893121d8019e971414a2a1273d54486b4eb9\n",
      "2019/07/31 07:17:19 existing blob: sha256:d6341e30912f12f56e18564a3b582853f65376766f5f9d641a68a724ed6db88f\n",
      "2019/07/31 07:17:19 existing blob: sha256:041cd0421648e4d2475068b2a57abe52210afeddd6d9d30f18093d1db9b1a895\n",
      "2019/07/31 07:17:19 existing blob: sha256:c60eba308238780085602c72a69337c634aba5207d54d2369ddd92e4120f808f\n",
      "2019/07/31 07:17:19 existing blob: sha256:3ae1eaa188d6c2f435213bce9a22318adb11d071ec01198affb8a3886f6f2495\n",
      "2019/07/31 07:17:19 existing blob: sha256:b251d940695c89cba7645ba18409d57bc50de2b1ae3500e7c7645b3ef8b81c3a\n",
      "2019/07/31 07:17:19 existing blob: sha256:0c1db95989906f161007d8ef2a6ef6e0ec64bc15bf2c993fd002edbdfc7aa7df\n",
      "2019/07/31 07:17:19 existing blob: sha256:5d71636fb824265e30ff34bf20737c9cdc4f5af28b6bce86f08215c55b89bfab\n",
      "2019/07/31 07:17:19 existing blob: sha256:687ed2fb2a0d7da5503478759fd00c23970b65d02b317119b3fb9025038a2594\n",
      "2019/07/31 07:17:23 pushed blob sha256:1f3e25254fa2623b6869aae0101af105516f50d30e5e7902bc64943356b13f79\n",
      "2019/07/31 07:17:23 pushed blob sha256:e72ce734a3c3b4cdd568bf0ca9c05667351a27938fa84165db20f1b896be26fe\n",
      "2019/07/31 07:17:23 pushed blob sha256:ca5530bda699010409db92fc22878456d448ee750ecefdd0bec61ab82a2edd13\n",
      "2019/07/31 07:17:23 pushed blob sha256:cea83c2b8a87ca8a8e076110df40a41e6de25f9dbd479b531c88a7e92059a598\n",
      "2019/07/31 07:17:26 gcr.io/trykube-248403/fairing-job-xgboost/fairing-job:FA205E89: digest: sha256:20c545721373d145edaa45a7507f43acb42abe89e499b76ae52463676963cc88 size: 2884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training job fairing-job-8h6hj launched.\n",
      "Waiting for fairing-job-8h6hj-xsk28 to start...\n",
      "Waiting for fairing-job-8h6hj-xsk28 to start...\n",
      "Waiting for fairing-job-8h6hj-xsk28 to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://kubeflow-trykube/train_fraud.csv...\n",
      "/ [1 files][793.4 KiB/793.4 KiB]                                                \n",
      "Operation completed over 1 objects/793.4 KiB.\n",
      "\n",
      "[0]\tvalidation_0-error:0.050938\n",
      "Will train until validation_0-error hasn't improved in 40 rounds.\n",
      "[1]\tvalidation_0-error:0.032172\n",
      "[2]\tvalidation_0-error:0.032172\n",
      "[3]\tvalidation_0-error:0.032172\n",
      "[4]\tvalidation_0-error:0.032172\n",
      "[5]\tvalidation_0-error:0.032172\n",
      "[6]\tvalidation_0-error:0.032172\n",
      "[7]\tvalidation_0-error:0.032172\n",
      "[8]\tvalidation_0-error:0.032172\n",
      "[9]\tvalidation_0-error:0.032172\n",
      "[10]\tvalidation_0-error:0.032172\n",
      "[11]\tvalidation_0-error:0.032172\n",
      "[12]\tvalidation_0-error:0.032172\n",
      "[13]\tvalidation_0-error:0.032172\n",
      "[14]\tvalidation_0-error:0.032172\n",
      "[15]\tvalidation_0-error:0.032172\n",
      "[16]\tvalidation_0-error:0.032172\n",
      "[17]\tvalidation_0-error:0.032172\n",
      "[18]\tvalidation_0-error:0.032172\n",
      "[19]\tvalidation_0-error:0.032172\n",
      "[20]\tvalidation_0-error:0.032172\n",
      "[21]\tvalidation_0-error:0.032172\n",
      "[22]\tvalidation_0-error:0.032172\n",
      "[23]\tvalidation_0-error:0.032172\n",
      "[24]\tvalidation_0-error:0.032172\n",
      "[25]\tvalidation_0-error:0.032172\n",
      "[26]\tvalidation_0-error:0.032172\n",
      "[27]\tvalidation_0-error:0.032172\n",
      "[28]\tvalidation_0-error:0.032172\n",
      "[29]\tvalidation_0-error:0.032172\n",
      "[30]\tvalidation_0-error:0.029491\n",
      "[31]\tvalidation_0-error:0.029491\n",
      "[32]\tvalidation_0-error:0.029491\n",
      "[33]\tvalidation_0-error:0.029491\n",
      "[34]\tvalidation_0-error:0.029491\n",
      "[35]\tvalidation_0-error:0.029491\n",
      "[36]\tvalidation_0-error:0.029491\n",
      "[37]\tvalidation_0-error:0.029491\n",
      "[38]\tvalidation_0-error:0.029491\n",
      "[39]\tvalidation_0-error:0.029491\n",
      "[40]\tvalidation_0-error:0.029491\n",
      "[41]\tvalidation_0-error:0.029491\n",
      "[42]\tvalidation_0-error:0.029491\n",
      "[43]\tvalidation_0-error:0.029491\n",
      "[44]\tvalidation_0-error:0.029491\n",
      "[45]\tvalidation_0-error:0.029491\n",
      "[46]\tvalidation_0-error:0.029491\n",
      "[47]\tvalidation_0-error:0.029491\n",
      "[48]\tvalidation_0-error:0.032172\n",
      "[49]\tvalidation_0-error:0.032172\n",
      "auc=0.99\n",
      "Copying file://trained_fraud_model.joblib [Content-Type=application/octet-stream]...\n",
      "AccessDeniedException: 403 Insufficient Permission                              \n",
      "Model export success: trained_fraud_model.joblib\n",
      "Best loss on eval: %.2f with %d rounds 0.029491 31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning up job fairing-job-8h6hj...\n"
     ]
    }
   ],
   "source": [
    "from fairing import TrainJob\n",
    "from fairing.backends import KubeflowGKEBackend\n",
    "\n",
    "train_job = TrainJob(FraudServe, BASE_IMAGE, input_files=[\"requirements.txt\"],\n",
    "                     docker_registry=DOCKER_REGISTRY, backend=KubeflowGKEBackend())\n",
    "train_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an XGBoost model remotely on Cloud ML Engine\n",
    "\n",
    "Import the `TrainJob` and `GCPManagedBackend` classes. Kubeflow Fairing packages the `FraudServe` class, the training data, and the training job's software prerequisites as a Docker image. Then Kubeflow Fairing deploys and runs the training job on Cloud ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using preprocessor: <class 'fairing.preprocessors.function.FunctionPreProcessor'>\n",
      "Using docker registry: gcr.io/trykube-248403/fairing-job-xgboost\n",
      "Using builder: <class 'fairing.builders.cluster.cluster.ClusterBuilder'>\n",
      "/opt/conda/lib/python3.6/site-packages/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "/opt/conda/lib/python3.6/site-packages/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Waiting for fairing-builder-6d4kb to start...\n",
      "Waiting for fairing-builder-6d4kb to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image gcr.io/trykube-248403/fairing-predict-example:latest\n",
      "\u001b[36mINFO\u001b[0m[0002] Executing 0 build triggers\n",
      "\u001b[36mINFO\u001b[0m[0002] Unpacking rootfs as cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi requires it.\n",
      "\u001b[36mINFO\u001b[0m[0051] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0067] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0067] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0067] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0067] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0067] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0068] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0087] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0087] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0087] Changed working directory to /app\n",
      "\u001b[36mINFO\u001b[0m[0087] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0108] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0108] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0108] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0108] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0108] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0108] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0120] No files were changed, appending empty layer to config. No layer added to image.\n",
      "\u001b[36mINFO\u001b[0m[0120] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0120] Using files from context: [/kaniko/buildcontext/app/requirements.txt]\n",
      "\u001b[36mINFO\u001b[0m[0120] COPY /app//requirements.txt /app/\n",
      "\u001b[36mINFO\u001b[0m[0120] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0120] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0120] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0120] args: [-c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.25.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.90)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.0)\n",
      "Requirement already satisfied: seldon-core in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from xgboost->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 5)) (0.21.3)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.9.0)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.11)\n",
      "Requirement already satisfied: Flask-OpenTracing==0.2.0 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.22.0)\n",
      "Requirement already satisfied: opentracing<2,>=1.2.2 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (5.1.1)\n",
      "Requirement already satisfied: flask-cors in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.0.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (2.22.0)\n",
      "Requirement already satisfied: grpcio-opentracing in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.1.4)\n",
      "Requirement already satisfied: jaeger-client==3.13.0 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.13.0)\n",
      "Requirement already satisfied: redis in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.3.4)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.11.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.8.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.0.8)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.32.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from protobuf->seldon-core->-r requirements.txt (line 6)) (40.6.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (3.0.4)\n",
      "Requirement already satisfied: thrift in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: threadloop<2,>=1 in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (1.0.2)\n",
      "Requirement already satisfied: tornado<5,>=4.3 in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (4.5.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (2.10.1)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (0.15.5)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (7.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow->-r requirements.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->-r requirements.txt (line 7)) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/site-packages (from Jinja2>=2.10.1->flask->seldon-core->-r requirements.txt (line 6)) (1.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0122] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0122] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0122] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0122] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0122] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0122] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0123] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0135] Using files from context: [/kaniko/buildcontext/app]\n",
      "\u001b[36mINFO\u001b[0m[0135] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0135] Taking snapshot of files...\n",
      "2019/07/31 07:45:01 existing blob: sha256:620aea26e85367b08cdf1f6768491fb44df6a2a71f7d663f835b1692e849c3ee\n",
      "2019/07/31 07:45:01 existing blob: sha256:5d71636fb824265e30ff34bf20737c9cdc4f5af28b6bce86f08215c55b89bfab\n",
      "2019/07/31 07:45:01 existing blob: sha256:687ed2fb2a0d7da5503478759fd00c23970b65d02b317119b3fb9025038a2594\n",
      "2019/07/31 07:45:01 existing blob: sha256:3ae1eaa188d6c2f435213bce9a22318adb11d071ec01198affb8a3886f6f2495\n",
      "2019/07/31 07:45:01 existing blob: sha256:d6341e30912f12f56e18564a3b582853f65376766f5f9d641a68a724ed6db88f\n",
      "2019/07/31 07:45:01 existing blob: sha256:c60eba308238780085602c72a69337c634aba5207d54d2369ddd92e4120f808f\n",
      "2019/07/31 07:45:01 existing blob: sha256:a8c5303780550b746a4781e5e4cd893121d8019e971414a2a1273d54486b4eb9\n",
      "2019/07/31 07:45:01 existing blob: sha256:2eeb5ce9b9240a928b0a799f9f2601027e2c6b7525394ae5c371f124058489d7\n",
      "2019/07/31 07:45:01 existing blob: sha256:087a57faf9491b1b82a83e26bc8cc90c90c30e4a4d858b57ddd5b4c2c90095f6\n",
      "2019/07/31 07:45:01 existing blob: sha256:b251d940695c89cba7645ba18409d57bc50de2b1ae3500e7c7645b3ef8b81c3a\n",
      "2019/07/31 07:45:01 existing blob: sha256:041cd0421648e4d2475068b2a57abe52210afeddd6d9d30f18093d1db9b1a895\n",
      "2019/07/31 07:45:01 existing blob: sha256:54f7e8ac135a5f502a6ee9537ef3d64b1cd2fa570dc0a40b4d3b6f7ac81e7486\n",
      "2019/07/31 07:45:01 existing blob: sha256:0c1db95989906f161007d8ef2a6ef6e0ec64bc15bf2c993fd002edbdfc7aa7df\n",
      "2019/07/31 07:45:05 pushed blob sha256:f6ac382357bb8742dcd73c65e3770a7d24fc5791afb02032d3b075c0baa7894d\n",
      "2019/07/31 07:45:05 pushed blob sha256:b6820e7adcef642d7e797db7a9494b862403878cb8ab64a1a5cfd36cabf31ab6\n",
      "2019/07/31 07:45:05 pushed blob sha256:9aff4fa70c8b88898133cccbfe6767a1ab36fdfd9717646463e880c20855710e\n",
      "2019/07/31 07:45:05 pushed blob sha256:9390fb00834237d1e2452af887a1acc162ebacabfd836555325477274bb2b4fb\n",
      "2019/07/31 07:45:09 gcr.io/trykube-248403/fairing-job-xgboost/fairing-job:FA205E89: digest: sha256:0904fc74aa0709ab2041f405f32d6baf9e172df16ee4c4ad2c331f9c76c54b42 size: 2884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "URL being requested: GET https://www.googleapis.com/discovery/v1/apis/ml/v1/rest\n",
      "URL being requested: POST https://ml.googleapis.com/v1/projects/trykube-248403/jobs?alt=json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training job with the following options: {'jobId': 'fairing_job_d8c7ed8f', 'trainingInput': {'scaleTier': 'BASIC', 'masterConfig': {'imageUri': 'gcr.io/trykube-248403/fairing-job-xgboost/fairing-job:FA205E89'}, 'region': 'us-central1'}}\n",
      "Job submitted successfully.\n",
      "Access job logs at the following URL:\n",
      "https://console.cloud.google.com/mlengine/jobs/fairing_job_d8c7ed8f?project=trykube-248403\n"
     ]
    }
   ],
   "source": [
    "from fairing import TrainJob\n",
    "from fairing.backends import GCPManagedBackend\n",
    "train_job = TrainJob(FraudServe, BASE_IMAGE, input_files=[\"requirements.txt\"],\n",
    "                     docker_registry=DOCKER_REGISTRY, backend=GCPManagedBackend())\n",
    "train_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the trained model to Kubeflow for predictions\n",
    "\n",
    "Import the `PredictionEndpoint` and `KubeflowGKEBackend` classes. Kubeflow Fairing packages the `FraudServe` class, the trained model, and the prediction endpoint's software prerequisites as a Docker image. Then Kubeflow Fairing deploys and runs the prediction endpoint on Kubeflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part only works for fairing version >=0.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using preprocessor: <class 'fairing.preprocessors.function.FunctionPreProcessor'>\n",
      "Using docker registry: gcr.io/trykube-248403/fairing-job-xgboost\n",
      "Using builder: <class 'fairing.builders.cluster.cluster.ClusterBuilder'>\n",
      "/opt/conda/lib/python3.6/site-packages/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "/opt/conda/lib/python3.6/site-packages/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "Waiting for fairing-builder-qtxch to start...\n",
      "Waiting for fairing-builder-qtxch to start...\n",
      "Waiting for fairing-builder-qtxch to start...\n",
      "Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0000] Downloading base image gcr.io/trykube-248403/fairing-predict-example:latest\n",
      "\u001b[36mINFO\u001b[0m[0002] Executing 0 build triggers\n",
      "\u001b[36mINFO\u001b[0m[0002] Unpacking rootfs as cmd RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi requires it.\n",
      "\u001b[36mINFO\u001b[0m[0067] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0079] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0079] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0079] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0079] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0079] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0079] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0098] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0098] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0098] Changed working directory to /app\n",
      "\u001b[36mINFO\u001b[0m[0098] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0119] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0119] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0119] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0119] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0119] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0119] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0132] No files were changed, appending empty layer to config. No layer added to image.\n",
      "\u001b[36mINFO\u001b[0m[0132] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0132] Using files from context: [/kaniko/buildcontext/app/requirements.txt]\n",
      "\u001b[36mINFO\u001b[0m[0132] COPY /app//requirements.txt /app/\n",
      "\u001b[36mINFO\u001b[0m[0132] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0132] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0132] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0132] args: [-c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (0.25.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.13.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.90)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (0.0)\n",
      "Requirement already satisfied: seldon-core in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 6)) (0.3.1)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/site-packages (from -r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas->-r requirements.txt (line 1)) (2019.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from xgboost->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/site-packages (from sklearn->-r requirements.txt (line 5)) (0.21.3)\n",
      "Requirement already satisfied: redis in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.3.4)\n",
      "Requirement already satisfied: opentracing<2,>=1.2.2 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: flask-cors in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.0.8)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.11)\n",
      "Requirement already satisfied: grpcio in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.22.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.9.0)\n",
      "Requirement already satisfied: jaeger-client==3.13.0 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (3.13.0)\n",
      "Requirement already satisfied: grpcio-opentracing in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.1.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (5.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (2.22.0)\n",
      "Requirement already satisfied: Flask-OpenTracing==0.2.0 in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: flask in /usr/local/lib/python3.6/site-packages (from seldon-core->-r requirements.txt (line 6)) (1.1.1)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.1.7)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.32.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.12.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/site-packages (from tensorflow->-r requirements.txt (line 7)) (1.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from protobuf->seldon-core->-r requirements.txt (line 6)) (40.6.2)\n",
      "Requirement already satisfied: tornado<5,>=4.3 in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (4.5.3)\n",
      "Requirement already satisfied: thrift in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: threadloop<2,>=1 in /usr/local/lib/python3.6/site-packages (from jaeger-client==3.13.0->seldon-core->-r requirements.txt (line 6)) (1.0.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->seldon-core->-r requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (2.10.1)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/site-packages (from flask->seldon-core->-r requirements.txt (line 6)) (0.15.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow->-r requirements.txt (line 7)) (3.1.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow->-r requirements.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/site-packages (from Jinja2>=2.10.1->flask->seldon-core->-r requirements.txt (line 6)) (1.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0134] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0134] Skipping paths under /dev, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0134] Skipping paths under /etc/secrets, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0134] Skipping paths under /kaniko, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0134] Skipping paths under /proc, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0134] Skipping paths under /sys, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0134] Skipping paths under /var/run, as it is a whitelisted directory\n",
      "\u001b[36mINFO\u001b[0m[0147] Using files from context: [/kaniko/buildcontext/app]\n",
      "\u001b[36mINFO\u001b[0m[0147] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0147] Taking snapshot of files...\n",
      "2019/07/31 07:51:44 existing blob: sha256:b251d940695c89cba7645ba18409d57bc50de2b1ae3500e7c7645b3ef8b81c3a\n",
      "2019/07/31 07:51:44 existing blob: sha256:0c1db95989906f161007d8ef2a6ef6e0ec64bc15bf2c993fd002edbdfc7aa7df\n",
      "2019/07/31 07:51:44 existing blob: sha256:c60eba308238780085602c72a69337c634aba5207d54d2369ddd92e4120f808f\n",
      "2019/07/31 07:51:44 existing blob: sha256:3ae1eaa188d6c2f435213bce9a22318adb11d071ec01198affb8a3886f6f2495\n",
      "2019/07/31 07:51:44 existing blob: sha256:a8c5303780550b746a4781e5e4cd893121d8019e971414a2a1273d54486b4eb9\n",
      "2019/07/31 07:51:44 existing blob: sha256:54f7e8ac135a5f502a6ee9537ef3d64b1cd2fa570dc0a40b4d3b6f7ac81e7486\n",
      "2019/07/31 07:51:44 existing blob: sha256:d6341e30912f12f56e18564a3b582853f65376766f5f9d641a68a724ed6db88f\n",
      "2019/07/31 07:51:44 existing blob: sha256:041cd0421648e4d2475068b2a57abe52210afeddd6d9d30f18093d1db9b1a895\n",
      "2019/07/31 07:51:44 existing blob: sha256:2eeb5ce9b9240a928b0a799f9f2601027e2c6b7525394ae5c371f124058489d7\n",
      "2019/07/31 07:51:44 existing blob: sha256:5d71636fb824265e30ff34bf20737c9cdc4f5af28b6bce86f08215c55b89bfab\n",
      "2019/07/31 07:51:44 existing blob: sha256:620aea26e85367b08cdf1f6768491fb44df6a2a71f7d663f835b1692e849c3ee\n",
      "2019/07/31 07:51:44 existing blob: sha256:087a57faf9491b1b82a83e26bc8cc90c90c30e4a4d858b57ddd5b4c2c90095f6\n",
      "2019/07/31 07:51:44 existing blob: sha256:687ed2fb2a0d7da5503478759fd00c23970b65d02b317119b3fb9025038a2594\n",
      "2019/07/31 07:51:48 pushed blob sha256:8d6234a97f7bcdcb12dee57f245d6576b92590e39993ef5816126bb9e7efee91\n",
      "2019/07/31 07:51:49 pushed blob sha256:7aa0c8b481d57822e371b5c83ba6e745e95ca014a677ca4f8093b1da9485ee87\n",
      "2019/07/31 07:51:49 pushed blob sha256:c66c867405379dccc4d48e45be3c1887c6d1ff42283ef7c041174267eb67ce46\n",
      "2019/07/31 07:51:49 pushed blob sha256:757c537364de72cdfc95eac464c1d1c69da3f3af94c0de6939461f23fa95281f\n",
      "2019/07/31 07:51:52 gcr.io/trykube-248403/fairing-job-xgboost/fairing-job:7776134B: digest: sha256:d580bac670cfee28a8e4114056cb7b35bf54a68e177f561f4524f50cd024da86 size: 2884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Endpoint fairing-deployer-szbzp launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for prediction endpoint to come up...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prediction endpoint: http://35.229.188.112:5000/predict\n"
     ]
    }
   ],
   "source": [
    "from fairing import PredictionEndpoint\n",
    "from fairing.backends import KubeflowGKEBackend\n",
    "# The trained_ames_model.joblib is exported during the above local training\n",
    "endpoint = PredictionEndpoint(FraudServe, BASE_IMAGE, input_files=['trained_fraud_model.joblib', \"requirements.txt\"],\n",
    "                              docker_registry=DOCKER_REGISTRY, backend=KubeflowGKEBackend())\n",
    "endpoint.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy model to gcp\n",
    "# from fairing.deployers.gcp.gcpserving import GCPServingDeployer\n",
    "# deployer = GCPServingDeployer()\n",
    "# deployer.deploy(VERSION_DIR, MODEL_NAME, VERSION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the prediction endpoint\n",
    "\n",
    "Create a test dataset, then call the endpoint on Kubeflow for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"data\":{\"names\":[\"t:0\",\"t:1\"],\"tensor\":{\"shape\":[1,2],\"values\":[0.0,0.0]}},\"meta\":{}}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "(train_X, train_y), (test_X, test_y) = read_input(GCP_Bucket + \"train_fraud.csv\")\n",
    "endpoint.predict_nparray(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the prediction endpoint\n",
    "\n",
    "Delete the prediction endpoint created by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleted service: kubeflow/fairing-service-hcd5h\n",
      "Deleted deployment: kubeflow/fairing-deployer-szbzp\n"
     ]
    }
   ],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
